,Questions,Answer
0,  What is Linear Regression Algorithm?,"In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.                                                       Image Source: Google Images "
1,  How do you interpret a linear regression model?,"As we know that the linear regression model is of the form:The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).Therefore, a linear regression model is quite easy to interpret.For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered."
2,  What are the basic assumptions of the Linear Regression Algorithm?,"The basic assumptions of the Linear regression algorithm are as follows: Linearity: The relationship between the features and target. Homoscedasticity: The error term has a constant variance. Multicollinearity: There is no multicollinearity between the features. Independence: Observations are independent of each other. Normality: The error(residuals) follows a normal distribution. Now, let’s break these assumptions into different categories:It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’. Normality assumption: The error terms, ε(i), are normally distributed. Zero mean assumption: The residuals have a mean value of zero. Constant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity. Independent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.  The independent variables are measured without error. There does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data. "
3,  Explain the difference between Correlation and Regression.,Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.Regression: It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.
4,  Explain the Gradient Descent algorithm with respect to linear regression.,"Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,y is the target column or output, and m is the number of data points in the training set.Step-1: Gradient Descent starts with a random solution,Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.The updated value for the parameter is given by the formulae:Repeat until convergence(upto minimum loss function)"
5,  Justify the cases where the linear regression algorithm is suitable for a given dataset.,"Generally, a Scatter plot is used to see if linear regression is suitable for any given data. So, we can go for a linear model if the relationship looks somewhat linear. Plotting the scatter plots is easy in the case of simple or univariate linear regression.But if we have more than one independent variable i.e, the case of multivariate linear regression, then two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted to find the suitableness.On the contrary, to make the relationship linear we have to apply some transformations."
6,  List down some of the metrics used to evaluate a Regression Model.,"Mainly, there are five metrics that are commonly used to evaluate the regression models: Mean Absolute Error(MAE) Mean Squared Error(MSE) Root Mean Squared Error(RMSE) R-Squared(Coefficient of Determination) Adjusted R-Squared "
7,"  For a linear regression model, how do we interpret a Q-Q plot?","The Q-Q plot represents a graphical plotting of the quantiles of two distributions with respect to each other. In simple words, we plot quantiles against quantiles in the Q-Q plot which is used to check the normality of errors.Whenever we interpret a Q-Q plot, we should concentrate on the ‘y = x’ line, which corresponds to a normal distribution. Sometimes, this line is also known as the 45-degree line in statistics.It implies that each of the distributions has the same quantiles. In case you witness a deviation from this line, one of the distributions could be skewed when compared to the other i.e, normal distribution."
8,"  In linear regression, what is the value of the sum of the residuals for a given dataset? Explain with proper justification.","The sum of the residuals in a linear regression model is 0 since it assumes that the errors (residuals) are normally distributed with an expected value or mean equal to 0, i.e.Y = βT X + εHere, Y is the dependent variable or the target column, and β is the vector of the estimates of the regression coefficient,X is the feature matrix containing all the features as the columns, ε is the residual term such that ε ~ N(0, σ2).Moreover, the sum of all the residuals is calculated as the expected value of the residuals times the total number of observations in our dataset. Since the expectation of residuals is 0, therefore the sum of all the residual terms is zero.Note: N(μ, σ2) denotes the standard notation for a normal distribution having mean μ and standard deviation σ2."
9,  What are RMSE and MSE? How to calculate it?,"RMSE and MSE are the two of the most common measures of accuracy for linear regression.MSE (Mean Squared Error) is defined as the average of all the squared errors(residuals) for all data points. In simple words, we can say it is an average of squared differences between predicted and actual values.RMSE (Root Mean Squared Error) is the square root of the average of squared differences between predicted and actual values.RMSE stands for Root mean square error, which represented by the formulae:MSE stands for Mean square error, which represented by the formulae:Increment in RMSE is larger than MAE as the test sample size increases. In general, as the variance of error magnitudes increase, MAE remains steady but RMSE increases."
10,  What is OLS?,"OLS stands for Ordinary Least Squares. The main objective of the linear regression algorithm is to find coefficients or estimates by minimizing the error term i.e, the sum of squared errors. This process is known as OLS.This method finds the best fit line, known as regression line by minimizing the sum of square differences between the observed and predicted values."
11,  What are MAE and MAPE?,"MAE stands for Mean Absolute Error, which is defined as the average of absolute or positive errors of all values. In simple words, we can say MAE is an average of absolute or positive differences between predicted values and the actual values.                                                       Image Source: Google ImagesMAPE stands for Mean Absolute Percent Error, which calculates the average absolute error in percentage terms. In simple words, It can be understood as the percentage average of absolute or positive errors.                                                        Image Source: Google Images "
12,  Ordinary Least Squares(Statistics domain): ,To implement this in Scikit-learn we have to use the LinearRegression() class.
13,  Gradient Descent(Calculus family):,To implement this in Scikit-learn we have to use the SGDRegressor() class. 
14,  Which evaluation metric should you prefer to use for a dataset having a lot of outliers in it?,"Mean Absolute Error(MAE) is preferred when we have too many outliers present in the dataset because MAE is robust to outliers whereas MSE and RMSE are very susceptible to outliers and these start penalizing the outliers by squaring the error terms, commonly known as residuals."
15,  Explain the normal form equation of the linear regression.,"The normal equation for linear regression is :β=(XTX)-1XTYThis is also known as the closed-form solution for a linear regression model.where,Y=βTX is the equation that represents the model for the linear regression,Y is the dependent variable or target column,β is the vector of the estimates of the regression coefficient, which is arrived at using the normal equation,X is the feature matrix that contains all the features in the form of columns. The thing to note down here is that the first column in the X matrix consists of all 1s, to incorporate the offset value for the regression line."
16,  When should it be preferred to the Gradient Descent method instead of the Normal Equation in Linear Regression Algorithm?,"To answer the given question, let’s first understand the difference between the Normal equation and Gradient descent method for linear regression: Needs hyper-parameter tuning for alpha (learning parameter). It is an iterative process. Time complexity- O(kn2) Preferred when n is extremely large.  No such need for any hyperparameter. It is a non-iterative process. Time complexity- O(n3) due to evaluation of XTX. Becomes quite slow for large values of n. where,‘k’ represents the maximum number of iterations used for the gradient descent algorithm, and‘n’ is the total number of observations present in the training dataset.Clearly, if we have large training data, a normal equation is not preferred for use due to very high time complexity but for small values of ‘n’, the normal equation is faster than gradient descent."
17,  What are R-squared and Adjusted R-squared?,"R-square (R2), also known as the coefficient of determination measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model.The main problem with the R-squared is that it will always remain the same or increases as we are adding more independent variables. Therefore, to overcome this problem, an Adjusted-R2 square comes into the picture by penalizing those adding independent variables that do not improve your existing model.To learn more about, R2 and adjusted-R2, refer to the link."
18,  What are the flaws in R-squared?,"There are two major flaws of R-squared:Problem- 1: As we are adding more and more predictors, R² always increases irrespective of the impact of the predictor on the model. As R² always increases and never decreases, it can always appear to be a better fit with the more independent variables(predictors) we add to the model. This can be completely misleading.Problem- 2: Similarly, if our model has too many independent variables and too many high-order polynomials, we can also face the problem of over-fitting the data. Whenever the data is over-fitted, it can lead to a misleadingly high R² value which eventually can lead to misleading predictions.To learn more about, flaws of R2, refer to the link."
19,  What is Multicollinearity?,"It is a phenomenon where two or more independent variables(predictors) are highly correlated with each other i.e. one variable can be linearly predicted with the help of other variables. It determines the inter-correlations and inter-association among independent variables. Sometimes, multicollinearity can also be known as collinearity.                                                       Image Source: Google Images Inaccurate use of dummy variables. Due to a variable that can be computed from the other variable in the dataset.  Impacts regression coefficients i.e, coefficients become indeterminate. Causes high standard errors.  By using the correlation coefficient. With the help of Variance inflation factor (VIF), and Eigenvalues. To learn more about, multicollinearity, refer to the link."
20,  What is Heteroscedasticity? How to detect it?,"It refers to the situation where the variations in a particular independent variable are unequal across the range of values of a second variable that tries to predict it.                                                            Image Source: Google ImagesTo detect heteroscedasticity, we can use graphs or statistical tests such as the Breush-Pagan test and NCV test, etc."
21,  What are the disadvantages of the linear regression Algorithm?,"The main disadvantages of linear regression are as follows: Assumption of linearity: It assumes that there exists a linear relationship between the independent variables(input) and dependent variables (output), therefore we are not able to fit the complex problems with the help of a linear regression algorithm. Outliers: It is sensitive to noise and outliers. Multicollinearity: It gets affected by multicollinearity. "
22,  What is VIF? How do you calculate it?,"VIF stands for Variance inflation factor, which measures how much variance of an estimated regression coefficient is increased due to the presence of collinearity between the variables. It also determines how much multicollinearity exists in a particular regression model.Firstly, it applies the ordinary least square method of regression that has Xi as a function of all the other explanatory or independent variables and then calculates VIF using the given below mathematical formula:"
23,  Is it possible to apply Linear Regression for Time Series Analysis?,"Yes, we can apply a linear regression algorithm for doing analysis on time series data, but the results are not promising and hence is not advisable to do so.The reasons behind not preferable linear regression on time-series data are as follows: Time series data is mostly used for the prediction of the future but in contrast, linear regression generally seldom gives good results for future prediction as it is basically not meant for extrapolation. Moreover, time-series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis. "
24,  What are the important assumptions of Linear regression?,"A linear relationshipRestricted Multi-collinearity valueHomoscedasticityFirstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed."
25,  What is heteroscedasticity?,"Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used."
26,  What is the difference between R square and adjusted R square?,"R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. i.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables(P values less than 0.05) to indicate the percentage of variation in the model."
27,  How to find RMSE and MSE?,"RMSE and MSE are the two of the most common measures of accuracy for a linear regression.RMSE indicates the Root mean square error, which indicated by the formulae:Where MSE indicates the Mean square error represented by the formulae:"
28,  What are the possible ways of improving the accuracy of a linear regression model?,"There could be multiple ways of improving the accuracy of a linear regression, most commonly used ways are as follows:-Regression is sensitive to outliers, hence it becomes very important to treat the outliers with appropriate values. Replacing the values with mean, median, mode or percentile depending on the distribution can prove to be useful."
29,  How to interpret a Q-Q plot in a Linear regression model?,"A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors."
30,  What is the significance of an F-test in a linear model?,"– The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression."
31,  What are the disadvantages of the linear model?,– Linear regression is sensitive to outliers which may affect the result.– Over-fitting– Under-fitting
32,  What is linear regression?,"In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables.  In technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method."
33,  What is the use of regularisation? Explain L1 and L2 regularisations.,"Regularisation is a technique that is used to tackle the problem of overfitting of the model. When a very complex model is implemented on the training data, it overfits. At times, the simple model might not be able to generalise the data and the complex model overfits. To address this problem, regularisation is used.  Regularisation is nothing but adding the coefficient terms (betas) to the cost function so that the terms are penalised and are small in magnitude. This essentially helps in capturing the trends in the data and at the same time prevents overfitting by not letting the model become too complex. "
34,  How to choose the value of the parameter learning rate (α)?,"Selecting the value of learning rate is a tricky business. If the value is too small, the gradient descent algorithm takes ages to converge to the optimal solution. On the other hand, if the value of the learning rate is high, the gradient descent will overshoot the optimal solution and most likely never converge to the optimal solution. To overcome this problem, you can try different values of alpha over a range of values and plot the cost vs the number of iterations. Then, based on the graphs, the value corresponding to the graph showing the rapid decrease can be chosen.  The aforementioned graph is an ideal cost vs the number of iterations curve. Note that the cost initially decreases as the number of iterations increases, but after certain iterations, the gradient descent converges and the cost does not decrease anymore.  If you see that the cost is increasing with the number of iterations, your learning rate parameter is high and it needs to be decreased. "
35,  How to choose the value of the regularisation parameter (λ)?,"Selecting the regularisation parameter is a tricky business. If the value of λ is too high, it will lead to extremely small values of the regression coefficient β, which will lead to the model underfitting (high bias – low variance). On the other hand, if the value of λ is 0 (very small), the model will tend to overfit the training data (low bias – high variance). There is no proper way to select the value of λ. What you can do is have a sub-sample of data and run the algorithm multiple times on different sets. Here, the person has to decide how much variance can be tolerated. Once the user is satisfied with the variance, that value of λ can be chosen for the full dataset.  One thing to be noted is that the value of λ selected here was optimal for that subset, not for the entire training data. "
36,  Can we use linear regression for time series analysis?,"One can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so. The reasons behind this are — "
37,  What value is the sum of the residuals of a linear regression close to? Justify.,"Ans The sum of the residuals of a linear regression is 0. Linear regression works on the assumption that the errors (residuals) are normally distributed with a mean of 0, i.e. Y = βT X + εHere, Y is the target or dependent variable,  β is the vector of the regression coefficient, X is the feature matrix containing all the features as the columns,  ε is the residual term such that ε ~ N(0,σ2).  So, the sum of all the residuals is the expected value of the residuals times the total number of data points. Since the expectation of residuals is 0, the sum of all the residual terms is zero.  Note: N(μ,σ2) is the standard notation for a normal distribution having mean μ and standard deviation σ2."
38,  How does multicollinearity affect the linear regression?,"Ans Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, let’s say, some business model, it may cause problems. One of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business.  A highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset. "
39,"  You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?","This case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently."
40,  Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?,"This condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises."
41,  What do you mean by adjusted R2? How is it different from R2?,"Adjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2  is —   Here, n is the number of data points, and k is the number of features. One drawback of R2 is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2 overcomes this drawback. The value of the adjusted R2 increases only if the newly added feature plays a significant role in the model."
42,  How do you interpret the residual vs fitted value curve?,"The residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model. The most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve."
43,"  What is heteroscedasticity? What are the consequences, and how can you overcome it?","A random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation).  The existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values.  One of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the least variance than other Linear Unbiased Estimators (LUEs). There is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — "
44,  What is VIF? How do you calculate it?,"Variance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—   Here, VIFj  is the value of VIF for the jth variable, Rj2 is the R2 value of the model when that variable is regressed against all the other independent variables.  If the value of VIF is high for a variable, it implies that the R2  value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables."
45,  How do you know that linear regression is suitable for any given data?,"To see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted."
46,  Explain gradient descent with respect to linear regression.,"Gradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function. Gradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).  Mathematically, the aim of gradient descent for linear regression is to find the solution of ArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by —    Here, h is the linear hypothesis model, h=Θ0 + Θ1x, y is the true output, and m is the number of the data points in the training set. Gradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value. The update is: Repeat until convergence "
47,  What is robust regression?,"A regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers.  A regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. "
48,  Which graphs are suggested to be observed before model fitting?,"Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc."
49,  What is the generalized linear model?,The generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.
50,  Explain the bias-variance trade-off.,"Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a low bias. Variance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance. For a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance. There is no escaping the relationship between bias and variance in machine learning.So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built."
51,  How can learning curves help create a better model?,"Learning curves give the indication of the presence of overfitting or underfitting.  In a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:  If the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias.  If there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a high variance.  Machine Learning Engineers: Myths vs. Realities That’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments. Co-authored by – Ojas Agarwal   Lead the AI Driven Technological Revolution   										PG Diploma in Machine Learning and Artificial Intelligence									 Learn More   "
52,  What is the difference between a Perceptron and Logistic Regression?,"A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.So, what’s the difference between the two?Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model: "
53,  Can we have the same bias for all neurons of a hidden layer?,"Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.A point to note is that both these strategies would give you very different results. "
54,"  What if we do not use any activation function(s) in a neural network?
","The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:Z = (weights * X) + biasStep 2: Apply an activation function to calculate the expected output:Y = Activation(Z)Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?Our equation for Y essentially becomes:Y = Z = (weights * X) + biasWait – isn’t this just a simple linear equation? Yes – and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model. "
55,"  In a neural network, what if all the weights are initialized with the same value?","In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.Therefore, if all weights have the same initial value, this would lead to underfitting.Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below. "
56,  List the supervised and unsupervised tasks in Deep Learning.,"Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:On the other hand, there are some unsupervised deep learning techniques as well:Here is a great article on applications of Deep Learning for unsupervised tasks: "
57,  What is the role of weights and bias in a neural network?,"This is a question best explained with a real-life example. Consider that you want to go out today to play a cricket match with your friends. Now, a number of factors can affect your decision-making, like:And so on. These factors can change your decision greatly or not too much. For example, if it is raining outside, then you cannot go out to play at all. Or if you have only one bat, you can share it while playing as well. The magnitude by which these factors can affect the game is called the weight of that factor.Factors like the weather or temperature might have a higher weight, and other factors like equipment would have a lower weight.However, does this mean that we can play a cricket match with only one bat? No – we would need 1 ball and 6 wickets as well. This is where bias comes into the picture. Bias lets you assign some threshold which helps you activate a decision-point (or a neuron) only when that threshold is crossed. "
58,  How does forward propagation and backpropagation work in deep learning?,"Now, this can be answered in two ways. If you are on a phone interview, you cannot perform all the calculus in writing and show the interviewer. In such cases, it best to explain it as such:For an in-person interview, it is best to take up the marker, create a simple neural network with 2 inputs, a hidden layer, and an output layer, and explain it.Forward propagation:Backpropagation:At layer L2, for all weights:At layer L1, for all weights:You need not explain with respect to the bias term as well, though you might need to expand the above equations substituting the actual derivatives. "
59,  What are the common data structures used in Deep Learning?,"Deep Learning goes right from the simplest data structures like lists to complicated ones like computation graphs.Here are the most common ones: Once the basics are out of the way, the interview would lead to slightly advanced deep learning concepts. These questions are much easier to answer if you have considerable practice with not only the mathematical concepts but also with coding them.Additionally, these questions can also become more project-specific. As a general rule of thumb, it is best to include examples of how you have used the concept asked in the question in your own projects. This has two advantages:Here, I have given an overview of the key concepts in the questions – you can always customize your answers to add more about your experiences with some of these deep learning algorithms and techniques. "
60,  Why should we use Batch Normalization?,"Once the interviewer has asked you about the fundamentals of deep learning architectures, they would move on to the key topic of improving your deep learning model’s performance.Batch Normalization is one of the techniques used for reducing the training time of our deep learning algorithm. Just like normalizing our input helps improve our logistic regression model, we can normalize the activations of the hidden layers in our deep learning model as well:We basically normalize a[1] and a[2] here. This means we normalize the inputs to the layer, and then apply the activation functions to the normalized inputs.Here is an article that explains Batch Normalization and other techniques for improving Neural Networks: Neural Networks – Hyperparameter Tuning, Regularization & Optimization. "
61,  List the activation functions you have used so far in your projects and how you would choose one.,"The most common activation functions are:While it is not important to know all the activation functions, you can always score points by knowing the range of these functions and how they are used. Here is a handy table for you to follow:Here is a great guide on how to use these and other activations functions: Fundamentals of Deep Learning – Activation Functions and When to Use Them?."
62,  Why does a Convolutional Neural Network (CNN) work better with image data?,"The key to this question lies in the Convolution operation. Unlike humans, the machine sees the image as a matrix of pixel values. Instead of interpreting a shape like a petal or an ear, it just identifies curves and edges.Thus, instead of looking at the entire image, it helps to just read the image in parts. Doing this for a 300 x 300 pixel image would mean dividing the matrix into smaller 3 x 3 matrices and dealing with them one by one. This is convolution.Mathematically, we just perform a small operation on the matrix to help us detect features in the image – like boundaries, colors, etc.Z = X * fHere, we are convolving (* operation – not multiplication) the input matrix X with another small matrix f, called the kernel/filter to create a new matrix Z. This matrix is then passed on to the other layers.If you have a board/screen in front of you, you can always illustrate this with a simple example:Learning more about how CNNs work here. "
63,  Why do RNNs work better with text data?,"The main component that differentiates Recurrent Neural Networks (RNN) from the other models is the addition of a loop at each node. This loop brings the recurrence mechanism in RNNs. In a basic Artificial Neural Network (ANN), each input is given the same weight and fed to the network at the same time. So, for a sentence like “I saw the movie and hated it”, it would be difficult to capture the information which associates “it” with the “movie”.The addition of a loop is to denote preserving the previous node’s information for the next node, and so on. This is why RNNs are much better for sequential data, and since text data also is sequential in nature, they are an improvement over ANNs."
64,"  In a CNN, if the input size 5 X 5 and the filter size is 7 X 7, then what would be the size of the output?","This is a pretty intuitive answer. As we saw above, we perform the convolution on ‘x’ one step at a time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with dimensions 3 X 3.Thus, to make the input size similar to the filter size, we make use of padding – adding 0s to the input matrix such that its new size becomes at least 7 X 7. Thus, the output size would be using the formula:Dimension of image = (n, n) = 5 X 5Dimension of filter = (f,f)  = 7 X 7Padding = 1 (adding 1 pixel with value 0 all around the edges)Dimension of output will be (n+2p-f+1) X (n+2p-f+1) = 1 X 1 "
65,  What’s the difference between valid and same padding in a CNN?,"This question has more chances of being a follow-up question to the previous one. Or if you have explained how you used CNNs in a computer vision task, the interviewer might ask this question along with the details of the padding parameters. "
66,  What do you mean by exploding and vanishing gradients?,"The key here is to make the explanation as simple as possible. As we know, the gradient descent algorithm tries to minimize the error by taking small steps towards the minimum value. These steps are used to update the weights and biases in a neural network.However, at times, the steps become too large and this results in larger updates to weights and bias terms – so much so as to cause an overflow (or a NaN) value in the weights. This leads to an unstable algorithm and is called an exploding gradient.On the other hand, the steps are too small and this leads to minimal changes in the weights and bias terms – even negligible changes at times. We thus might end up training a deep learning model with almost the same weights and biases each time and never reach the minimum error function. This is called the vanishing gradient.A point to note is that both these issues are specifically evident in Recurrent Neural Networks – so be prepared for follow-up questions on RNN! "
67,  What are the applications of transfer learning in Deep Learning?,"I am sure you would have a doubt as to why a relatively simple question was included in the Intermediate Level. The reason is the sheer volume of subsequent questions it can generate!The use of transfer learning has been one of the key milestones in deep learning. Training a large model on a huge dataset, and then using the final parameters on smaller simpler datasets has led to defining breakthroughs in the form of Pretrained Models. Be it Computer Vision or NLP, pretrained models have become the norm in research and in the industry.Some popular examples include BERT, ResNet, GPT-2, VGG-16, etc and many more.It is here that you can earn brownie points by pointing out specific examples/projects where you used these models and how you used them as well.It is not possible to discuss all of them, so here are a few resources to get started: It is here that questions become really specific to your projects or to what you have discussed in the interview before.Also, depending on the domain – with Computer Vision or Natural Language Processing, these questions can change. While it is not important to know the architecture of each model in detail, you would need to know the intuition behind them and why these models were needed in the first place.Again, just like the intermediate level, it is important to always bring in examples that you have studied or implemented yourself into the discussion. "
68,  How backpropagation is different in RNN compared to ANN?,"In Recurrent Neural Networks, we have an additional loop at each node:This loop essentially includes a time component into the network as well. This helps in capturing sequential information from the data, which could not be possible in a generic artificial neural network.This is why the backpropagation in RNN is called Backpropagation through Time, as in backpropagation at each time step.You can find a detailed explanation of RNNs here: Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks. "
69,  How does LSTM solve the vanishing gradient challenge?,"The LSTM model is considered a special case of RNNs. The problems of vanishing gradients and exploding gradients we saw earlier are a disadvantage while using the plain RNN model.In LSTMs, we add a forget gate, which is basically a memory unit that retains information that is retained across timesteps and discards the other information that is not needed. This also necessitates the need for input and output gates to include the results of the forget gate as well. "
70,  Why is GRU faster as compared to LSTM?,"As you can see, the LSTM model can become quite complex. In order to still retain the functionality of retaining information across time and yet not make a too complex model, we need GRUs.Basically, in GRUs, instead of having an additional Forget gate, we combine the input and Forget gates into a single Update Gate:It is this reduction in the number of gates that makes GRU less complex and faster than LSTM. You can learn about GRUs, LSTMs and other sequence models in detail here: Must-Read Tutorial to Learn Sequence Modeling & Attention Models. "
71,  How is the transformer architecture better than RNN?,"Advancements in deep learning have made it possible to solve many tasks in Natural Language Processing. Networks/Sequence models like RNNs, LSTMs, etc. are specifically used for this purpose – so as to capture all possible information from a given sentence, or a paragraph. However, sequential processing comes with its caveats:This gave rise to the Transformer architecture. Transformers use what is called the attention mechanism. This basically means mapping dependencies between all the parts of a sentence."
72,  What is deep learning?,"Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc."
73,"  What are the main differences between AI, Machine Learning, and Deep Learning?"," AI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior. Machine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.  Deep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making. "
74,  Differentiate supervised and unsupervised deep learning procedures.," Supervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing. Unsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data. "
75,  What are the applications of deep learning?,There are various applications of deep learning: Computer vision Natural language processing and pattern recognition Image recognition and processing Machine translation Sentiment analysis Question Answering system Object Classification and Detection Automatic Handwriting Generation Automatic Text Generation. 
76,  Do you think that deep network is better than a shallow one?,"Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input."
77,"  What do you mean by ""overfitting""?",Overfitting is the most common issue which occurs in deep learning. It usually occurs when a deep learning algorithm apprehends the sound of specific data. It also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.
78,  What is Backpropagation?,Backpropagation is a training algorithm which is used for multilayer neural networks. It transfers the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient.Backpropagation can be divided into the following steps: It can forward propagation of training data through the network to generate output. It uses target value and output value to compute error derivative concerning output activations. It can backpropagate to compute the derivative of the error concerning output activations in the previous layer and continue for all hidden layers. It uses the previously calculated derivatives for output and all hidden layers to calculate the error derivative concerning weights. It updates the weights. 
79,  What is the function of the Fourier Transform in Deep Learning?,"Fourier transform package is highly efficient for analyzing, maintaining, and managing a large databases. The software is created with a high-quality feature known as the special portrayal. One can effectively utilize it to generate real-time array data, which is extremely helpful for processing all categories of signals."
80,  Describe the theory of autonomous form of deep learning in a few words.,"There are several forms and categories available for the particular subject, but the autonomous pattern represents independent or unspecified mathematical bases which are free from any specific categorizer or formula."
81,"  What is the use of Deep learning in today's age, and how is it adding data scientists?",Deep learning has brought significant changes or revolution in the field of machine learning and data science. The concept of a complex neural network (CNN) is the main center of attention for data scientists. It is widely taken because of its advantages in performing next-level machine learning operations. The advantages of deep learning also include the process of clarifying and simplifying issues based on an algorithm due to its utmost flexible and adaptable nature. It is one of the rare procedures which allow the movement of data in independent pathways. Most of the data scientists are viewing this particular medium as an advanced additive and extended way to the existing process of machine learning and utilizing the same for solving complex day to day issues.
82,  What are the deep learning frameworks or tools?,"Deep learning frameworks or tools are:Tensorflow, Keras, Chainer, Pytorch, Theano & Ecosystem, Caffe2, CNTK, DyNetGensim, DSSTNE, Gluon, Paddle, Mxnet, BigDL"
83,  What are the disadvantages of deep learning?,"There are some disadvantages of deep learning, which are: Deep learning model takes longer time to execute the model. In some cases, it even takes several days to execute a single model depends on complexity. The deep learning model is not good for small data sets, and it fails here. "
84,  What is the meaning of term weight initialization in neural networks?,"In neural networking, weight initialization is one of the essential factors. A bad weight initialization prevents a network from learning. On the other side, a good weight initialization helps in giving a quicker convergence and a better overall error. Biases can be initialized to zero. The standard rule for setting the weights is to be close to zero without being too small."
85,  Explain Data Normalization.,"Data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. It assures better convergence during backpropagation. In general, data normalization boils down to subtracting the mean of each data point and dividing by its standard deviation."
86,  Why is zero initialization not a good weight initialization process?,"If the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.As a result, the network cannot learn at all because there is no source of asymmetry between neurons. That is the reason why we need to add randomness to the weight initialization process."
87,  What are the prerequisites for starting in Deep Learning?,"There are some basic requirements for starting in Deep Learning, which are: Machine Learning Mathematics Python Programming "
88,  What are the supervised learning algorithms in Deep learning?, Artificial neural network Convolution neural network Recurrent neural network 
89,  What are the unsupervised learning algorithms in Deep learning?, Self Organizing Maps Deep belief networks (Boltzmann Machine) Auto Encoders 
90,  How many layers in the neural network?, Input Layer The input layer contains input neurons which send information to the hidden layer. Hidden Layer The hidden layer is used to send data to the output layer. Output Layer The data is made available at the output layer. 
91,  What is the use of the Activation function?,"The activation function is used to introduce nonlinearity into the neural network so that it can learn more complex function. Without the Activation function, the neural network would be only able to learn function, which is a linear combination of its input data.Activation function translates the inputs into outputs. The activation function is responsible for deciding whether a neuron should be activated or not. It makes the decision by calculating the weighted sum and further adding bias with it. The basic purpose of the activation function is to introduce non-linearity into the output of a neuron."
92,  How many types of activation function are available?, Binary Step Sigmoid Tanh ReLU Leaky ReLU Softmax Swish 
93,  What is a binary step function?,"The binary step function is an activation function, which is usually based on a threshold. If the input value is above or below a particular threshold limit, the neuron is activated, then it sends the same signal to the next layer. This function does not allow multi-value outputs."
94,  What is the sigmoid function?,"The sigmoid activation function is also called the logistic function. It is traditionally a trendy activation function for neural networks. The input data to the function is transformed into a value between 0.0 and 1.0. Input values that are much larger than 1.0 are transformed to the value 1.0. Similarly, values that are much smaller than 0.0 are transformed into 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. It was the default activation used on neural networks, in the early 1990s."
95,  What is Tanh function?,"The hyperbolic tangent function, also known as tanh for short, is a similar shaped nonlinear activation function. It provides output values between -1.0 and 1.0. Later in the 1990s and through the 2000s, this function was preferred over the sigmoid activation function as models. It was easier to train and often had better predictive performance."
96,  What is ReLU function?,"A node or unit which implements the activation function is referred to as a rectified linear activation unit or ReLU for short. Generally, networks that use the rectifier function for the hidden layers are referred to as rectified networks.Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution."
97,  What is the use of leaky ReLU function?,The Leaky ReLU (LReLU or LReL) manages the function to allow small negative values when the input is less than zero.
98,  What is the softmax function?,"The softmax function is used to calculate the probability distribution of the event over 'n' different events. One of the main advantages of using softmax is the output probabilities range. The range will be between 0 to 1, and the sum of all the probabilities will be equal to one. When the softmax function is used for multi-classification model, it returns the probabilities of each class, and the target class will have a high probability."
99,  What is a Swish function?,"Swish is a new, self-gated activation function. Researchers at Google discovered the Swish function. According to their paper, it performs better than ReLU with a similar level of computational efficiency. "
100,  What is the most used activation function?,Relu function is the most used activation function. It helps us to solve vanishing gradient problems.
101,  Can Relu function be used in output layer?,"No, Relu function has to be used in hidden layers."
102,  In which layer softmax activation function used?,Softmax activation function has to be used in the output layer.
103,  What do you understand by Autoencoder?,"Autoencoder is an artificial neural network. It can learn representation for a set of data without any supervision. The network automatically learns by copying its input to the output; typically,internet representation consists of smaller dimensions than the input vector. As a result, they can learn efficient ways of representing the data. Autoencoder consists of two parts; an encoder tries to fit the inputs to the internal representation, and a decoder converts the internal state to the outputs."
104,  What do you mean by Dropout?,"Dropout is a cheap regulation technique used for reducing overfitting in neural networks. We randomly drop out a set of nodes at each training step. As a result, we create a different model for each training case, and all of these models share weights. It's a form of model averaging."
105,  What do you understand by Tensors?,"Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, which allows us to represent the data having higher dimensions. In general, we deal with high dimensional data sets where dimensions refer to different features present in the data set."
106,  What do you understand by Boltzmann Machine?,"A Boltzmann machine (also known as stochastic Hopfield network with hidden units) is a type of recurrent neural network. In a Boltzmann machine, nodes make binary decisions with some bias. Boltzmann machines can be strung together to create more sophisticated systems such as deep belief networks. Boltzmann Machines can be used to optimize the solution to a problem. Some important points about Boltzmann Machine- It uses a recurrent structure. It consists of stochastic neurons, which include one of the two possible states, either 1 or 0. The neurons present in this are either in an adaptive state (free state) or clamped state (frozen state). If we apply simulated annealing or discrete Hopfield network, then it would become a Boltzmann Machine. "
107,  What is Model Capacity?,"The capacity of a deep learning neural network controls the scope of the types of mapping functions that it can learn. Model capacity can approximate any given function. When there is a higher model capacity, it means that the larger amount of information can be stored in the network."
108,  What is the cost function?,"A cost function describes us how well the neural network is performing with respect to its given training sample and the expected output. It may depend on variables such as weights and biases.It provides the performance of a neural network as a whole. In deep learning, our priority is to minimize the cost function. That's why we prefer to use the concept of gradient descent."
109,  Explain gradient descent?,"An optimization algorithm that is used to minimize some function by repeatedly moving in the direction of steepest descent as specified by the negative of the gradient is known as gradient descent. It's an iteration algorithm, in every iteration algorithm, we compute the gradient of a cost function, concerning each parameter and update the parameter of the function via the following formula:Where,Θ - is the parameter vector, α - learning rate,  J(Θ) - is a cost functionIn machine learning, it is used to update the parameters of our model. Parameters represent the coefficients in linear regression and weights in neural networks."
110,"  Explain the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch?"," Stochastic Gradient Descent Stochastic gradient descent is used to calculate the gradient and update the parameters by using only a single training example. Batch Gradient Descent Batch gradient descent is used to calculate the gradients for the whole dataset and perform just one update at each iteration. Mini-batch Gradient Descent Mini-batch gradient descent is a variation of stochastic gradient descent. Instead of a single training example, mini-batch of samples is used. Mini-batch gradient descent is one of the most popular optimization algorithms. "
111,  What are the main benefits of Mini-batch Gradient Descent?," It is computationally efficient compared to stochastic gradient descent. It improves generalization by finding flat minima. It improves convergence by using mini-batches. We can approximate the gradient of the entire training set, which might help to avoid local minima. "
112,  What is matrix element-wise multiplication? Explain with an example.,Element-wise matrix multiplication is used to take two matrices of the same dimensions. It further produces another combined matrix with the elements that are a product of corresponding elements of matrix a and b.
113,  What do you understand by a convolutional neural network?,"A convolutional neural network, often called CNN, is a feedforward neural network. It uses convolution in at least one of its layers. The convolutional layer contains a set of filter (kernels). This filter is sliding across the entire input image, computing the dot product between the weights of the filter and the input image. As a result of training, the network automatically learns filters that can detect specific features."
114,  Explain the different layers of CNN.,"There are four layered concepts that we should understand in CNN (Convolutional Neural Network): Convolution This layer comprises of a set of independent filters. All these filters are initialized randomly. These filters then become our parameters which will be learned by the network subsequently. ReLU The ReLu layer is used with the convolutional layer. Pooling It reduces the spatial size of the representation to lower the number of parameters and computation in the network. This layer operates on each feature map independently. Full Collectedness Neurons in a completely connected layer have complete connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can be easily computed with a matrix multiplication followed by a bias offset. "
115,  What is an RNN?,"RNN stands for Recurrent Neural Networks. These are the artificial neural networks which are designed to recognize patterns in sequences of data such as handwriting, text, the spoken word, genomes, and numerical time series data. RNN use backpropagation algorithm for training because of their internal memory. RNN can remember important things about the input they received, which enables them to be very precise in predicting what's coming next."
116,  What are the issues faced while training in Recurrent Networks?,"Recurrent Neural Network uses backpropagation algorithm for training, but it is applied on every timestamp. It is usually known as Back-propagation Through Time (BTT).There are two significant issues with Back-propagation, such as: Vanishing Gradient When we perform Back-propagation, the gradients tend to get smaller and smaller because we keep on moving backward in the Network. As a result, the neurons in the earlier layer learn very slowly if we compare it with the neurons in the later layers.Earlier layers are more valuable because they are responsible for learning and detecting simple patterns. They are the building blocks of the network. If they provide improper or inaccurate results, then how can we expect the next layers and complete network to perform nicely and provide accurate results. The training procedure tales long, and the prediction accuracy of the model decreases. Exploding Gradient Exploding gradients are the main problem when large error gradients accumulate. They provide result in very large updates to neural network model weights during training. Gradient Descent process works best when updates are small and controlled. When the magnitudes of the gradient accumulate, an unstable network is likely to occur. It can cause poor prediction of results or even a model that reports nothing useful. "
117,  Explain the importance of LSTM.,"LSTM stands for Long short-term memory. It is an artificial RNN (Recurrent Neural Network) architecture, which is used in the field of deep learning. LSTM has feedback connections which makes it a ""general purpose computer."" It can process not only a single data point but also entire sequences of data.They are a special kind of RNN which are capable of learning long-term dependencies."
118,  What are the different layers of Autoencoders? Explain briefly.,An autoencoder contains three layers: Encoder The encoder is used to compress the input into a latent space representation. It encodes the input images as a compressed representation in a reduced dimension. The compressed images are the distorted version of the original image. Code The code layer is used to represent the compressed input which is fed to the decoder. Decoder The decoder layer decodes the encoded image back to its original dimension. The decoded image is a reduced reconstruction of the original image. It is automatically reconstructed from the latent space representation. 
119,  What do you understand by Deep Autoencoders?,"Deep Autoencoder is the extension of the simple Autoencoder. The first layer present in DeepAutoencoder is responsible for first-order functions in the raw input. The second layer is responsible for second-order functions corresponding to patterns in the appearance of first-order functions. Deeper layers which are available in the Deep Autoencoder tend to learn even high-order features.A deep autoencoder is the combination of two, symmetrical deep-belief networks: First four or five shallow layers represent the encoding half. The other combination of four or five layers makes up the decoding half. "
120,  What are the three steps to developing the necessary assumption structure in Deep learning?,"The procedure of developing an assumption structure involves three specific actions.  The first step contains algorithm development. This particular process is lengthy. The second step contains algorithm analyzing, which represents the in-process methodology.  The third step is about implementing the general algorithm in the final procedure. The entire framework is interlinked and required for throughout the process. "
121,"  What do you understand by Perceptron? Also, explain its type.",A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features. It is an algorithm for supervised learning of binary classifiers. This algorithm is used to enable neurons to learn and processes elements in the training set one at a time.There are two types of perceptrons: Single-Layer Perceptron Single layer perceptrons can learn only linearly separable patterns. Multilayer Perceptrons Multilayer perceptrons or feedforward neural networks with two or more layers have the higher processing power. 
122,  What is Deep Learning?,"If you are going for a deep learning interview, you definitely know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example. Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog)."
123,  What is a Neural Network?,"Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.The most common Neural Networks consist of three network layers:Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course"
124,  What Is a Multi-layer Perceptron(MLP)?,"As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called “backpropagation.” In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately)."
125,"  What Is Data Normalization, and Why Do We Need It?","The process of standardizing and reforming data is called “Data Normalization.” It’s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence."
126,  What is the Boltzmann Machine?,"One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected."
127,  What Is the Role of Activation Functions in a Neural Network?,"At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions."
128,  What Is the Cost Function?,"Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s performance is. It’s used to compute the error of the output layer during backpropagation. We push that error backward through the neural network and use that during the different training functions."
129,  What Is Gradient Descent?,Gradient Descent is an optimal algorithm to minimize the cost function or to minimize an error. The aim is to find the local-global minima of a function. This determines the direction the model should take to reduce the error.
130,  What Do You Understand by Backpropagation?,This is one of the most frequently asked deep learning interview questions. Backpropagation is a technique to improve the performance of the network. It backpropagates the error and updates the weights to reduce the error.
131,  What Is the Difference Between a Feedforward Neural Network and Recurrent Neural Network?,"In this deep learning interview question, the interviewee expects you to give a detailed answer.A Feedforward Neural Network signals travel in one direction from input to output. There are no feedback loops; the network considers only the current input. It cannot memorize previous inputs (e.g., CNN).Deep Learning Course (with TensorFlow & Keras)Master the Deep Learning Concepts and ModelsView CourseA Recurrent Neural Network’s signals travel in both directions, creating a looped network. It considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory."
132,  What Are the Applications of a Recurrent Neural Network (RNN)?,"The RNN can be used for sentiment analysis, text mining, and image captioning. Recurrent Neural Networks can also address time series problems such as predicting the prices of stocks in a month or quarter."
133,  What Are the Softmax and ReLU Functions?,"Softmax is an activation function that generates the output between zero and one. It divides each output, such that the total sum of the outputs is equal to one. Softmax is often used for output layers.ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an output of X if X is positive and zeros otherwise. ReLU is often used for hidden layers."
134,  What Are Hyperparameters?,"This is another frequently asked deep learning interview question. With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.)."
135,  What Will Happen If the Learning Rate Is Set Too Low or Too High?,"When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point.If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train)."
136,  What Is Dropout and Batch Normalization?,Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping 20 percent of the nodes). It doubles the number of iterations needed to converge the network.Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.
137,  What Is the Difference Between Batch Gradient Descent and Stochastic Gradient Descent?,"Batch Gradient DescentStochastic Gradient DescentThe batch gradient computes the gradient using the entire dataset.It takes time to converge because the volume of data is huge, and weights update slowly.The stochastic gradient computes the gradient using a single sample.It converges much faster than the batch gradient because it updates weight more frequently."
138,"  What is Overfitting and Underfitting, and How to Combat Them?","Overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. It is more likely to occur with nonlinear models that have more flexibility when learning a target function. An example would be if a model is looking at cars and trucks, but only recognizes trucks that have a specific box shape. It might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. The model performs well on training data, but not in the real world.Underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. This usually happens when there is less and incorrect data to train a model. Underfitting has both poor performance and accuracy.To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model."
139,  How Are Weights Initialized in a Network?,"There are two methods here: we can either initialize the weights to zero or assign them randomly.Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless.Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method.Free Deep Learning for Beginners CourseMaster the Basics of Deep LearningEnroll Now"
140,  What Are the Different Layers on CNN?,There are four layers in CNN:
141,"  What is Pooling on CNN, and How Does It Work?",Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix.
142,  How Does an LSTM Network Work?,"Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. There are three steps in an LSTM network:"
143,  What Are Vanishing and Exploding Gradients?,"While training an RNN, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a “Vanishing Gradient.” When the slope tends to grow exponentially instead of decaying, it’s referred to as an “Exploding Gradient.” Gradient problems lead to long training times, poor performance, and low accuracy."
144,  Why is Tensorflow the Most Preferred Library in Deep Learning?,"Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and GPU computing devices."
145,  What Do You Mean by Tensor in Tensorflow?,This is another most frequently asked deep learning interview question. A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with different dimensions and ranks fed as input to the neural network are called “Tensors.”
146,  What Are the Programming Elements in Tensorflow?,"Constants - Constants are parameters whose value does not change. To define a constant we use  tf.constant() command. For example:a = tf.constant(2.0,tf.float32)b = tf.constant(3.0)Print(a, b)Variables - Variables allow us to add new trainable parameters to graph. To define a variable, we use the tf.Variable() command and initialize them before running the graph in a session. An example:W = tf.Variable([.3].dtype=tf.float32)b = tf.Variable([-.3].dtype=tf.float32)Placeholders - these allow us to feed data to a tensorflow model from outside a model. It permits a value to be assigned later. To define a placeholder, we use the tf.placeholder() command. An example:a = tf.placeholder (tf.float32)b = a*2with tf.Session() as sess:result = sess.run(b,feed_dict={a:3.0})print resultSessions - a session is run to evaluate the nodes. This is called the “Tensorflow runtime.” For example:a = tf.constant(2.0)b = tf.constant(4.0)c = a+b# Launch SessionSess = tf.Session()# Evaluate the tensor cprint(sess.run(c))FREE Machine Learning CourseLearn In-demand Machine Learning Skills and ToolsStart Learning"
147,  Explain a Computational Graph.,"Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a “DataFlow Graph.”"
148,  Explain Generative Adversarial Network.,"Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner’s check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic.The forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.Let us understand this example with the help of an image shown above.There is a noise vector coming into the forger who is generating fake wine.Here the forger acts as a Generator.The shop owner acts as a Discriminator.The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake.So, there are two primary components of Generative Adversarial Network (GAN) named:The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images."
149,  What Is an Auto-encoder?,This Neural Network has three layers in which the input neurons are equal to the output neurons. The network's target outside is the same as the input. It uses dimensionality reduction to restructure the input. It works by compressing the image input to a latent space representation then reconstructing the output from this representation.
150,  What Is Bagging and Boosting?,"Bagging and Boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call.With Bagging, we take a dataset and split it into training data and test data. Then we randomly select data to place into the bags and train the model separately.With Boosting, the emphasis is on selecting data points which give wrong output to improve the accuracy."
151,  What is the difference between Machine Learning and Deep Learning?,"Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience. Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.  Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks."
152,  What is a perceptron?,"A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output. A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.  A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation."
153,  How is Deep Learning better than Machine Learning?,"Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.  Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this."
154,  What are some of the most used applications of Deep Learning?,Deep Learning is used in a variety of fields today. The most used ones are as follows:  Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition   Deep Learning is used in a variety of fields today. The most used ones are as follows: Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition 
155,  What is the meaning of overfitting?,"Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.  Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented."
156,  What are activation functions?,Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias. Using an activation function makes the model output to be non-linear. There are many types of activation functions:  ReLU Softmax Sigmoid Linear Tanh  Get 50% Hike!Master Most in Demand Skills Now !                                             Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias.Using an activation function makes the model output to be non-linear. There are many types of activation functions: ReLU Softmax Sigmoid Linear Tanh Get 50% Hike!Master Most in Demand Skills Now !
157,  Why is Fourier transform used in Deep Learning?,Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.  Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.
158,  What are the steps involved in training a perception in Deep Learning?,There are five main steps that determine the learning of a perceptron:  Initialize thresholds and weights Provide inputs Calculate outputs Update weights in each step Repeat steps 2 to 4   There are five main steps that determine the learning of a perceptron:
159,  What is the use of the loss function?,"The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset. The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.  The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset.The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training."
160,  What are some of the Deep Learning frameworks or tools that you have used?,"This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools. However, some of the top Deep Learning frameworks out there today are:  TensorFlow Keras PyTorch Caffe2 CNTK MXNet Theano  This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools.However, some of the top Deep Learning frameworks out there today are: TensorFlow Keras PyTorch Caffe2 CNTK MXNet Theano "
161,  What is the use of the swish function?,The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency. The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency.
162,  What are autoencoders?,"Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs. Autoencoders, as the name suggests, consist of two entities:  Encoder: Used to fit the input into an internal computation state Decoder: Used to convert the computational state back into the output  Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs.Autoencoders, as the name suggests, consist of two entities: Encoder: Used to fit the input into an internal computation state Decoder: Used to convert the computational state back into the output "
163,  What are the steps to be followed to use the gradient descent algorithm?,There are five main steps that are used to initialize and use the gradient descent algorithm:  Initialize biases and weights for the network Send input data through the network (the input layer) Calculate the difference (the error) between expected and predicted values Change values in neurons to minimize the loss function Multiple iterations to determine the best weights for efficient working  There are five main steps that are used to initialize and use the gradient descent algorithm: Initialize biases and weights for the network Send input data through the network (the input layer) Calculate the difference (the error) between expected and predicted values Change values in neurons to minimize the loss function Multiple iterations to determine the best weights for efficient working 
164,  Differentiate between a single-layer perceptron and a multi-layer perceptron.,    Single-layer Perceptron Multi-layer Perceptron   Cannot classify non-linear data points Can classify non-linear data   Takes in a limited amount of parameters Withstands a lot of parameters   Less efficient with large data Highly efficient with large datasets      Career Transition                                                   Career Transition
165,  What is data normalization in Deep Learning?,Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation. Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation.
166,  What is forward propagation?,"Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer. Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer."
167,  What is backpropagation?,"Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers. Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers."
168,  What are hyperparameters in Deep Learning?,"Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers, and more, present in the neural network. Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers, and more, present in the neural network."
169,  How can hyperparameters be trained in neural networks?,"Hyperparameters can be trained using four components as shown below:  Batch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement. Epochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data. Momentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training. Learning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn.  Next up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.  Intermediate Interview Questions Hyperparameters can be trained using four components as shown below: Batch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement. Epochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data. Momentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training. Learning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn. Next up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions."
170,  What is the meaning of dropout in Deep Learning?,"Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby, causing lower efficiency. Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby, causing lower efficiency."
171,  What are tensors?,"Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors is easily understood and broadly used. Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors is easily understood and broadly used."
172,  What is the meaning of model capacity in Deep Learning?,"In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network. We will check out neural network interview questions alongside as it is also a vital part of Deep Learning. In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network.We will check out neural network interview questions alongside as it is also a vital part of Deep Learning."
173,  What is a Boltzmann machine?,"A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there. A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there."
174,  What are some of the advantages of using TensorFlow?,"TensorFlow has numerous advantages, and some of them are as follows:  High amount of flexibility and platform independence Trains using CPU and GPU Supports auto differentiation and its features Handles threads and asynchronous computation easily Open-source Has a large community   Courses you may like      TensorFlow has numerous advantages, and some of them are as follows: High amount of flexibility and platform independence Trains using CPU and GPU Supports auto differentiation and its features Handles threads and asynchronous computation easily Open-source Has a large community Courses you may like"
175,  What is a computational graph in Deep Learning?,"A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability. If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course. A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course."
176,  What is a CNN?,CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily. These Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary. CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily.These Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary.
177,  What are the various layers present in a CNN?,There are four main layers that form a convolutional neural network:  Convolution: These are layers consisting of entities called filters that are used as parameters to train the network. ReLu: It is used as the activation function and is always used with the convolution layer. Pooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage. Connectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.  There are four main layers that form a convolutional neural network: Convolution: These are layers consisting of entities called filters that are used as parameters to train the network. ReLu: It is used as the activation function and is always used with the convolution layer. Pooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage. Connectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily. 
178,  What is an RNN in Deep Learning?,"RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements. RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements."
179,  What is a vanishing gradient when using RNNs?,"Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network. Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network."
180,  What is exploding gradient descent in Deep Learning?,Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training. The working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model. Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training.The working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model.
181,  What is the use of LSTM?,LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity. LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity.
182,  Where are autoencoders used?,Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:  Adding color to black–white images Removing noise from images Dimensionality reduction Feature removal and variation  Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones: Adding color to black–white images Removing noise from images Dimensionality reduction Feature removal and variation 
183,  What are the types of autoencoders?,There are four main types of autoencoders:  Deep autoencoders Convolutional autoencoders Sparse autoencoders Contractive autoencoders  There are four main types of autoencoders: Deep autoencoders Convolutional autoencoders Sparse autoencoders Contractive autoencoders 
184,  What is a Restricted Boltzmann Machine?,"A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform:  Dimensionality reduction Regression Classification Collaborative filtering Topic modeling  Next up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.   Advanced Interview Questions A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform: Dimensionality reduction Regression Classification Collaborative filtering Topic modeling Next up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions."
185,  What are some of the limitations of Deep Learning?,There are a few disadvantages of Deep Learning as mentioned below:  Networks in Deep Learning require a huge amount of data to train well. Deep Learning concepts can be complex to implement sometimes. Achieving a high amount of model efficiency is difficult in many cases.  These are some of the vital advanced deep learning interview questions that you have to know about! There are a few disadvantages of Deep Learning as mentioned below: Networks in Deep Learning require a huge amount of data to train well. Deep Learning concepts can be complex to implement sometimes. Achieving a high amount of model efficiency is difficult in many cases. These are some of the vital advanced deep learning interview questions that you have to know about!
186,  What are the variants of gradient descent?,"There are three variants of gradient descent as shown below:  Stochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters. Batch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration. Mini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent.  There are three variants of gradient descent as shown below: Stochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters. Batch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration. Mini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent. "
187,  Why is mini-batch gradient descent so popular?,Mini-batch gradient descent is popular as:  It is more efficient when compared to stochastic gradient descent. Generalization is done by finding the flat minima. It helps avoid the local minima by allowing the approximation of the gradient for the entire dataset.  Mini-batch gradient descent is popular as: It is more efficient when compared to stochastic gradient descent. Generalization is done by finding the flat minima. It helps avoid the local minima by allowing the approximation of the gradient for the entire dataset. 
188,  What are deep autoencoders?,"Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on. Usually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where:  The first five shallow layers consist of the encoding part The other layers take care of the decoding part  On the next set of Deep Learning questions, let us look further into the topic. Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on.Usually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where: The first five shallow layers consist of the encoding part The other layers take care of the decoding part On the next set of Deep Learning questions, let us look further into the topic."
189,  Why is the Leaky ReLU function used in Deep Learning?,"Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero. Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero."
190,  What are some of the examples of supervised learning algorithms in Deep Learning?,There are three main supervised learning algorithms in Deep Learning:  Artificial neural networks Convolutional neural networks Recurrent neural networks  There are three main supervised learning algorithms in Deep Learning: Artificial neural networks Convolutional neural networks Recurrent neural networks 
191,  What are some of the examples of unsupervised learning algorithms in Deep Learning?,"There are three main unsupervised learning algorithms in Deep Learning:  Autoencoders Boltzmann machines Self-organizing maps  Next up, let us look at  more neural network interview questions that will help you ace the interviews. There are three main unsupervised learning algorithms in Deep Learning: Autoencoders Boltzmann machines Self-organizing maps Next up, let us look at  more neural network interview questions that will help you ace the interviews."
192,  Can we initialize the weights of a network to start from zero?,"Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons. Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons."
193,  What is the meaning of valid padding and same padding in CNN?," Valid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n – f + 1) X (n – f + 1) after convolution. Same padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix.   Valid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n – f + 1) X (n – f + 1) after convolution. Same padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix. "
194,  What are some of the applications of transfer learning in Deep Learning?,"Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks. The popular examples of transfer learning are in the case of:  BERT ResNet GPT-2 VGG-16  Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks.The popular examples of transfer learning are in the case of: BERT ResNet GPT-2 VGG-16 "
195,  How is the transformer architecture better than RNNs in Deep Learning?,"With the use of sequential processing, programmers were up against:  The usage of high processing power The difficulty of parallel execution  This caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.  With the use of sequential processing, programmers were up against: The usage of high processing power The difficulty of parallel execution This caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models."
196,  What are the steps involved in the working of an LSTM network?,There are three main steps involved in the working of an LSTM network:  The network picks up the information that it has to remember and identifies what to forget. Cell state values are updated based on Step 1. The network calculates and analyzes which part of the current state should make it to the output.  There are three main steps involved in the working of an LSTM network: The network picks up the information that it has to remember and identifies what to forget. Cell state values are updated based on Step 1. The network calculates and analyzes which part of the current state should make it to the output. 
197,  What are the elements in TensorFlow that are programmable?,"In TensorFlow, users can program three elements:  Constants Variables Placeholders  In TensorFlow, users can program three elements: Constants Variables Placeholders "
198,  What is the meaning of bagging and boosting in Deep Learning?,Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model. Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy. Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model.Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy.
199,  What are generative adversarial networks (GANs)?,"Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output. The generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator. Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output.The generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator."
200,  Why are generative adversarial networks (GANs) so popular?,"Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working.  Creation of art: GANs are used to create artistic images, sketches, and paintings. Image enhancement: They are used to greatly enhance the resolution of the input images. Image translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily.  If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Course. With this program, you can become proficient in all of the concepts of Deep Learning and AI and earn a course certificate as well. Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working. Creation of art: GANs are used to create artistic images, sketches, and paintings. Image enhancement: They are used to greatly enhance the resolution of the input images. Image translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily. "
201,  Why is it necessary to introduce non-linearities in a neural network?,"Solution: otherwise, we would have a composition of linear functions, which is also a linear function, giving a linear model. A linear model has a much smaller number of parameters, and is therefore limited in the complexity it can model."
202,  Describe two ways of dealing with the vanishing gradient problem in a neural network.,Solution:Using ReLU activation instead of sigmoid.Using Xavier initialization.
203,  What are some advantages in using a CNN (convolutional neural network) rather than a DNN (dense neural network) in an image classification task?,"Solution: while both models can capture the relationship between close pixels, CNNs have the following properties:It is translation invariant — the exact location of the pixel is irrelevant for the filter.It is less likely to overfit — the typical number of parameters in a CNN is much smaller than that of a DNN.Gives us a better understanding of the model — we can look at the filters’ weights and visualize what the network “learned”.Hierarchical nature — learns patterns in by describing complex patterns using simpler ones."
204,  Describe two ways to visualize features of a CNN in an image classification task.,"Solution:Input occlusion — cover a part of the input image and see which part affect the classification the most. For instance, given a trained image classification model, give the images below as input. If, for instance, we see that the 3rd image is classified with 98% probability as a dog, while the 2nd image only with 65% accuracy, it means that the part covered in the 2nd image is more important.Activation Maximization — the idea is to create an artificial input image that maximize the target response (gradient ascent)."
205,"  Is trying the following learning rates: 1, 2,…, 5 a good strategy to optimize the learning rate?","Solution: No, it is recommended to try a logarithmic scale to optimize the learning rate."
206,  Suppose you have a NN with 3 layers and ReLU activations. What will happen if we initialize all the weights with the same value? what if we only had 1 layer (i.e linear/logistic regression?),"Solution: If we initialize all the weights to be the same we would not be able to break the symmetry; i.e, all gradients will be updated the same and the network will not be able to learn. In the 1-layers scenario, however, the cost function is convex (linear/sigmoid) and thus the weights will always converge to the optimal point, regardless of the initial value (convergence may be slower)."
207,  Explain the idea behind the Adam optimizer.,"Solution: Adam, or adaptive momentum, combines two ideas to improve convergence: per-parameter updates which give faster convergence, and momentum which helps to avoid getting stuck in saddle point."
208,"  Compare batch, mini-batch and stochastic gradient descent.","Solution: batch refers to estimating the data by taking the entire data, mini-batch by sampling a few datapoints, and SGD refers to update the gradient one datapoint at each epoch. The tradeoff here is between how precise the calculation of the gradient is versus what size of batch we can keep in memory. Moreover, taking mini-batch rather than the entire batch has a regularizing effect by adding random noise at each epoch."
209,  What is data augmentation? Give examples.,"Solution: Data augmentation is a technique to increase the input data by performing manipulations on the original data. For instance in images, one can: rotate the image, reflect (flip) the image, add Gaussian blur."
210,  What is the idea behind GANs?,"Solution: GANs, or generative adversarial networks, consist of two networks (D,G) where D is the “discriminator” network and G is the “generative” network. The goal is to create data — images, for instance, which are undistinguishable from real images. Suppose we want to create an adversarial example of a cat. The network G will generate images. The network D will classify images according to whether they are a cat or not. The cost function of G will be constructed such that it tries to “fool” D — to classify its output always as cat."
211,  What are the advantages of using Batchnorm?,Solution: Batchnorm accelerates the training process. It also (as a byproduct of including some noise) has a regularizing effect.
212,  What is multi-task learning? When should it be used?,"Solution: Multi-tasking is useful when we have a small amount of data for some task, and we would benefit from training a model on a large dataset of another task. Parameters of the models are shared — either in a “hard” way (i.e the same parameters) or a “soft” way (i.e regularization/penalty to the cost function)."
213,  What is end-to-end learning? Give a few of its advantages.,"Solution: End-to-end learning is usually a model which gets the raw data and outputs directly the desired outcome, with no intermediate tasks or feature engineering. It has several advantages, among which: there is no need to handcraft features, and it generally leads to lower bias."
214,  What happens if we use a ReLU activation and then a sigmoid as the final layer?,"Solution: Since ReLU always outputs a non-negative result, the network will constantly predict one class for all the inputs!"
215,  How to solve the exploding gradient problem?,"Solution: A simple solution to the exploding gradient problem is gradient clipping — taking the gradient to be ±M when its absolute value is bigger than M, where M is some large number."
216,  Is it necessary to shuffle the training data when using batch gradient descent?,"Solution: No, because the gradient is calculated at each epoch using the entire training data, so shuffling does not make a difference."
217,"  When using mini batch gradient descent, why is it important to shuffle the data?","Solution: otherwise, suppose we train a NN classifier and have two classes — A and B, and that all samples of one class come before the other class. Not shuffling the data will make the weights converge to a wrong value."
218,  Describe some hyperparameters for transfer learning.,"Solution: How many layers to keep, how many layers to add, how many to freeze."
219,  Is dropout used on the test set?,Solution: No! only in the train set. Dropout is a regularization technique that is applied in the training process.
220,  Explain why dropout in a neural network acts as a regularizer.,"Solution: There are several (related) explanations to why dropout works. It can be seen as a form of model averaging — at each step we “turn off” a part of the model and average the models we get. It also adds noise, which naturally has a regularizing effect. It also leads to more sparsity of the weights and essentially prevents co-adaptation of neurons in the network."
221,  Give examples in which a many-to-one RNN architecture is appropriate.,"Solution: A few examples are: sentiment analysis, gender recognition from speech."
222,  When can’t we use BiLSTM? Explain what assumption has to be made.,"Solution: in any bi-directional model, we assume that we have access to the next elements of the sequence in a given “time”. This is the case for text data (i.e sentiment analysis, translation etc.), but not the case for time-series data."
223,  True/false: adding L2 regularization to a RNN can help with the vanishing gradient problem.,"Solution: false! Adding L2 regularization will shrink the weights towards zero, which can actually make the vanishing gradients worse in some cases."
224,  Suppose the training error/cost is high and that the validation cost/error is almost equal to it. What does it mean? What should be done?,"Solution: this indicates underfitting. One can add more parameters, increase the complexity of the model, or lower the regularization."
225,  Describe how L2 regularization can be explained as a sort of a weight decay.,"Solution: Suppose our cost function is C(w), and that we add a penalization c|w|2 . When using gradient descent, the iterations will look likew = w -grad(C)(w) — 2cw = (1–2c)w — grad(C)(w)In this equation, the weight is multiplied by a factor < 1."
226,  What is the difference between data science and big data?,Ans. The common differences between data science and big data are –Big DataData ScienceThis question is among the basic data science interview questions and you must prepare for such questions.You may also be interested in exploring: 
227,  How do you check for data quality?,Ans. Some of the definitions used to check for data quality are: Completeness Consistency Uniqueness Integrity Conformity Accuracy  
228,"  Suppose you are given survey data, and it has some missing data, how would you deal with missing values ​​from that survey?","Ans. This is among the important data science interview questions. There are two main techniques for dealing with missing values – Debugging Techniques – It is a Data Cleaning process consisting of evaluating the quality of the information collected, increasing its quality, in order to avoid lax analysis. The most popular debugging techniques are – Searching the list of values: It is about searching the data matrix for values ​​that are outside the response range. These values ​​can be considered as missing, or the correct value can be estimated from other variablesFiltering questions: It is about comparing the number of responses of a filter category and another filtered category. If any anomaly is observed that cannot be solved, it will be considered as a lost value.Checking for Logical Consistencies: The answers that may be considered contradictory to each other are checked.Counting the Level of representativeness: A count is made of the number of responses obtained in each variable. If the number of unanswered questions is very high, it is possible to assume equality between the answers and the non-answers or to make an imputation of the non-answer. Imputation Technique This technique consists of replacing the missing values ​​with valid values ​​or answers by estimating them. There are three types of imputation: Random imputation Hot Deck imputation  Imputation of the mean of subclasses "
229,  How would you deal with missing random values ​​from a data set?,"Ans. There are two forms of randomly missing values:MCAR or Missing completely at random. Such errors happen when the missing values are randomly distributed across all observations. We can confirm this error by partitioning the data into two parts –After we have partitioned the data, we conduct a t-test of mean difference to check if there is any difference in the sample between the two data sets.In case the data are MCAR, we may choose a pair-wise or a list-wise deletion of missing value cases.   MAR or Missing at random. It is a common occurrence. Here, the missing values are not randomly distributed across observations but are distributed within one or more sub-samples. We cannot predict the probability from the variables in the model. Data imputation is mainly performed to replace them."
230,"  What is Hadoop, and why should I care?",Ans. Hadoop is an open-source processing framework that manages data processing and storage for big data applications running on pooled systems.Apache Hadoop is a collection of open-source utility software that makes it easy to use a network of multiple computers to solve problems involving large amounts of data and computation. It provides a software framework for distributed storage and big data processing using the MapReduce programming model.Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packets of code to nodes to process the data in parallel. This allows the data set to be processed faster and more efficiently than if conventional supercomputing architecture were used.
231,  What is ‘fsck’?,"Ans. ‘fsck ‘ abbreviation for ‘ file system check.’ It is a type of command that searches for possible errors in the file. fsck generates a summary report, which lists the file system’s overall health and sends it to the Hadoop distributed file system.This is among the important data science interview questions and you must prepare for the related terminologies as well."
232,  Which is better – good data or good models?,"Ans. This might be one of the frequently asked data science interview questions.The answer to this question is very subjective and depends on the specific case. Big companies prefer good data; it is the foundation of any successful business. On the other hand, good models couldn’t be created without good data.Based on your personal preference, you will probably choose no right or wrong answer (unless the company requires one specifically)."
233,  What are Recommender Systems?,"Ans. Recommender systems are a subclass of information filtering systems, used to predict how users would rate or score particular objects (movies, music, merchandise, etc.). Recommender systems filter large volumes of information based on the data provided by a user and other factors, and they take care of the user’s preference and interest.Recommender systems utilize algorithms that optimize the analysis of the data to build the recommendations. They ensure a high level of efficiency as they can associate elements of our consumption profiles such as purchase history, content selection, and even our hours of activity, to make accurate recommendations.To know more about the job profile and responsibilities of a Data Scientist, refer to this article on What is Data Scientist?"
234,  What are the different types of Recommender Systems?,"Ans. There are three main types of Recommender systems.Collaborative filtering – Collaborative filtering is a method of making automatic predictions by using the recommendations of other people. There are two types of collaborative filtering techniques – User-User collaborative filtering Item-Item collaborative filtering Content-Based Filtering– Content-based filtering is based on the description of an item and a user’s choices. As the name suggests, it uses content (keywords) to describe the items, and the user profile is built to state the type of item this user likes. Image – Collaborative filtering & Content-based filteringHybrid Recommendation Systems – Hybrid Recommendation engines are a combination of diverse rating and sorting algorithms. A hybrid recommendation engine can recommend a wide range of products to consumers as per their history and preferences with precision."
235,  Differentiate between wide and long data formats.,"Ans. In a wide format, categorical data are always grouped.The long data format is in which there are a number of instances with many variables and subject variables."
236,  What are Interpolation and Extrapolation?,Ans. Interpolation – This is the method to guess data points between data sets. It is a prediction between the given data points.Extrapolation – This is the method to guess data point beyond data sets. It is a prediction beyond given data points.Also Read>>Skills That Employers Look For In a Data Scientist
237,  How much data is enough to get a valid outcome?,"Ans. All the businesses are different and measured in different ways. Thus, you never have enough data and there will be no right answer. The amount of data required depends on the methods you use to have an excellent chance of obtaining vital results."
238,  What is the difference between ‘expected value’ and ‘average value’?,"Ans. When it comes to functionality, there is no difference between the two. However, they are used in different situations.An expected value usually reflects random variables, while the average value reflects the population sample."
239,  What happens if two users access the same HDFS file at the same time?,"Ans. This is a bit of a tricky question. The answer itself is not complicated, but it is easy to confuse by the similarity of programs’ reactions.When the first user is accessing the file, the second user’s inputs will be rejected because HDFS NameNode supports exclusive write."
240,  What is power analysis?,Ans. Power analysis allows the determination of the sample size required to detect an effect of a given size with a given degree of confidence.
241,  Is it better to have too many false negatives or too many false positives?,"Ans. This is among the popularly asked data science interview questions and will depend on how you show your viewpoint. Give examplesThese are some of the popular data science interview questions. Always be prepared to answer all types of data science interview questions— technical skills, interpersonal, leadership, or methodologies. If you are someone who has recently started your career in Data Science, you can always get certified to improve your skills and boost your career opportunities."
242,  What is the importance of statistics in data science?,"Ans. Statistics help data scientists to get a better idea of a customer’s expectations. Using statistical methods, data Scientists can acquire knowledge about consumer interest, behavior, engagement, retention, etc. It also helps to build robust data models to validate certain inferences and predictions."
243,  What are the different statistical techniques used in data science?,"Ans. There are many statistical techniques used in data science, including –The arithmetic mean – It is a measure of the average of a set of dataGraphic display – Includes charts and graphs to visually display, analyze, clarify, and interpret numerical data through histograms, pie charts, bars, etc.Correlation – Establishes and measures relationships between different variablesRegression – Allows identifying if the evolution of one variable affects othersTime series – It predicts future values ​​by analyzing sequences of past valuesData mining and other Big Data techniques to process large volumes of dataSentiment analysis – It determines the attitude of specific agents or people towards an issue, often using data from social networksSemantic analysis – It helps to extract knowledge from large amounts of textsA / B testing – To determine which of two variables works best with randomized experimentsMachine learning using automatic learning algorithms to ensure excellent performance in the presence of big dataCheck Out Our Data Science Courses"
244,  What is an RDBMS? Name some examples for RDBMS?,"Ans.  This is among the most frequently asked data science interview questions.A relational database management system (RDBMS) is a database management system that is based on a relational model.Some examples of RDBMS are MS SQL Server, IBM DB2, Oracle, MySQL, and Microsoft Access.Interviewers often ask such data science interview questions and you must prepare for such abbreviations."
245,"  What are a Z test, Chi-Square test, F test, and T-test?",Ans. Z test is applied for large samples. Z test = (Estimated Mean – Real Mean)/ (square root real variance / n).Chi-Square test is a statistical method assessing the goodness of fit between a set of observed values and those expected theoretically.F-test is used to compare 2 populations’ variances. F = explained variance/unexplained variance.T-test is applied for small samples. T-test = (Estimated Mean – Real Mean)/ (square root Estimated variance / n).
246,  What does P-value signify about the statistical data?,"Ans. The p-value is the probability for a given statistical model that, when the null hypothesis is true, the statistical summary would be the same as or more extreme than the actual observed results.When,P-value>0.05, it denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected.P-value <= 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected.P-value=0.05is the marginal value indicating it is possible to go either way"
247,"  Differentiate between univariate, bivariate, and multivariate analysis.","Ans. Univariate analysis is the simplest form of statistical analysis where only one variable is involved.Bivariate analysis is where two variables are analyzed and in multivariate analysis, multiple variables are examined."
248,  What is association analysis? Where is it used?,Ans. Association analysis is the task of uncovering relationships among data. It is used to understand how the data items are associated with each other.Also Read –  Top 6 Industries Hiring Data Scientists in 2021
249,  What is the difference between squared error and absolute error?,"Ans. Squared error measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated.Absolute error is the difference between the measured or inferred value of a quantity and its actual value."
250,  What is an API? What are APIs used for?,"Ans. API stands for Application Program Interface and is a set of routines, protocols, and tools for building software applications.With API, it is easier to develop software applications."
251,  What is Collaborative filtering?,Ans. Collaborative filtering is a method of making automatic predictions by using the recommendations of other people.
252,  Why do data scientists use combinatorics or discrete probability?,Ans. It is used because it is useful in studying any predictive model.Also Read>>How are Data Scientist and Data Analyst different?
253,  What do you understand by Recall and Precision?,"Ans. Precision is the fraction of retrieved instances that are relevant, while Recall is the fraction of relevant instances that are retrieved.Become Machine Learning Expert Now>>"
254,  What is market basket analysis?,"Ans. Market Basket Analysis is a modeling technique based upon the theory that if you buy a certain group of items, you are more (or less) likely to buy another group of items."
255,  What is the central limit theorem?,"Ans. The central limit theorem states that the distribution of an average will tend to be Normal as the sample size increases, regardless of the distribution from which the average is taken except when the moments of the parent distribution do not exist."
256,  Explain the difference between type I and type II errors.,"Ans. Type I error is the rejection of a true null hypothesis or false-positive finding, while Type II error is the non-rejection of a false null hypothesis or false-negative finding."
257,  What is Linear Regression?,Ans. It is one of the most commonly asked networking interview questions.Linear regression is the most popular type of predictive analysis. It is used to model the relationship between a scalar response and explanatory variables.
258,  What are the limitations of a Linear Model/Regression?,"Ans. Linear models are limited to linear relationships, such as dependent and independent variables Linear regression looks at a relationship between the mean of the dependent variable and the independent variables, and not the extremes of the dependent variable Linear regression is sensitive to univariate or multivariate outliers Linear regression tend to assume that the data are independent "
259,  What is the goal of A/B Testing?,"Ans. A/B testing is a comparative study, where two or more variants of a page are presented before random users and their feedback is statistically analyzed to check which variation performs better."
260,  What is the main difference between overfitting and underfitting?,"Ans. Overfitting – In overfitting, a statistical model describes any random error or noise, and occurs when a model is super complex. An overfit model has poor predictive performance as it overreacts to minor fluctuations in training data. Underfitting – In underfitting, a statistical model is unable to capture the underlying data trend. This type of model also shows poor predictive performance."
261,  What is a Gaussian distribution and how it is used in data science?,Ans. Gaussian distribution or commonly known as bell curve is a common probability distribution curve. Mention the way it can be used in data science in a detailed manner.
262,  Explain the purpose of group functions in SQL. Cite certain examples of group functions.,Ans. Group functions provide summary statistics of a data set. Some examples of group functions are – a) COUNT b) MAX c) MIN d) AVG e) SUM f) DISTINCT
263,  What is Root Cause Analysis?,"Ans. Root Cause is defined as a fundamental failure of a process. To analyze such issues, a systematic approach has been devised that is known as Root Cause Analysis (RCA). This method addresses a problem or an accident and gets to its “root cause”."
264,  What is the difference between a Validation Set and a Test Set? ,"Ans. The validation set is used to minimize overfitting. This is used in parameter selection, which means that it helps to verify any accuracy improvement over the training data set. Test Set is used to test and evaluate the performance of a trained Machine Learning model."
265,  What is the Confusion Matrix?,"Ans. The confusion matrix is ​​a very useful tool to assess how good a classification model based on machine learning is. It is also known as an error matrix and can be presented as a summary table to evaluate the performance of a classification model. The number of correct and incorrect predictions are summarized with the count values ​​and broken down by each class.The confusion matrix serves to show explicitly when one class is confused with another, which allows us to work separately with different types of errors.Positive (P): The observation is positive (for example, it is a dog)Negative (N): The observation is not positive (for example, it is not a dog)True Positive (TP): Result in which the model correctly predicts the positive classTrue Negative (TN): Result where the model correctly predicts the negative classFalse Positive (FP): Also called a type 1 error, a result where the model incorrectly predicts the positive class when it is actually negativeFalse Negative (FN): Also called a type 2 error, a result in which the model incorrectly predicts the negative class when it is actually positive"
266,  What is the p-value?,Ans. A p-value helps to determine the strength of results in a hypothesis test. It is a number between 0 and 1 and Its value determines the strength of the results.
267,  What is the difference between Causation and Correlation?,"Ans. Causation denotes any causal relationship between two events and represents its cause and effects. Correlation determines the relationship between two or more variables. Causation necessarily denotes the presence of correlation, but correlation doesn’t necessarily denote causation."
268,  What is cross-validation?,"Ans. Cross-validation is a technique to assess the performance of a model on a new independent dataset. One example of cross-validation could be – splitting the data into two groups – training and testing data, where you use the testing data to test the model and training data to build the model."
269,  What do you mean by logistic regression?,"Ans. Also known as the logit model, logistic regression is a technique to predict the binary result from a linear amalgamation of predictor variables."
270,  What is ‘cluster sampling’?,"Ans. Cluster sampling is a probability sampling technique where the researcher divides the population into separate groups, called clusters. Then a simple cluster sample is selected from the population. The researcher conducts his analysis of data from the sample pools."
272,  What are the Resampling methods?,"Ans. Resampling methods are used to estimate the precision of the sample statistics, exchanging labels on data points, and validating models."
273,"  What is selection bias, and how can you avoid it?","Ans. Selection bias is an experimental error that occurs when the participant pool, or the subsequent data, is not representative of the target population.Selection biases cannot be overcome with statistical analysis of existing data alone, though Heckman correction may be used in special cases."
274,  What is the binomial distribution?,Ans. A binomial distribution is a discrete probability distribution that describes the number of successes when conducting independent experiments on a random variable.Formula –Where:n = Number of experimentsx = Number of successesp = Probability of successq = Probability of failure (1-p)
275,  What is covariance in statistics?,Ans. Covariance is a measure of the joint variability of two random variables. The covariance between two variables x and y can be calculated as follows:Where: Xi – the values of the X-variable Yj – the values of the Y-variable X̄ – the mean (average) of the X-variable Ȳ – the mean (average) of the Y-variable n – the number of data points 
276,  What is Root Cause Analysis?,Ans. Root Cause Analysis (RCA) is the process of uncovering the root causes of problems to identify appropriate solutions. The RCA assumes that it is much more useful to systematically prevent and resolve underlying issues than just treating symptoms ad hoc and putting out fires.
277,  What is Correlation Analysis?,"Ans. Correlation Analysis is a statistical method to evaluate the strength of the relationship between two quantitative variables. It consists of autocorrelation coefficients, estimated and calculated to make a different spatial relationship. It is used to correlate data based on distance."
278,  What is imputation? List the different types of imputation techniques.,"Ans. Imputation is the process that allows you to replace missing data with other values. Types of imputation techniques include –Single Imputation: Single imputation denotes that the missing value is replaced by a value.Hot-deck: The missing value is imputed from a similar register, which is chosen at random, based on a punched card.Cold deck Imputation: Select donor data from other sets.Mean Imputation: Substitute the stored value for the mean of that variable in other cases.Mean Imputation: Its purpose is to replace the missing value with predicted values ​​of a variable that is based on others.Stochastic Regression: equal to the regression, but adds the mean regression variance to the regression imputation.Multiple Imputation: It is a general approach to the problem of missing data, available in commonly used statistical packages. Unlike single imputation, Multiple Imputation estimates the values ​​multiple times."
279,  What is the difference between a bar graph and a histogram?,"Ans. Bar charts and histograms can be used to compare the sizes of the different groups. A bar chart is made up of bars plotted on a chart. A histogram is a graph that represents a frequency distribution; the heights of the bars represent observed frequencies. In other words, a histogram is a graphical display of data using bars of different heights. Generally, there is no space between adjacent bars.Bar Charts The columns are placed on a label that represents a categorical variable. The height of the column indicates the size of the group defined by the categories Histogram The columns are placed on a label that represents a quantitative variable. The column label can be a single value or a range of values. In bar charts, each column represents a group defined by a categorical variable; and with histograms, each column represents a group defined by a quantitative variable."
280,  Name some of the prominent resampling methods in data science.,"Ans. The Bootstrap, Permutation Tests, Cross-validation, and Jackknife."
281,  What is an Eigenvalue and Eigenvector?,Ans. Eigenvectors are used for understanding linear transformations.Eigenvalue can be referred to as the strength of the transformation in the direction of the eigenvector or the factor by which the compression occurs.
282,  Which technique is used to predict categorical responses?,Ans. Classification techniques are used to predict categorical responses.
283,  What is the importance of Sampling?,Ans. Sampling is a crucial statistical technique to analyze large volumes of datasets. This involves taking out some samples that represent the entire data population. It is imperative to choose samples that are the true representatives of the whole data set. There are two types of sampling methods – Probability Sampling and Non Probability Sampling.
284,  Is it possible to stack two series horizontally? If yes then how will you do it?,"Ans. Yes, it is possible to stack two series horizontally. We can use concat() function and setting axis = 1.df = pd.concat([s1, s2], axis=1)"
285,  Tell me the method to convert date-strings to timeseries in a series.,Ans.Input:We will use the to_datetime() function Explore – Python Online Courses & Certifications
286,  Write a program in Python that takes input as the weight of the coins and produces output as the money value of the coins.,Ans. Here is an example of the code. You can change the values.
287,  Why does Python score high over other programming languages?,"Ans. This is among the very commonly asked data science interview questions. Python has a wealth of data science libraries; it is incredibly fast and easy to read and learn. The Python suite specializing in deep learning and other machine learning libraries includes popular tools such as sci-kit-learn, Keras, and TensorFlow, which allow data scientists to develop sophisticated data models directly integrated into a production system.To discover data revelations, you will need to use Pandas, the data analysis library for Python. It can handle large amounts of data without the lag of Excel. You can do numerical modeling analysis with Numpy, do scientific computation and calculation with SciPy, and access many powerful machine learning algorithms with the Sci-Kit-learn code library. With the Python API and the iPython Notebook that comes with Anaconda, you will have robust options to visualize your data."
288,  What are the data types used in Python?,"Ans. Python has the following built-in data types: Number (float, integer) String Tuple List Set Dictionary Numbers, strings, and tuples are immutable data types, which means that they cannot be modified at run time. Lists, sets, and dictionaries are mutable, which means they can be modified at run time."
289,  What is a Python dictionary?,"Ans. A dictionary is one of the built-in data types in Python. Defines a messy mapping of unique keys to values. Dictionaries are indexed by keys, and the values ​​can be any valid Python data type (even a user-defined class). It should be noted that dictionaries are mutable, which means that they can be modified. A dictionary is created with braces and is indexed using bracket notation.Such common data science interview questions are often asked by the interviewers."
290,  What libraries do data scientists use to plot data in Python?,"Ans. Matplotlib is the main library used to plot data in Python. However, graphics created with this library need a lot of tweaking to make them look bright and professional. For that reason, many data scientists prefer Seaborn, which allows you to create attractive and meaningful charts with just one line of code."
291,  Explain the difference between lists and tuples.,"Ans. Both lists and tuples are made up of elements, which are values ​​of any Python data type. However, these data types have a number of differences:Lists are mutable, while tuples are immutable.Lists are created in brackets (for example, my_list = [a, b, c]), while tuples are in parentheses (for example, my_tuple = (a, b, c)).Lists are slower than tuples."
292,  What are lambda functions?,"Ans. Lambda functions are anonymous functions in Python. They are very useful when you need to define a function that is very short and consists of a single expression. So instead of formally defining the little function with a specific name, body, and return statement, you can write everything in a short line of code using a lambda function."
293,  What is PyTorch?,"Ans. PyTorch is a Python-based scientific computing package designed to perform numerical calculations using the programming of tensors. It also allows its execution on GPU to speed up calculations. PyTorch is used to replace NumPy and process calculations on GPUs and for research and development in the field of machine learning, mainly focused on the development of neural networks.PyTorch is designed to seamlessly integrate with Python and its popular libraries like NumPy and is easier to learn than other Deep Learning frameworks.  PyTorch has a simple Python interface, provides a simple but powerful API, and provides the ability to run models in a production environment, making it a popular deep learning framework."
294,  What are the alternatives to PyTorch?,"Ans. Some of the best-known alternatives to PyTorch are –Tensorflow – Google Brain Team developed Tensorflow, which is a free software designed for numerical computation using graphs.Caffe – Caffe is a machine learning framework designed with the aim of being used in computer vision or image classification. Caffe is popular for its library of training models that do not require any extra implementation.Microsoft CNTK – Microsoft CNTK is the free software framework developed by Microsoft. It is very popular in the area of ​​speech recognition although it can also be used for other fields such as text and images.Theano – Theano is another python library. It helps to define, optimize and evaluate mathematical expressions that involve calculations with multidimensional arrays.Keras – Keras is a high-level API for developing neural networks written in Python. It uses other libraries internally such as Tensorflow, CNTK, and Theano. It was developed to facilitate and speed up the development and experimentation with neural networks.You may consider such data science interview questions to be basic, but such questions are the favorite of interviewers as interviewees often leave behind such data science interview questions while preparing."
295,  What packages are used for data mining in Python and R?,"Ans. There are various packages in Python and R:Python – Orange, Pandas, NLTK, Matplotlib, and Scikit-learn are some of them.R – Arules, tm, Forecast, and GGPlot are some of the packages."
296,  Which would you prefer – R or Python?,"Ans.  One of the most important data science interview questions.Both R and Python have their own pros and cons. R is mainly used when the data analysis task requires standalone computing or analysis on individual servers. Python, when your data analysis tasks need to be integrated with web apps or if statistics code needs to be incorporated into a production database.Read More – What is Python?"
297,  Which package is used to do data import in R and Python? How do you do data import in SAS?,"Ans. In R, RODBC is used for RDBMS data and data.table for fast-import.In SAS, data and sas7bdat are used to import data.In Python, Pandas package and the commands read_csv, read_sql are used for reading data.Must Read – What is Machine Learning?"
298,  What are the various types of classification algorithms?,"Ans. There are 7 types of classification algorithms, including – a) Linear Classifiers: Logistic Regression, Naive Bayes Classifier b) Nearest Neighbor c) Support Vector Machines d) Decision Trees e) Boosted Trees f) Random Forest g) Neural Networks"
299,  What is Gradient Descent?,"Ans. Gradient Descent is a popular algorithm used for training Machine Learning models and find the values of parameters of a function (f), which helps to minimize a cost function."
300,  What is Regularization and what kind of problems does regularization solve?,Ans. Regularization is a technique used in an attempt to solve the overfitting problem in statistical models.It helps to solve the overfitting problem in machine learning.
301,  What is a Boltzmann Machine?,Ans. Boltzmann Machines have a simple learning algorithm that helps to discover interesting features in training data. These machines represent complex regularities and are used to optimize the weights and the quantity for the problems.This is one of the important data science interview questions that you must prepare for your interview. 
302,  What is hypothesis testing?,Ans. Hypothesis testing is an important aspect of any testing procedure in Machine Learning or Data Science to analyze various factors that may have any impact on the outcome of the experiment.
303,   What is Pattern Recognition?,Ans. Pattern recognition is the process of data classification that includes pattern recognition and identification of data regularities. This methodology involves the extensive use of machine learning algorithms.
304,   Where can you use Pattern Recognition?,"Ans. Pattern Recognition has multiple usabilities, across- Bio-Informatics Computer Vision Data Mining Informal Retrieval Statistics Speech Recognition "
305,  What is an Autoencoder?,Ans. These are feedforward learning networks where the input is the same as the output. Autoencoders reduce the number of dimensions in the data to encode it while ensuring minimal error and then reconstruct the output from this representation.Also Explore – Deep Learning Online Courses & Certifications
306,  What is the bias-variance trade-off?,"Ans. Bias – Bias is the difference between the average prediction of a model and the correct value we are trying to predict.Variance – Variance is the variability of model prediction for a given data point or a value that tells us the spread of our data.Models with high variance focus on training data and such models perform very well on training data. On the other hand, a model with high bias doesn’t focus on training data and oversimplifies the model, leading to increased training and test data error.Fig – Optimal balance – Bias vs. Variance (Source – towardsdatascience.com)"
307,  When do you need to update the algorithm in Data science?,Ans. You need to update an algorithm in the following situation: You want your data model to evolve as data streams using infrastructure The underlying data source is changing If it is non-stationarity 
308,  Why should you perform dimensionality reduction before fitting an SVM?,"Ans. These SVMs tend to perform better in reduced space. If the number of features is large as compared to the number of observations, then we should perform dimensionality reduction before fitting an SVM."
309,  Name the different kernels of SVM.,Ans. There are nine types of kernels in SVM. Polynomial kernel Gaussian kernel Gaussian radial basis function (RBF) Laplace RBF kernel Hyperbolic tangent kernel Sigmoid kernel Bessel function of the first kind Kernel ANOVA radial basis kernel Linear splines kernel in one-dimension 
310,  What is the Hierarchical Clustering Algorithm?,"Ans. Hierarchical grouping algorithm combines and divides the groups that already exist, in this way they create a hierarchical structure that presents the order in which the groups are split or merged."
311,  What is ‘Power Analysis’?,Ans. Power Analysis is a type of analysis used to determine what kind of effect a unit will have based simply on its size. Power Analysis can be used to estimate the minimum sample size required for an experiment and is directly related to hypothesis testing. The primary purpose underlying power analysis is to help the investigator determine the smallest sample size that is adequate to detect the effect of a certain test at the desired level of significance.
312,  Have you contributed to any open source project?,"Ans. This question seeks a continuous learning mindset. It also tells the interviewer that a candidate is curious and how well they work as a team. Good data scientists are collaborative people, sharing new ideas, knowledge, and information with each other to keep up with rapidly changing data science.You must say specifically which projects you have worked on and what was their objective. A good answer would also include what you have learned from participating in open source projects."
313,  How to deal with unbalanced data?,"Ans. Machine learning algorithms don’t work well with imbalanced data. We can handle this data in a number of ways –  Using appropriate evaluation metrics for model generated using imbalanced data Resampling the training set through undersampling and oversampling Properly applying cross-validation while using the over-sampling method to address imbalance problems Using more data, primarily by ensembling different resampled datasets Resampling with different ratios, where the best ratio majorly depends on data and models used Clustering the abundant class Designing your own models and be creative in using different techniques and approaches to get the best outcome "
314,  How will you recover the information from a given data set? What are the most common issues in the process of information retrieval?,"Ans. The recovery process is carried out through queries to the database where the structured information is stored, using a suitable interrogation language. It is necessary to take into account the key elements that allow the search to be carried out, determining a greater degree of relevance and precision, such as indexes, keywords, thesauri, and the phenomena that can occur in the process such as noise and documentary silence.One of the most common problems that arise when searching for information is whether what we retrieve is “a lot or a little”, that is, depending on the type of search, a multitude of documents or simply a very small number can be retrieved. This phenomenon is called Silence or Documentary Noise.Documentary silence – These are the documents stored in the database but are unrecovered, because the search strategy has been too specific or because the keywords used are not adequate to define the search.Documentary noise – These are the document recovered by the system but are irrelevant. This usually happens when the search strategy has been defined as too generic."
315,  What is Big Data?,"Ans. Big Data is a set of massive data, a collection of huge in size and exponentially growing data, that cannot be managed, stored, and processed by traditional data management tools.To learn more about Big Data, read our blog – What is Big Data?"
316,  What are some of the important tools used in Big Data analytics?,Ans. The important Big Data analytics tools are – • NodeXL • KNIME • Tableau • Solver • OpenRefine • Rattle GUI • Qlikview
317,  What is a decision tree method?,"Ans. The Decision Tree method is an analytical method that facilitates better decisions making through a schematic representation of the available alternatives. These decisions trees are very helpful when there are risks, costs, benefits, and multiple options involved. The name is derived from the appearance of the model similar to a tree and its use is widespread in the field of decision making under uncertainty (Decision Theory)."
318,  What is the importance of the decision tree method?,"Ans. The decision tree method mitigates the risks of unforeseen consequences and allows you to include smaller details that will lead you to create a step-by-step plan. Once you choose your path, you only need to follow it. Broadly speaking, this is a perfect technique for – Analyzing problems from different perspectives Evaluating all possible solutions Estimating the business costs of each decision Making reasoned decisions with real and existing information about any company Analyzing alternatives and probabilities that result in the success of a business "
319,  How to create a good decision tree?,"Ans. Following steps are involved in developing a good decision tree – Identify the variables of a central problem List all the factors causing the identified problem or risk Prioritize and limit each decision criterion Find and list the factors from highest to lowest importance Establish some clear variables to get some factors that include strengths and weaknesses Generate assumptions in an objective way, taking out their ramifications Select the most relevant alternatives for your business Implement the alternatives consistent with your possible problems and risks Evaluate the effectiveness of the decision "
320,  What is Natural Language Processing?,"Ans. Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret, and manipulate human language. It focuses on the processing of human communications, dividing them into parts, and identifying the most relevant elements of the message. With the Comprehension and Generation of Natural Language, it ensures that machines can understand, interpret and manipulate human language."
321,  Why is natural language processing important?,Ans. NLP helps computers communicate with humans in their language and scales other language-related tasks. It contributes towards structuring a highly unstructured data source.
322,  What is the usage of natural language processing?,"Ans. There are several usages of NLP, including –Content categorization – Generate a linguistics-based summary of the document, including search and indexing, content alerts, and duplication detection.Discovery and modeling of themes – Accurately capture meaning and themes in text collections, and apply advanced analytics to text, such as optimization and forecasting.Contextual extraction – Automatically extract structured information from text-based sources.Sentiment analysis – Identification of mood or subjective opinions in large amounts of text, including sentiment mining and average opinions.Speech-to-text and text-to-speech conversion – Transformation of voice commands into written text and vice versa.Document summarization – Automatic generation of synopses of large bodies of text.Machine-based translation – Automatic translation of text or speech from one language to another."
323,  What is Ensemble Learning?,"Ans. This is among the most commonly asked data science interview questions. Ensemble methods is a machine learning method that contributes to combining base models to create one efficient predictive model. It boosts the overall development of the process. Ensemble learning includes two common techniquesBagging – Bagging includes two machine-learning models, Bootstrapping and Aggregation into a single ensemble model. Here the data set is split for parallel processing of models for accuracy.Boosting – Boosting is a sequential technique where one model is passed to another model with an aim to reduce error and create an efficient model."
324,  What is the main difference between supervised and unsupervised machine learning?,"Ans. Supervised learning includes training labeled data for a range of tasks such as data classification, while unsupervised learning does not require explicitly labeling data."
325,  What is DBSCAN Clustering?,Ans.:  DBSCAN or density-based spatial clustering is an unsupervised approach that splits vectors into different groups basis minimum distance and a number of points in that range. There are two significant parameters in DBSCAN clustering.Epsilon – Minimum radius or distance between the two data pointsMin – Sample Points – Minimum sample number within a range to identify as one cluster.
326,  What is data visualization?,"Ans. Data visualization is the process of presenting datasets and other information through visual mediums like charts, graphs, and others. It enables the user to detect patterns, trends, and correlations that might otherwise go unnoticed in traditional reports, tables, or spreadsheets."
327,  What is Deep Learning?,"Ans. It is among the most frequently asked data science interview questions. Deep Learning is an artificial intelligence function used in decision-making. Deep Learning imitates the human brain’s functioning to process the data and create the patterns used in decision-making. Deep learning is a key technology behind automated driving, automated machine translation, automated game playing, object classification in photographs, and automated handwriting generation, among others.Read More – What is Deep Learning?"
328,  Name different Deep Learning Frameworks.,Ans. a) Caffe b) Chainer c) Pytorch d) TensorFlow e) Microsoft Cognitive Toolkit f) Keras
329,  What do you understand by linear regression?,"Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression. Interested in learning Data Science? Click here to learn more in this Data Science Course!  Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.Interested in learning Data Science? Click here to learn more in this Data Science Course!"
330,  What do you understand by logistic regression?,"Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works."
331,  What is a confusion matrix?,"The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works. The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.CTAWatch this comprehensive Data Science tutorial to learn more:  Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          Watch this comprehensive Data Science tutorial to learn more: Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021        "
332,  What do you understand by true positive rate and false positive rate?,"True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives. Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                             True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                           Get 50% Hike!Master Most in Demand Skills Now !"
333,  What is Data Science?,"Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc."
334,  How is Data Science different from traditional application programming?,"Data Science takes a fundamentally different approach to building systems that provide value than traditional application development. In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc. Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model). As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.   Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies."
335,  Explain the differences between supervised and unsupervised learning.,"Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.     Supervised Learning Unsupervised Learning   Works on the data that contains both inputs and the expected output, i.e., the labeled data Works on the data that contains no mappings from input to output, i.e., the unlabeled data   Used to create models that can be employed to predict or classify things Used to extract meaningful information out of large volumes of data   Commonly used supervised learning algorithms: Linear regression, decision tree, etc. Commonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.      Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems."
336,  What is dimensionality reduction?,"Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.  Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information."
337,  What is bias in Data Science?,"Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.  Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc."
338,  Why Python is used for Data Cleaning in DS?,"Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.  Python libraries such as  Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc.  Learn more about Data Cleaning in Data Science Tutorial! Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc. Python libraries such as  Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. Learn more about Data Cleaning in Data Science Tutorial!"
339,  Why R is used in Data Visualization?,"R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.  It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets.   Career Transition                                                                                                   R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow. It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets.   Career Transition                                                                                                  Career Transition"
340,  What are the popular libraries used in Data Science?,"Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:  TensorFlow: Supports parallel computing with impeccable library management backed by Google.  SciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts. Pandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications. Matplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.  PyTorch: Best for projects which involve Machine Learning algorithms and Deep Neural Networks.   Interested to learn more about Data Science, check out our Data Science Course in New York! Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:Interested to learn more about Data Science, check out our Data Science Course in New York!"
341,  What is variance in Data Science?,"Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting. Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting."
342,  What is pruning in a decision tree algorithm?,"Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.  Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed. "
343,  What is entropy in a decision tree algorithm?,"In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset. For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles. In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles."
344,  What is information gain in a decision tree algorithm?,"When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data.  When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. "
345,  What is k-fold cross-validation?,"In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes. In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes."
346,  Explain how a recommender system works.,"A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform. For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.  Courses you may like      A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.  Courses you may like     Courses you may like"
347,  What is a normal distribution?,"Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution. Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution."
348,  What is Deep Learning?,"Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.  Career Transition                                                                                                   Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.  Career Transition                                                                                                  Career Transition"
349,  What is an RNN (recurrent neural network)?,"A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results. A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results."
350,  Explain selection bias.,"Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.  Intermediate Data Science Interview Questions Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study."
351,  What is ROC curve?,"It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph:  It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: "
352,  What do you understand by a decision tree?,"A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition. Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now! A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!"
353,  What do you understand by a random forest model?,"It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.  It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model."
354,  Two candidates Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/  What is the probability that at least of them will crack the interview?,"The probability of Aman getting selected for the interview is 1/8 P(A) = 1/8 The probability of Mohan getting selected for the interview is 5/12 P(B)=5/12 Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1) Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job. To calculate the final answer, we first have to find out the value of P(A ∩ B) So, P(A ∩ B) = P(A) * P(B) 1/8 * 5/12 5/96 Now, put the value of P(A ∩ B) into equation 1 P(A U B) =P(A)+ P(B) – (P(A ∩ B)) 1/8 + 5/12 -5/96 So, the answer will be 47/96. The probability of Aman getting selected for the interview is 1/8 P(A) = 1/8 The probability of Mohan getting selected for the interview is 5/12 P(B)=5/12Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which meansP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job. To calculate the final answer, we first have to find out the value of P(A ∩ B) So, P(A ∩ B) = P(A) * P(B)1/8 * 5/125/96Now, put the value of P(A ∩ B) into equation 1P(A U B) =P(A)+ P(B) – (P(A ∩ B))1/8 + 5/12 -5/96So, the answer will be 47/96."
355,  How is Data modeling different from Database design?,"Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters. Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters."
356,  What are precision?,"Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:  Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:"
357,  What is recall?,Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:  Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:
358,  What is the F1 score and how to calculate it?,"F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score:  F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: "
359,  What is p-value? ,"P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis. P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis."
360,  Why do we use p-value?,We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:  We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:
361,  What is the difference between an error and a residual error?,"An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error. An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error."
362,  Why do we use the summary function?,"The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better. The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better."
363,  How are Data Science and Machine Learning related to each other?,"Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other. Data Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc. Machine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output. In short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.  Courses you may like      Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.Data Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.Machine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.In short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.  Courses you may like     Courses you may like"
364,"  Explain univariate, bivariate, and multivariate analyses.","When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.  Univariate analysis: Univariate analysis involves analysing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends out of it. Example: Analyzing the weight of a group of people. Bivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude. Multivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable). Example: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc.  When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean."
365,  How can we handle missing data?,"To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up. One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values. Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode. Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way. To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way."
366,  What is the benefit of dimensionality reduction?,"Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy. Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy."
367,  What is bias–variance trade-off in Data Science?,"When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance. Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance. When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance."
368,  What is RMSE?,"RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows: First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate. RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate."
369,  What is a kernel function in SVM?,"In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable. In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable."
370,  How can we select an appropriate value of k in k-means?,"Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance. This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm. Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm."
371,  How can we deal with outliers?,"Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model. In case the outliers are not that extreme, then we can try:  A different kind of model. For example, if we were using a linear model, then we can choose a non-linear model Normalizing the data, which will shift the extreme values closer to other data points Using algorithms that are not so affected by outliers, such as random forest, etc.  Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.In case the outliers are not that extreme, then we can try:"
372,  How to calculate the accuracy of a binary classification algorithm using its confusion matrix?,"In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:  True positives: Number of observations correctly classified as True True negatives: Number of observations correctly classified as False False positives: Number of observations incorrectly classified as True False negatives: Number of observations incorrectly classified as False  To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows: In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:"
373,  What is ensemble learning?,"When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning. When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning."
374,  Explain collaborative filtering in recommender systems.,"Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not. Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not."
375,  Explain content-based filtering in recommender systems.,"Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users. Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users."
376,  Explain bagging in Data Science.,"Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency. Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency."
377,  Explain boosting in Data Science.,"Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well. Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well."
378,  Explain stacking in Data Science.,"Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models. Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models."
379,  Explain how Machine Learning is different from Deep Learning.,"A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed. Deep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers. A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.Deep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers."
380,  Why does Naive Bayes have the word ‘naive’ in it?,"Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred. It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc. To learn more about Data Science, check out our Data Science Course in Hyderabad. Advanced Data Science Interview Questions Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.To learn more about Data Science, check out our Data Science Course in Hyderabad."
381,  Write a function to calculate the Euclidean distance between two points.,"The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows: √(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2)) Code for calculating the Euclidean distance is as given below: def euclidean_distance(P1, P2): return (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5 The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:Code for calculating the Euclidean distance is as given below:"
382,  Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.,"To calculate the root mean square error (RMSE), we have to:   Calculate the errors, i.e., the differences between the actual and the predicted values  Square each of these errors  Calculate the mean of these squared errors  Return the square root of the mean  The code in Python for calculating RMSE is given below: def rmse(actual, predicted):   errors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]   squared_errors = [x ** 2 for x in errors]   mean = sum(squared_errors) / len(squared_errors)   return mean ** .5  Check out this Machine Learning Course to get an in-depth understanding of Machine Learning. To calculate the root mean square error (RMSE), we have to:The code in Python for calculating RMSE is given below:Check out this Machine Learning Course to get an in-depth understanding of Machine Learning."
383,  Mention the different kernel functions that can be used in SVM.,"In SVM, there are four types of kernel functions:  Linear kernel Polynomial kernel Radial basis kernel Sigmoid kernel  In SVM, there are four types of kernel functions:"
384,  How to detect if the time series data is stationary?,"Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary. Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary."
385,  Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.,"We can use the code given below to calculate the accuracy of a binary classification algorithm: def accuracy_score(matrix):   true_positives = matrix[0][0]   true_negatives = matrix[1][1]   total_observations = sum(matrix[0]) + sum(matrix[1])   return (true_positives + true_negatives) / total_observations  We can use the code given below to calculate the accuracy of a binary classification algorithm:"
386,  What does root cause analysis mean?,"Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas. Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas."
387,  What is A/B testing?,"A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product. Check out this Python Course to get deeper into Python programming. A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.Check out this Python Course to get deeper into Python programming."
388,"  Out of collaborative filtering and content-based filtering, which one is considered better, and why?","Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides. In the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users. Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.In the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users."
389,  Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.,"We can use the below for this purpose: def calculate_precsion_and_recall(matrix):   true_positive  = matrix[0][0]   false_positive  = matrix[0][1]   false_negative = matrix[1][0]   return {     'precision': (true_positive) / (true_positive + false_positive),     'recall': (true_positive) / (true_positive + false_negative)   }  We can use the below for this purpose:"
390,  What is reinforcement learning?,"Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal. Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal."
391,  Explain TF/IDF vectorization.,The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval. The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.
392,  What are the assumptions required for linear regression?,"There are several assumptions required for linear regression. They are as follows:  The data, which is a sample drawn from a population, used to train the model should be representative of the population. The relationship between independent variables and the mean of dependent variables is linear. The variance of the residual is going to be the same for any value of an independent variable. It is also represented as X. Each observation is independent of all other observations. For any value of an independent variable, the independent variable is normally distributed.  There are several assumptions required for linear regression. They are as follows:"
393,  What happens when some of the assumptions required for linear regression are violated?,"These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model. Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance. "
394,  What do you understand by linear regression?,"Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression. Interested in learning Data Science? Click here to learn more in this Data Science Course!  Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.Interested in learning Data Science? Click here to learn more in this Data Science Course!Interested in learning Data Science? Click here to learn more in this Data Science Course!"
395,  What do you understand by logistic regression?,"Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works. S"
396,  What is a confusion matrix?,"The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works. The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.True Positive (d)False Negative (c)False Positive (b):True Negative (a):CTACTAWatch this comprehensive Data Science tutorial to learn more:  Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          Watch this comprehensive Data Science tutorial to learn more: Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021        "
397,  What do you understand by true positive rate and false positive rate?,"True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives. Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                             True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.True positive rateFormulaFalse positive rateFormulaCheck out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                           Check out this comprehensive Data Science Course in India!Master Most in Demand Skills Now !"
403,  Why Python is used for Data Cleaning in DS?,"Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.  Python libraries such as  Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc.  Learn more about Data Cleaning in Data Science Tutorial! Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc. Python libraries such as  Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. Learn more about Data Cleaning in Data Science Tutorial!Learn more about Data Cleaning in Data Science Tutorial!"
405,  What are the popular libraries used in Data Science?,"Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:  TensorFlow: Supports parallel computing with impeccable library management backed by Google.  SciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts. Pandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications. Matplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.  PyTorch: Best for projects which involve Machine Learning algorithms and Deep Neural Networks.   Interested to learn more about Data Science, check out our Data Science Course in New York! Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:Interested to learn more about Data Science, check out our Data Science Course in New York!Interested to learn more about Data Science, check out our Data Science Course in New York!"
416,  What is ROC curve?,"It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph:  It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: Receiver Operating Characteristic"
417,  What do you understand by a decision tree?,"A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition. Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now! A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!"
419,  Two candidates Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least of them will crack the interview?,"The probability of Aman getting selected for the interview is 1/8 P(A) = 1/8 The probability of Mohan getting selected for the interview is 5/12 P(B)=5/12 Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1) Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job. To calculate the final answer, we first have to find out the value of P(A ∩ B) So, P(A ∩ B) = P(A) * P(B) 1/8 * 5/12 5/96 Now, put the value of P(A ∩ B) into equation 1 P(A U B) =P(A)+ P(B) – (P(A ∩ B)) 1/8 + 5/12 -5/96 So, the answer will be 47/96. The probability of Aman getting selected for the interview is 1/8 P(A) = 1/8 The probability of Mohan getting selected for the interview is 5/12 P(B)=5/12Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which meansP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job. To calculate the final answer, we first have to find out the value of P(A ∩ B) So, P(A ∩ B) = P(A) * P(B)1/8 * 5/125/96Now, put the value of P(A ∩ B) into equation 1P(A U B) =P(A)+ P(B) – (P(A ∩ B))1/8 + 5/12 -5/96So, the answer will be 47/96."
420,  How is Data modeling different from Database design?,"Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters. Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.Data ModelingDatabase Design"
421,  What are precision?,"Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:  Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:Precision"
422,  What is recall?,Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:  Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:Recall
426,  What is the difference between an error and a residual error?,"An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error. An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.errorresidual error"
445,  Why does Naive Bayes have the word ‘naive’ in it?,"Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred. It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc. To learn more about Data Science, check out our Data Science Course in Hyderabad. Advanced Data Science Interview Questions Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.To learn more about Data Science, check out our Data Science Course in Hyderabad.To learn more about Data Science, check out our Data Science Course in Hyderabad."
446,"  Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.","Accuracy is calculated as: Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives) To build a confusion matrix in R, we will use the table function: table(test$target,pred_heart>0.6) Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1. Then, we calculate the accuracy by the formula for calculating Accuracy.  Accuracy is calculated as:Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)To build a confusion matrix in R, we will use the table function:Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.Then, we calculate the accuracy by the formula for calculating Accuracy.Accuracy"
447,  Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.,"First, we will load the pandas dataframe and the customer_churn.csv file: customer_churn=pd.read_csv(“customer_churn.csv”)  After loading this dataset, we can have a glance at the head of the dataset by using the following command: customer_churn.head() Now, we will separate the dependent and the independent variables into two separate objects: x=pd.Dataframe(customer_churn[‘MonthlyCharges’])  y=customer_churn[‘ Churn’]  #Splitting the data into training and testing sets  from sklearn.model_selection import train_test_split  x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0) Now, we will see how to build the model and calculate log_loss. from sklearn.linear_model, we have to import LogisticRegression  l=LogisticRegression()  l.fit(x_train,y_train)  y_pred=l.predict_proba(x_test) As we are supposed to calculate the log_loss, we will import it from sklearn.metrics: from sklearn.metrics import log_loss  print(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred Output: 0.5555020595194167 Become a master of Data Science by going through this online Data Science Course in Toronto! First, we will load the pandas dataframe and the customer_churn.csv file:After loading this dataset, we can have a glance at the head of the dataset by using the following command:Now, we will separate the dependent and the independent variables into two separate objects:Now, we will see how to build the model and calculate log_loss.log_lossAs we are supposed to calculate the log_loss, we will import it from sklearn.metrics:sklearn.metricsOutput:OutputBecome a master of Data Science by going through this online Data Science Course in Toronto!Become a master of Data Science by going through this online Data Science Course in Toronto!"
456,"  In the following confusion matrix, calculate precision and recall.","    Total = 510 Actual   Predicted  P N   P 156 11   N 16 327     The formulae for precision and recall are given below. Precision: (True Positive) / (True Positive + False Positive) Recall: (True Positive) / (True Positive + False Negative) Based on the given data, precision and recall are: Precision: 156 / (156 + 11) = 93.4 Recall: 156 / (156 + 16) = 90.7  The formulae for precision and recall are given below."
461,  What happens when some of the assumptions required for linear regression are violated?,"These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model. Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance. These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance."
462,  Is logistic regression a generative or a descriptive classifier? Why?,"Logistic regression is a descriptive model. Logistic regression learns to classify by knowing what features differentiate two or more classes of objects. For example, to classify between an apple and an orange, it will learn that the orange is orange in color and an apple is not. On the other hand, a generative classifier like a Naive Bayes will store all the classes' critical features and then classify based on the features the test case best fits."
463,  Can you use logistic regression for classification between more than two classes?,"Yes, it is possible to use logistic regression for classification between more than two classes, and it is called multinomial logistic regression. However, this is not possible to implement without modifications to the vanilla logistic regression model."
464,  How do you implement multinomial logistic regression?,"The multinomial logistic classifier can be implemented using a generalization of the sigmoid, called the softmax function. The softmax represents each class with a value in the range (0,1), with all the values summing to 1. Alternatively, you could use the one-vs-all or one-vs-one approach using multiple simple binary classifiers."
465,  Suppose that you are trying to predict whether a consumer will recommend a particular brand of chocolate or not. Let us say your hypothesis function outputs h(x)=0.55 where h(x) is the probability that y=1 (or that a consumer recommends the chocolate) given any input x. Does this mean that the consumer will recommend the chocolate?,"The answer to this question is 'cannot be determined.' And this will remain the case unless you are provided additional data on the decision boundary. Let us say that you set the decision boundary such that y=1 is h(x)≥0.5 and 0; otherwise, then the answer for this question would be a resounding YES. However, if you set the decision boundary (although this is not very common practice) such that y=1 is h(x)≥0.6 and 0, otherwise the answer will be a NO.Get Closer To Your Dream of Becoming a Data Scientist with 70+ Solved End-to-End ML Projects"
466,  Why can't we use the mean square error cost function used in linear regression for logistic regression?,"If we use mean square error in logistic regression, the resultant cost function will be non-convex, i.e., a function with many local minima, owing to the presence of the sigmoid function in h(x). As a result, an attempt to find the parameters using gradient descent may fail to optimize cost function properly. It may end up choosing a local minima instead of the actual global minima."
467,"  If you observe that the cost function decreases rapidly before increasing or stagnating at a specific high value, what could you infer?",A trend pattern of the cost curve exhibiting a rapid decrease before then increasing or stagnating at a specific high value indicates that the learning rate is too high. The gradient descent is bouncing around the global minimum but missing it owing to the larger than necessary step size.
468,  What alternative could you suggest using a for loop (which is time-consuming) when using Gradient Descent to find the optimum parameters for logistic regression?,"One commonly used efficient alternative to using for loop is vectorization, i.e., representing the parameter values to be optimized in a vector. By using this approach, all the vectors can be updated instead of iterating over them in a for loop.Get FREE Access to Machine Learning Example Codes for Data Cleaning, Data Munging, and Data Visualization"
469,  Are there alternatives to find optimum parameters for logistic regression besides using Gradient Descent?,"Yes, Gradient Descent is merely one of the many available optimization algorithms. Other advanced optimization algorithms can often help arrive at the optimum parameters faster and help with scaling for significant machine learning problems. A few such algorithms are Conjugate Gradient, BFGS, and L-BFGS algorithms."
470,  How many binary classifiers would you need to implement one-vs-all for three classes? How does it work?,"You would need three binary classifiers to implement one-vs-all for three classes since the number of binary classifiers is precisely equal to the number of classes with this approach. If you have three classes given by y=1, y=2, and y=3, then the three classifiers in the one-vs-all approach would consist of h(1)(x), which classifies the test cases as 1 or not 1, h(2)(x) which classifies the test cases as 2 or not 2 and so on. You can then take the results together to arrive at the correct classification. For example, with three categories, Cats, Dogs, and Rabbits, to implement the one-vs-all approach, we need to make the following comparisons:    Binary Classification Problem 1: Cats vs. Dogs, Rabbits (or not Cats)    Binary Classification Problem 2: Dogs vs. Cats, Rabbits (or not Dogs)    Binary Classification Problem 3: Rabbits vs. Cats, Dogs (or not Rabbits)Recommended Reading: "
471,  How many binary classifiers would you need to implement one-vs-one for four classes? How does it work?,"To implement one-vs-one for four classes, you will require six binary classifiers. This is because you will need to compare each class with each other class. In general, the formula for calculating the number of binary classifiers b is given as b=(no. of classes * (no. of classes -1))/ 2.Suppose we have four different categories into which we need to classify the weather for a particular day: Sun, Rain, Snow, Overcast. Then to implement the one-vs-one approach, we need to make the following comparisons:    Binary Classification Problem 1: Sun vs. Rain    Binary Classification Problem 2: Sun vs. Snow    Binary Classification Problem 3: Sun vs. Overcast    Binary Classification Problem 4: Rain vs. Snow    Binary Classification Problem 5: Rain vs. Overcast    Binary Classification Problem 6: Snow vs. Overcast"
472,  What is the importance of regularisation? ,"Regularisation is a technique that can help alleviate the problem of overfitting a model. It is beneficial when a large number of parameters are present, which help predict the target function. In these circumstances, it is difficult to select which features to keep manually. Regularisation essentially involves adding coefficient terms to the cost function so that the terms are penalized and are small in magnitude. This helps, in turn, to preserve the overall trends in the data while not letting the model become too complex. These penalties, in effect, restrict the influence a predictor variable can have over the target by compressing the coefficients, thereby preventing overfitting."
473,  Why is the Wald Test useful in logistic regression but not in linear regression? ,"The Wald test, also known as the Wald Chi-Squared Test, is a method to find whether the independent variables in a model are of significance. The significance of variables is decided by whether they contribute to the predictions or not. The variables that add no value to the model can therefore be deleted without risking severe adverse effects to the model. The Wald test is unnecessary in linear regression because it is easy to compare a more complicated model to a simpler model to check the influence of the added independent variables. After all, we can use the R2 value to make this comparison. However, this is not possible with logistic regression as we use Maximum Likelihood Estimate, which uses the previously mentioned method infeasible. The Wald test can be used for many different models, including those with binary variables or continuous variables, and has the added advantage that it only requires estimating one model."
474,  Will the decision boundary be linear or non-linear in logistic regression models? Explain with an example.,"The decision boundary is essentially a line or a plane that demarcates the boundary between the classes to which linear regression classifies the dependent variables. The shape of the decision boundary will depend entirely on the logistic regression model.For logistic regression model given by hypothesis function h(x)=g(Tx)where g is the sigmoid function, if the hypothesis function is h(x)=g(1+2x2+3x3)then the decision boundary is linear. Alternatively, if h(x)=g(1+2x22+3x32)then the decision boundary is non-linear."
475,  What are odds? Why is it used in logistic regression?,"Odds are the ratio of the probability of success to the probability of failure. The odds serve to provide the constant effect a particular predictor or independent variable has on the output prediction. Expressing the effect of a predictor on the likelihood of the target having a particular value through probability does not describe this constant effect. In linear regression models, we often want to measure the unique effect of each independent variable on the output for which the odds are very useful.Get More Practice, More Data Science and Machine Learning Projects, and More guidance.Fast-Track Your Career Transition with ProjectPro"
476,"  Given fair die, what are the odds of occurrence of odd numbers?","The odds of occurrence of odd numbers is 1. There are three odd and three even numbers in a fair die, and therefore, the probability of occurrence of odd numbers is 3/6 or 0.5. Similarly, the odds of occurrence of numbers that are not odd is 0.5. Since odds is the ratio of the probability of success and that of failure, Odds = 0.5/0.5=1."
477,"  In classification problems like logistic regression, classification accuracy alone is not considered a good measure. Why?","Classification accuracy considers both true positives and false positives with equal significance. If this were just another machine learning problem of not too much consequence, this would be acceptable. However, when the problems involve deciding whether to consider a candidate for life-saving treatment, false positives might not be as bad as false negatives. The opposite can also be true in some cases. Therefore, while there is no single best way to evaluate a classifier, accuracy alone may not serve as a good measure."
478,"  It is common practice that when the number of features or independent variables is larger in comparison to the training set, it is common to use logistic regression or support vector machine (SVM) with a linear kernel. What is the reasoning behind this?","It is common to use logistic regression or SVM with a linear kernel because when there are many features with a limited number of training examples, a linear function should be able to perform reasonably well. Besides, there is not enough training data to allow for the training of more complex functions."
479,"  Between SVM and logistic regression, which algorithm is most likely to work better in the presence of outliers? Why?","SVM is capable of handling outliers better than logistic regression. SVM is affected only by the points closest to the decision boundary. Logistic regression, on the other hand, tries to maximize the conditional likelihood of the training data and is therefore strongly affected by the presence of outliers.Access Data Science and Machine Learning Project Code Examples"
480,  Which is the most preferred algorithm for variable selection?,"Lasso is the most preferred for variable selection because it performs regression analysis using a shrinkage parameter where the data is shrunk to a point, and variable selection is made by forcing the coefficients of not so significant variables to be set to zero through a penalty."
481,  What according to you is the method to best fit the data in logistic regression?,Maximum Likelihood Estimation to obtain the model coefficients which relate to the predictors and target.
482,  What do you mean by the Logistic Regression?,"It’s a classification algorithm that is used where the target variable is of categorical nature. The main objective behind Logistic Regression is to determine the relationship between features and the probability of a particular outcome.For Example, when we need to predict whether a student passes or fails in an exam given the number of hours spent studying as a feature, the target variable comprises two values i.e. pass and fail.Therefore, we can solve classification problem statements which is a supervised machine learning technique using Logistic Regression."
483,  Explain the intuition behind Logistic Regression in detail.,"Given:By using the training dataset, we can find the dependent(x) and independent variables(y), so if we can determine the parameters w (Normal) and b (y-intercept), then we can easily find a decision boundary that can almost separate both the classes in a linear fashion.Objective: In order to train a Logistic Regression model, we just need w and b to find a line(in 2D), plane(3D), or hyperplane(in more than 3-D dimension) that can separate both the classes point as perfect as possible so that when it encounters with any new unseen data point, it can easily classify, from which class the unseen data point belongs to.For Example, Let us consider we have only two features as x1 and x2.Let’s take any of the +ve class points (figure below) and find the shortest distance from that point to the plane. Here, the shortest distance is computed using:di = wT*xi / ||w||If weight vector is a unit vector i.e, ||w||=1. Then,di = wT*xiSince w and xi are on the same side of the decision boundary therefore distance will be +ve. Now for a negative point, we have to compute dj = wT*xj. For point xj, distance will be -ve since this point is the opposite side of w.Thus we can conclude, points that are in the same direction of w are considered as +ve points and the points which are in the opposite direction of w are considered as -ve points.Now, we can easily classify the unseen data points as -ve and +ve points. If the value of wT*xi>0, then y =+1 and if value of wT*xi < 0 then y = -1.Now, by observing all the cases above now our objective is that our classifier minimizes the miss-classification error, i.e, we want the values of yi*wT*xi to be greater than 0.In our problem, xi and yi are fixed because these are coming from the dataset.As we change the values of the parameters w, and b the sum will change and we want to find that w and b that maximize the sum given below. To calculate the parameters w and b, we can use the Gradient Descent optimizer. Therefore, the optimization function for logistic regression is:"
484,  What are the odds?,"Odds are defined as the ratio of the probability of an event occurring to the probability of the event not occurring.For Example, let’s assume that the probability of winning a game is 0.02. Then, the probability of not winning is 1- 0.02 = 0.98."
485,  What factors can attribute to the popularity of Logistic Regression?,"Logistic Regression is a popular algorithm as it converts the values of the log of odds which can range from -inf to +inf to a range between 0 and 1.Since logistic functions output the probability of occurrence of an event, they can be applied to many real-life scenarios therefore these models are very popular. "
486,  Is the decision boundary Linear or Non-linear in the case of a Logistic Regression model?,"The decision boundary is a line or a plane that separates the target variables into different classes that can be either linear or nonlinear. In the case of a Logistic Regression model, the decision boundary is a straight line.Logistic Regression model formula = α+1X1+2X2+….+kXk. This clearly represents a straight line.It is suitable in cases where a straight line is able to separate the different classes. However, in cases where a straight line does not suffice then nonlinear algorithms are used to achieve better results."
487,  What is the Impact of Outliers on Logistic Regression?,"The estimates of the Logistic Regression are sensitive to unusual observations such as outliers, high leverage, and influential observations. Therefore, to solve the problem of outliers, a sigmoid function is used in Logistic Regression."
488,  What is the difference between the outputs of the Logistic model and the Logistic function?,"The Logistic model outputs the logits, i.e. log-odds; whereas the Logistic function outputs the probabilities.Logistic model = α+1X1+2X2+….+kXk. Therefore, the output of the Logistic model will be logits.Logistic function = f(z) = 1/(1+e-(α+1X1+2X2+….+kXk)). Therefore, the output of the Logistic function will be the probabilities."
489,  How do we handle categorical variables in Logistic Regression?,"The inputs given to a Logistic Regression model need to be numeric. The algorithm cannot handle categorical variables directly. So, we need to convert the categorical data into a numerical format that is suitable for the algorithm to process.Each level of the categorical variable will be assigned a unique numeric value also known as a dummy variable. These dummy variables are handled by the Logistic Regression model in the same manner as any other numeric value. 10. Which algorithm is better in the case of outliers present in the dataset i.e., Logistic Regression or SVM?SVM (Support Vector Machines) handles the outliers in a better manner than the Logistic Regression.Logistic Regression: Logistic Regression will identify a linear boundary if it exists to accommodate the outliers. To accommodate the outliers, it will shift the linear boundary.SVM: SVM is insensitive to individual samples. So, to accommodate an outlier there will not be a major shift in the linear boundary. SVM comes with inbuilt complexity controls, which take care of overfitting, which is not true in the case of Logistic Regression."
490,  Can we solve the multiclass classification problems using Logistic Regression? If Yes then How?,"Yes, in order to deal with multiclass classification using Logistic Regression, the most famous method is known as the one-vs-all approach. In this approach, a number of models are trained, which is equal to the number of classes. These models work in a specific way.For Example, the first model classifies the datapoint depending on whether it belongs to class 1 or some other class(not class 1); the second model classifies the datapoint into class 2 or some other class(not class 2) and so-on for all other classes.So, in this manner, each data point can be checked over all the classes."
491,  How can we express the probability of a Logistic Regression model as conditional probability?,"We define probability P(Discrete value of Target variable | X1, X2, X3…., Xk) as the probability of the target variable that takes up a discrete value (either 0 or 1 in the case of binary classification problems) when the values of independent variables are given.For Example, the probability an employee will attain (target variable) given his attributes such as his age, salary, etc."
492,  Discuss the space complexity of Logistic Regression.,"During training: We need to store four things in memory: x, y, w, and b during training a Logistic Regression model.Therefore, the space complexity of Logistic Regression while training is O(nd + n +d).During Runtime or Testing: After training the model what we just need to keep in memory is w. We just need to perform wT*xi to classify the points.Hence, the space complexity during runtime is in the order of d, i.e, O(d)."
493,  Discuss the Test or Runtime complexity of Logistic Regression.,"At the end of the training, we test our model on unseen data and calculate the accuracy of our model. At that time knowing about runtime complexity is very important. After the training of Logistic Regression, we get the parameters w and b.To classify any new point, we have to just perform the operation wT * xi. If wT*xi>0, the point is +ve, and if wT*xi < 0, the point is negative. As w is a vector of size d, performing the operation wT*xi takes O(d) steps as discussed earlier.Therefore, the testing complexity of the Logistic Regression is O(d).Hence, Logistic Regression is very good for low latency applications, i.e, for applications where the dimension of the data is small."
494,  Why is Logistic Regression termed as Regression and not classification?,"The major difference between Regression and classification problem statements is that the target variable in the Regression is numerical (or continuous) whereas in classification it is categorical (or discrete).Logistic Regression is basically a supervised classification algorithm. However, the Logistic Regression builds a model just like linear regression in order to predict the probability that a given data point belongs to the category numbered as “1”.For Example, Let’s have a binary classification problem, and ‘x’ be some feature and ‘y’ be the target outcome which can be either 0 or 1.The probability that the target outcome is 1 given its input can be represented as:If we predict the probability by using linear Regression, we can describe it as:where, p(x) = p(y=1|x)Logistic regression models generate predicted probabilities as any number ranging from neg to pos infinity while the probability of an outcome can only lie between 0< P(x)<1.However, to solve the problem of outliers, a sigmoid function is used in Logistic Regression. The Linear equation is put in the sigmoid function."
495,  Discuss the Train complexity of Logistic Regression.,"In order to train a Logistic Regression model, we just need w and b to find a line(in 2-D), plane(in 3-D), or hyperplane(in more than 3-D dimension) that can separate both the classes point as perfect as possible so that when it encounters with any new point, it can easily classify, from which class the unseen data point belongs to.The value of w and b should be such that it maximizes the sum yi*wT*xi > 0.Now, let’s calculate its time complexity in terms of Big O notation:Therefore, the overall time complexity of the Logistic Regression during training is n(O(d))=O(nd)."
496,  Why can’t we use Mean Square Error (MSE) as a cost function for Logistic Regression?,"In Logistic Regression, we use the sigmoid function to perform a non-linear transformation to obtain the probabilities. If we square this nonlinear transformation, then it will lead to the problem of non-convexity with local minimums and by using gradient descent in such cases, it is not possible to find the global minimum. As a result, MSE is not suitable for Logistic Regression.So, in the Logistic Regression algorithm, we used Cross-entropy or log loss as a cost function. The property of the cost function for Logistic Regression is that:By optimizing this cost function, convergence is achieved."
497,  Can you explain cost function of decision trees?,"Answer: Before we answer this question, it is important to note that Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. Hence their cost functions are also different.Cost function for classification type problems:Gini impurity is an important concept before we can understand cost function, so let me first explain that.where “p” is the ratio of class k instances among the training instances at the ith node. What does that mean? Let’s understand from an example below. Figure I displays a simple visualization of Iris Decision Tree of depth 2. Top level is the root node. The concept of dividing a training set into a set of decisions is fairly simple in the algorithm. Here, for instance, Iris data set is divided into two subsets based on a single feature called “petal width” at the root node. If petal width is less than or equal to 0.8, then the algorithm goes to depth 1, left. If not, it goes to depth 1, right. Where it further divides instances based on an additional feature of “petal width”. Depth 1, right node has a sample of 100 instances and applies 0 instances to Iris-Setosa, 50 instances to Iris-Versicolor and remaining 50 to Iris-Virginica.So this node’s gini impurity is 0.5:Similarly, at depth 1, left node, Gini impurity is zero because all training instances apply to the same class. The node is essentially “pure”.Now that understand what is Gini impurity, let’s get into the meat of the answer. Decision Trees use Classification and Regression Tree (CART) algorithm for training purposes based on a simple concept that the data set will be split into two subsets using a single feature (k) and threshold (t) . In Iris data set the feature was “petal width” and threshold was 0.8. How does it choose k and t? It searches for the pair (k, t) that produces the purest subsets. So the cost function that the algorithm tries to minimize is given by below equation:where G left or right represent gini impurity of subsets while m represents instances of the subsets.Cost function for regression type problems:For regression trees, cost function is fairly intuitive. We use Residual Sum of Squares (RSS). Equation III displays cost function of regression type trees, where “y” is ground truth and “y-hat” is the predicted value."
498,  How does collinearity affect your models?,"Answer: Collinearity refers to a situation where two or more predictor variables are closely related to one another. Figure 2 below shows as example of collinear variables. Variable 2 strictly follows variable 1 with a Pearson correlation coefficient of 1. So obviously one of these variables will behave like noise when fed into machine learning models.The presence of collinearity can become problematic in regression type questions, since it becomes difficult to separate out the individual effects of collinear variables on the response. Or in other words collinearity reduces the accuracy of the estimates of the regression coefficients and results in increase in errors. This will ultimately lead to decline in the t-statistic, as a result, in the presence of collinearity we may fail to reject the null hypothesis.A simple way to detect collinearity is to look at the correlation matrix of the predictor variables. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore collinearity problem with the data. Unfortunately not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for the collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. Such a situation is called multi-collinearity. For such cases, instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF for each variable can be computed using the formula:Where R-square term is the regression of variable X, onto all of the other predictors. If VIF is close to or more than one then collinearity is present. When faced with the problem of collinearity there are two possible solutions. One is to drop the redundant variable. This can be done without compromising the regression fit. The second solution is to combine the collinear variables together into a single predictor."
499,  How will you explain deep neural network to a layman?,"Answer: The idea of Neural Network (NN) originally stemmed from human brain that is designed to identify patterns. NN is a set of algorithms that interpret sensory data through machine perception, labeling and clustering raw input data. Any type of real world data, be it images, text, sound or even time series data must be converted into a vector space containing numbers.The word “deep” in deep neural network means that the neural network consists of multiple layers. These layers are made of nodes where computation takes place. An analogy of node is a neuron in human brain which fires when it encounters sufficient stimuli. A node combines data from the raw input with their coefficients or weights that either dampen or amplify that input based on the weight. The product of input and weight is then summed at the summation node shown below in Fig. 3, which is then passed on to the activation function which determines whether and to what extend that signal should progress further through the network to affect the ultimate outcome. A node layer is a row of such neuron-like switches that turn on or off as the input is fed through the network.Deep neural networks differs from earlier version of neural networks such as perceptrons because they were shallow and simply consisted of input and output layer along with one hidden layer."
500,  What would be a 3-minute pitch for your data science take home project?,"Answer: A typical data science interview process starts with a take home data analysis project. I have taken two of those and time span may vary depending on the complexity of the take home project. First time, I was given two days to solve a problem using machine learning along with an executive summary. While the second time I was given two weeks to solve a problem. It is needless to point out that second time it was a much harder problem where I was dealing with class imbalanced data set. So 3-minutes sales pitch type interview question allows you to showcase your grasp of the problem at hand. Please be sure to start with what is your interpretation of the problem; your brief approach to solving the problem; what types of machine learning models did you use in your approach and why? And close this by boasting about the accuracy of your models.I believe that this is a very important question during your interview that enables you to prove that you are a leader in Data Science field and can solve a complex problem with the latest and greatest tools at your disposal."
501,  What do you mean by model regularization and how will you achieve regularization in linear models?,"Answer: Regularization is a term used for constraining your machine learning model. One good way to constrain or reduce overfitting in machine learning models is to have fewer degrees of freedom. With fewer degrees of freedom, it gets harder for the model to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees of freedom. However, for linear models, regularization is typically achieved by constraining the weights of the model. So, instead of linear regression, Ridge regression, Lasso Regression and Elastic Net models have three different ways to constraint the weights. For the sake of completeness, lets start with the definition of linear regression first:Mean Square Error cost function for a Linear Regression model is defined as:Where thetaT is the transpose of theta (a row vector instead of column vector).Ridge Regression: Is a regularized version of Linear Regression, i.e., an additional regularization term is added to the cost function. This forces the learning algorithms to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.The hyperparameter alpha controls how much you want to regularize the model. If alpha is zero, then ridge regression is just linear regression.Lasso Regression: Least Absolute Shrinkage and Selection Operator Regression (simple called Lasso Regression) is another regularized version of Linear Regression: Just like the Ridge Regression, it adds a regularization term to the cost function, but it uses the L1 norm of the weight vector instead of half the square of the L2 norm .An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with a few non-zero feature weights).Elastic Net Regression: This is a middle ground between Ridge and Lasso regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization term and can be controlled with “r”. When r=0, Elastic Net is equivalent to Ridge Regression, and when r=1, it is equivalent to Lasso Regression.It is always preferable to have at least a little bit of regularization and generally plain linear regression should always be avoided. Ridge is a good default, but if only a few features are useful in a particular data set, then Lasso should be used. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of instances or when several features are strongly correlated."
502,What is a Decision Tree?,A decision tree is a tree in which every node specifies a test of some attribute of the data and each branch descending from that node corresponds to one of the possible values for this attribute.
503,On what basis is an attribute selected in the decision tree for choosing it as a node?,Attribute selection is done using Information Gain in decision trees. The attribute with maximum information gain is selected.
504,What is Information Gain? What are its disadvantages?,"Information gain is the reduction in entropy due to the selection of an attribute. Information gain ratio biases the decision tree against considering attributes with a large number of distinct values which might lead to overfitting. In order to solve this problem, information gain ratio is used."
505,What is the inductive bias of decision trees?,Shorter trees are preferred over longer trees. Trees that place high information gain attributes close to the root are preferred over those that do not.
506,How does a decision tree handle continuous attributes?,By converting continuous attributes to a threshold-based boolean attribute. The threshold is decided by maximizing the information gain.
507,How does a decision tree handle missing attribute values?,One way to assign the most common value of that attribute to the missing attribute value. The other way is to assign a probability to each of the possible values of the attribute based on other samples.
508,Name some algorithms used for deriving decision trees?,ID3 and C4.5
509,  What is the KNN Algorithm?,"KNN(K-nearest neighbours) is a supervised learning and non-parametric algorithm that can be used to solve both classification and regression problem statements.It uses data in which there is a target column present i.e, labelled data to model a function to produce an output for the unseen data. It uses the euclidean distance formula to compute the distance between the data points for classification or prediction.The main objective of this algorithm is that similar data points must be close to each other so it uses the distance to calculate the similar points that are close to each other.Image Source: Google Images"
510,  Why is KNN a non-parametric Algorithm?,"The term “non-parametric” refers to not making any assumptions on the underlying data distribution. These methods do not have any fixed numbers of parameters in the model.Similarly in KNN, the model parameters grow with the training data by considering each training case as a parameter of the model. So, KNN is a non-parametric algorithm."
511,  What is “K” in the KNN Algorithm?,"K represents the number of nearest neighbours you want to select to predict the class of a given item, which is coming as an unseen dataset for the model. "
512,  Why is the odd value of “K” preferred over even values in the KNN Algorithm?,"The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting. If the square root of a number of data points is even, then add or subtract 1 to it to make it odd. "
513,  How does the KNN algorithm make the predictions on the unseen dataset?,"The following operations have happened during each iteration of the algorithm. For each of the unseen or test data point, the kNN classifier must:Step-1: Calculate the distances of test point to all points in the training set and store themStep-2: Sort the calculated distances in increasing orderStep-3: Store the K nearest points from our training datasetStep-4: Calculate the proportions of each classStep-5: Assign the class with the highest proportionImage Source: Google Images"
514,  Is Feature Scaling required for the KNN Algorithm? Explain with proper justification.,"Yes, feature scaling is required to get the better performance of the KNN algorithm.For Example, Imagine a dataset having n number of instances and N number of features. There is one feature having values ranging between 0 and 1. Meanwhile, there is also a feature that varies from -999 to 999. When these values are substituted in the formula of Euclidean Distance, this will affect the performance by giving higher weightage to variables having a higher magnitude. "
515,  What is space and time complexity of the KNN Algorithm?,"Time complexity:The distance calculation step requires quadratic time complexity, and the sorting of the calculated distances requires an O(N log N) time. Together, we can say that the process is an O(N3 log N) process, which is a monstrously long process.Space complexity:Since it stores all the pairwise distances and is sorted in memory on a machine, memory is also the problem. Usually, local machines will crash, if we have very large datasets."
516,  Can the KNN algorithm be used for regression problem statements?,"Yes, KNN can be used for regression problem statements.In other words, the KNN algorithm can be applied  when the dependent variable is continuous. For regression problem statements, the predicted value is given by the average of the values of its k nearest neighbours."
517,  Why is the KNN Algorithm known as Lazy Learner?,"When the KNN algorithm gets the training data, it does not learn and make a model, it just stores the data. Instead of finding any discriminative function with the help of the training data, it follows instance-based learning and also uses the training data when it actually needs to do some prediction on the unseen datasets.As a result, KNN does not immediately learn a model rather delays the learning thereby being referred to as Lazy Learner."
518,  Why is it recommended not to use the KNN Algorithm for large datasets?,"The Problem in processing the data:KNN works well with smaller datasets because it is a lazy learner. It needs to store all the data and then make a decision only at run time. It includes the computation of distances for a given point with all other points. So if the dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm.Sensitive to noise:Another thing in the context of large datasets is that there is more likely a chance of noise in the dataset which adversely affects the performance of the KNN algorithm since the KNN algorithm is sensitive to the noise present in the dataset."
519,  How to handle categorical variables in the KNN Algorithm?,"To handle the categorical variables we have to create dummy variables out of a categorical variable and include them instead of the original categorical variable. Unlike regression, create k dummies instead of (k-1).For example, a categorical variable named “Degree” has 5 unique levels or categories. So we will create 5 dummy variables. Each dummy variable has 1 against its degree and else 0."
520,  How to choose the optimal value of K in the KNN Algorithm?,"There is no straightforward method to find the optimal value of K in the KNN algorithm.You have to play around with different values to choose which value of K should be optimal for my problem statement. Choosing the right value of K is done through a process known as Hyperparameter Tuning.The optimum value of K for KNN is highly dependent on the data itself. In different scenarios, the optimum K may vary. It is more or less a hit and trial method.There is no one proper method of finding the K value in the KNN algorithm. No method is the rule of thumb but you should try the following suggestions:1. Square Root Method: Take the square root of the number of samples in the training dataset and assign it to the K value.2. Cross-Validation Method: We should also take the help of cross-validation to find out the optimal value of K in KNN. Start with the minimum value of k i.e, K=1, and run cross-validation, measure the accuracy, and keep repeating till the results become consistent.As the value of K increases, the error usually goes down after each one-step increase in K, then stabilizes, and then raises again. Finally, pick the optimum K at the beginning of the stable zone. This technique is also known as the Elbow Method. Image Source: Google Images3. Domain Knowledge: Sometimes with the help of domain knowledge for a particular use case we are able to find the optimum value of K (K should be an odd number).I would therefore suggest trying a mix of all the above points to reach any conclusion."
521,  How can you relate KNN Algorithm to the Bias-Variance tradeoff?,"Problem with having too small K:The major concern associated with small values of K lies behind the fact that the smaller value causes noise to have a higher influence on the result which will also lead to a large variance in the predictions.Problem with having too large K:The larger the value of K, the higher is the accuracy. If K is too large, then our model is under-fitted. As a result, the error will go up again. So, to prevent your model from under-fitting it should retain the generalization capabilities otherwise there are fair chances that your model may perform well in the training data but drastically fail in the real data. The computational expense of the algorithm also increases if we choose the k very large.So, choosing k to a large value may lead to a model with a large bias(error).The effects of k values on the bias and variance is explained below :So, there is a tradeoff between overfitting and underfitting and you have to maintain a balance while choosing the value of K in KNN. Therefore, K should not be too small or too large.                                                                              Image Source: Google Images"
522,  Which algorithm can be used for value imputation in both categorical and continuous categories of data?,"KNN is the only algorithm that can be used for the imputation of both categorical and continuous variables. It can be used as one of many techniques when it comes to handling missing values.To impute a new sample, we determine the samples in the training set “nearest” to the new sample and averages the nearby points to impute. A Scikit learn library of Python provides a quick and convenient way to use this technique.Note: NaNs are omitted while distances are calculated. Hence we replace the missing values with the average value of the neighbours. The missing values will then be replaced by the average value of their “neighbours”. 15. Explain the statement- “The KNN algorithm does more computation on test time rather than train time”.The above-given statement is absolutely true.The basic idea behind the kNN algorithm is to determine a k-long list of samples that are close to a sample that we want to classify. Therefore, the training phase is basically storing a training set, whereas during the prediction stage the algorithm looks for k-neighbours using that stored data. Moreover, KNN does not learn anything from the training dataset as well."
523,  What are the things which should be kept in our mind while choosing the value of k in the KNN Algorithm?,"If K is small, then results might not be reliable because the noise will have a higher influence on the result. If K is large, then there will be a lot of processing to be done which may adversely impact the performance of the algorithm.So, the following things must be considered while choosing the value of K:"
524,  What are the advantages of the KNN Algorithm?,"Some of the advantages of the KNN algorithm are as follows:1. No Training Period: It does not learn anything during the training period since it does not find any discriminative function with the help of the training data. In simple words, actually, there is no training period for the KNN algorithm. It stores the training dataset and learns from it only when we use the algorithm for making the real-time predictions on the test dataset.As a result, the KNN algorithm is much faster than other algorithms which require training. For Example, SupportVector Machines(SVMs), Linear Regression, etc.Moreover, since the KNN algorithm does not require any training before making predictions as a result new data can be added seamlessly without impacting the accuracy of the algorithm.2. Easy to implement and understand: To implement the KNN algorithm, we need only two parameters i.e. the value of K and the distance metric(e.g. Euclidean or Manhattan, etc.). Since both the parameters are easily interpretable therefore they are easy to understand."
525,  What are the disadvantages of the KNN Algorithm?,"Some of the disadvantages of the KNN algorithm are as follows:1. Does not work well with large datasets: In large datasets, the cost of calculating the distance between the new point and each existing point is huge which decreases the performance of the algorithm.2. Does not work well with high dimensions: KNN algorithms generally do not work well with high dimensional data since, with the increasing number of dimensions, it becomes difficult to calculate the distance for each dimension.3. Need feature scaling: We need to do feature scaling (standardization and normalization) on the dataset before feeding it to the KNN algorithm otherwise it may generate wrong predictions.4. Sensitive to Noise and Outliers: KNN is highly sensitive to the noise present in the dataset and requires manual imputation of the missing values along with outliers removal."
526,  Is it possible to use the KNN algorithm for Image processing? ,"Yes, KNN can be used for image processing by converting a 3-dimensional image into a single-dimensional vector and then using it as the input to the KNN algorithm."
527,  What are the real-life applications of KNN Algorithms?,"The various real-life applications of the KNN Algorithm includes:1. KNN allows the calculation of the credit rating. By collecting the financial characteristics vs. comparing people having similar financial features to a database we can calculate the same. Moreover, the very nature of a credit rating where people who have similar financial details would be given similar credit ratings also plays an important role. Hence the existing database can then be used to predict a new customer’s credit rating, without having to perform all the calculations.2. In political science: KNN can also be used to predict whether a potential voter “will vote” or “will not vote”, or to “vote Democrat” or “vote Republican” in an election."
528,  What are Support Vector Machines (SVMs)?,"👉 SVM is a supervised machine learning algorithm that works on both classification and regression problem statements.👉 For classification problem statements, it tries to differentiate data points of different classes by finding a hyperplane that maximizes the margin between the classes in the training data.👉 In simple words, SVM tries to choose the hyperplane which separates the data points as widely as possible since this margin maximization improves the model’s accuracy on the test or the unseen data.Image Source: link"
529,  What are Support Vectors in SVMs?,"👉 Support vectors are those instances that are located on the margin itself. For SVMS, the decision boundary is entirely determined by using only the support vectors.👉 Any instance that is not a support vector (not on the margin boundaries) has no influence whatsoever; you could remove them or add more instances, or move them around, and as long as they stay off the margin they won’t affect the decision boundary.👉 For computing the predictions, only the support vectors are involved, not the whole training set."
530,  What is the basic principle of a Support Vector Machine?,"It’s aimed at finding an optimal hyperplane that is linearly separable, and for the dataset which is not directly linearly separable, it extends its formulation by transforming the original data to map into a new space, which is also called kernel trick."
531,  What are hard margin and soft Margin SVMs?,"👉  Hard margin SVMs work only if the data is linearly separable and these types of SVMs are quite sensitive to the outliers.👉  But our main objective is to find a good balance between keeping the margins as large as possible and limiting the margin violation i.e. instances that end up in the middle of margin or even on the wrong side, and this method is called soft margin SVM."
532,  What do you mean by Hinge loss?,"The function defined by max(0, 1 – t) is called the hinge loss function.Image Source: linkProperties of Hinge loss function:👉 It is equal to 0 when the value of t is greater than or equal to 1 i.e, t>=1.👉 Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1.👉 It is not differentiable at t = 1.👉 It penalizes the model for wrongly classifying the instances and increases as far the instance is classified from the correct region of classification."
533,  What is the “Kernel trick”?,👉 A Kernel is a function capable of computing the dot product of instances mapped in higher dimension space without actually transforming all the instances into the higher feature space and calculating the dot product.👉 This trick makes the whole process much less computationally expensive than that actual transformation to calculate the dot product and this is the essence of the kernel trick.Image Source: link
534,  What is the role of the C hyper-parameter in SVM? Does it affect the bias/variance trade-off?,"👉 The balance between keeping the margins as large as possible and limiting the margin violation is controlled by the C parameter: a small value leads to a wider street but more margin violation and a higher value of C makes fewer margin violations but ends up with a smaller margin and overfitting.👉 Here thing becomes a little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make W as small as possible to increase the margin. This is where the role of the C hyperparameter comes in which allows us to define the trade-off between these two objectives.8. Explain different types of kernel functions.A function is called kernel if there exist a function ϕ that maps a and b into another space such that K(a, b) = ϕ(a)T · ϕ(b). So you can use K as a kernel since you just know that a mapping ϕ exists, even if you don’t know what ϕ function is. These are the very good things about kernels.Some of the kernel functions are as follows:👉 Polynomial Kernel: These are the kernel functions that represent the similarity of vectors in a feature space over polynomials of original variables.👉 Gaussian Radial Basis Function (RBF) kernel:  Gaussian RBF kernel maps each training instance to an infinite-dimensional space, therefore it’s a good thing that you don’t need to perform the mapping.Image Source: link"
535,  How you formulate SVM for a regression problem statement?,"For formulating SVM as a regression problem statement we have to reverse the objective: instead of trying to fit the largest possible street between two classes which we will do for classification problem statements while limiting margin violations, now for SVM Regression, it tries to fit as many instances as possible between the margin while limiting the margin violations."
536,  What affects the decision boundary in SVM?,"Adding more instances off the margin of the hyperplane does not affect the decision boundary, it is fully determined (or supported ) by the instances located at the edge of the street called support vectors"
537,  What is a slack variable?,"👉 To meet the soft margin objective, we need to introduce a slack variable ε>=0 for each sample; it measures how much any particular instance is allowed to violate the margin.👉 Here thing becomes little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make w (weight matrix) as small as possible to increase the margin. This is where the role of the C hyperparameter comes which allows us to define the trade-off between these two objectives.Fig. Picture Showing the slack variablesImage Source: link"
538,  What is a dual and primal problem and how is it relevant to SVMs?,"👉 Given a constrained optimization problem, known as the Primal problem, it is possible to express a different but closely related problem, which is known as its Dual problem.👉 The solution to the dual problem typically provides a lower bound to the solution of the primal problem, but under some conditions, it can be possible that it has even the same solutions as the primal problem.👉 Fortunately, the SVM problem completes these conditions, so that you can choose to solve the primal problem or the dual problem; and they both will have the same solution. "
539,  Can an SVM classifier outputs a confidence score when it classifies an instance? What about a probability?,"👉 An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities.👉 But if you set probability=True when building a model of SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s scores. By using this techniques, we can add the predict_proba() and predict_log_proba() methods to the SVM model."
540,  If you train an SVM classifier with an RBF kernel. It seems to underfit the training dataset: should you increase or decrease the hyper-parameter γ (gamma)? What about the C hyper-parameter?,"If we trained an SVM classifier using a Radial Basis Function (RBF) kernel, then it underfits the training set, so there might be too much regularization. To decrease it, you need to increase the gamma or C hyper-parameter."
541,  Is SVM sensitive to the Feature Scaling?,"Yes, SVMs are sensitive to feature scaling as it takes input data to find the margins around hyperplanes and gets biased for the variance in high values."
542,  Can you explain SVM?,"Explanation: Support vector machines is a supervised machine learning algorithm which works both on classification and regression problems. It tries to classify data by finding a hyperplane that maximizes the margin between the classes in the training data. Hence, SVM is an example of a large margin classifier.The basic idea of support vector machines:"
543,  What is the geometric intuition behind SVM?,"Explanation: If you are asked to classify two different classes. There can be multiple hyperplanes which can be drawn.SVM chooses the hyperplane which separates the data points as widely as possible. SVM draws a hyperplane parallel to the actual hyperplane intersecting with the first point of class A (also known as Support Vectors) and another hyperplane parallel to the actual hyperplane intersecting with the first point of class B. SVM tries to maximize these margins. Eventually, this margin maximization improves the model’s accuracy on unseen data."
544,  How would explain Convex Hull in light of SVMs?,Explanation: We simply build a convex hull for class A and class B and draw a perpendicular on the shortest distance between the closest points of both these hulls.
545,  What do know about Hard Margin SVM and Soft Margin SVM?,"Explanation: If a point Xi satisfies the equation Yi(WT*Xi +b) ≥ 1, then Xi is correctly classified else incorrectly classified. So we can see that if the points are linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called hard margin SVM (since we have very strict constraints to correctly classify each and every data point).To overcome this, we introduce a term ( ξ ) (pronounced as Zeta)if ξi= 0, the points can be considered as correctly classified.if ξi> 0 , Incorrectly classified points."
546,  What is Hinge Loss?,"Explanation: Hinge Loss is a loss function which penalises the SVM model for inaccurate predictions.If Yi(WT*Xi +b) ≥ 1, hinge loss is ‘0’ i.e the points are correctly classified. WhenYi(WT*Xi +b) < 1, then hinge loss increases massively.As Yi(WT*Xi +b) increases with every misclassified point, the upper bound of hinge loss {1- Yi(WT*Xi +b)} also increases exponentially.Hence, the points that are farther away from the decision margins have a greater loss value, thus penalising those points.We can formulate hinge loss as max[0, 1- Yi(WT*Xi +b)]"
547,  Explain the Dual form of SVM formulation?,"Explanation: The aim of the Soft Margin formulation is to minimizesubject toThis is also known as the primal form of SVM.The duality theory provides a convenient way to deal with the constraints. The dual optimization problem can be written in terms of dot products, thereby making it possible to use kernel functions.It is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions, it can even have the same solutions as the primal problem. Luckily, the SVM problem happens to meet these conditions, so you can choose to solve the primal problem or the dual problem; both will have the same solution."
548,  What’s the “kernel trick” and how is it useful?,"Explanation: Earlier we have discussed applying SVM on linearly separable data but it is very rare to get such data. Here, kernel trick plays a huge role. The idea is to map the non-linear separable data-set into a higher dimensional space where we can find a hyperplane that can separate the samples.It reduces the complexity of finding the mapping function. So, Kernel function defines the inner product in the transformed space. Application of the kernel trick is not limited to the SVM algorithm. Any computations involving the dot products (x, y) can utilize the kernel trick."
549,  What is Polynomial kernel?,"Explanation: Polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.For d-degree polynomials, the polynomial kernel is defined as:"
550,  What is RBF-Kernel?,"Explanation:The RBF kernel on two samples x and x’, represented as feature vectors in some input space, is defined as||x-x’||² recognized as the squared Euclidean distance between the two feature vectors. sigma is a free parameter."
551,  Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?,"Explanation: This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m² and m³. So, if there are millions of instances, you should use the primal form, because the dual form will be much too slow."
552,  Explain about SVM Regression?,"Explanation: The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because the output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM13. Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm.Explanation:"
553,  What is the role of C in SVM? How does it affect the bias/variance trade-off?,"Explanation:In the given Soft Margin Formulation of SVM, C is a hyperparameter.C hyperparameter adds a penalty for each misclassified data point.Large Value of parameter C implies a small margin, there is a tendency to overfit the training model.Small Value of parameter C implies a large margin which might lead to underfitting of the model."
554,"  SVM being a large margin classifier, is it influenced by outliers?","Explanation: Yes, if C is large, otherwise not."
555,"  In SVM, what is the angle between the decision boundary and theta?","Explanation: Decision boundary is a plane having equation Theta1*x1+Theta2*x2+……+c = 0, so as per the property of a plane, it’s coefficients vector is normal to the plane. Hence, the Theta vector is perpendicular to the decision boundary."
556,  What is the difference between logistic regression and SVM without a kernel?,Explanation: They differ only in the implementation . SVM is much more efficient and has good optimization packages.
557,  Can any similarity function be used for SVM?,Explanation: No. It has to have to satisfy Mercer’s theorem.
558,  Does SVM give any probabilistic output?,"Explanation: SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation"
559,  Explain hard and soft margin in context with SVM.,"Hard Margin Does not allow any errorSoft Margin Allow error, it reduces the chance of overfitting"
560,  Why does SVM gets slower when the number of examples increases?,"The computation cost depends upon the selection of pair of vectors (nC2, as derived in the optimization section)."
561,  What are the hyperparameters associated with SVM?,KernelC: Number of permitted errors. Large C gives a narrow margin and vice versa.
562,  Why the SVM algorithm is known to be memory efficient?,"Not all the data points are required to make the decision, once the support vectors are decided only the support vectors and equation of the hyperplane is required to make the decision."
563,  State the statement is True or False: ‘SVM decision boundary is perpendicular bisector to the line joining the closest point of convex hull of the two classes’.,"True statement. Geometrical explanation: make the convex hull of each of the classes, try to join the convex hulls using the closest point and the classification boundary will be the perpendicular bisector."
564,  Give a typical example where SVM will be used over RandomForest.,Text classification: SVM is an algorithm of choice for higher-dimensional space when the number of features is large.
565,  What is regularization? How is an SVM algorithm regularized? Is it L1 or L2 regularization in nature?,"‘C parameter’ is actually the regularization parameter. The C parameter is multiplied by the sum of errors thus the regularization, in general, is L1 in nature. But if the cost function is modified in such a way that the cost of SVM is C multiplied by the sum of squares of the error the regularization becomes L2 in nature."
566,Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?,"Ans. The performance depends on many factorsthe number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your modelIn general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)"
567,  Why SVM is an example of a large margin classifier?,"Why SVM is an example of a large margin classifier?SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier."
568,  What is the role of C in SVM?,"What is the role of C in SVM?Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable."
569,  What is the intuition of a large margin classifier?,"What is the intuition of a large margin classifier?Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above."
570,  What is a kernel in SVM? Why do we use kernels in SVM?,"What is a kernel in SVM? Why do we use kernels in SVM?Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces."
571,  Can we apply the kernel trick to logistic regression? Why is it not used in practice then?,"Can we apply the kernel trick to logistic regression? Why is it not used in practice then?Ans.Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy"
572,  What is the difference between logistic regression and SVM without a kernel?,"What is the difference between logistic regression and SVM without a kernel?Ans. Only in implementation, One is much more efficient and has good optimization packages"
573,  What is the difference between logistic regression and SVM,"What is the difference between logistic regression and SVMAns. Logistic regression assumes that the predictors aren’t sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If there’s a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.Non-regularized logistic regression techniques don’t work well (in fact, the fitted coefficients diverge) when there’s a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and there’s no guarantee that you’ll get the best one. What you get is an extremely confident model with poor predictive power near the margin.SVMs get you the best separating hyperplane, and they’re efficient in high dimensional spaces. They’re similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there’s noise in the data.Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.Logistic regression is great in a low number of dimensions and when the predictors don’t suffice to give more than a probabilistic estimate of the response. SVMs do better when there’s a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses."
574,  Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?,"Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.For a higher gamma, the model will capture the shape of the dataset well."
575,  What is generalization error in terms of the SVM?,What is generalization error in terms of the SVM?Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.
576, Is XGBoost faster than random forest?,"Solution: XGBoost is usually used to train gradient-boosted decision trees (GBDT) and other gradient boosted models. Random forests also use the same model representation and inference as gradient-boosted decision trees, but it is a different training algorithm. XGBoost can be used to train a standalone random forest. Also, random forest can be used as a base model for gradient boosting techniques.Solution:Further, random forest is an improvement over bagging that helps in reducing the variance. Random forest builds trees in parallel, while in boosting, trees are built sequentially. Meaning, each of the trees is grown using information from previously grown trees, unlike bagging, where multiple copies of original training data are created and fit separate decision tree on each. This is the reason why XGBoost generally performs better than random forest. Know more here."
577, What are the advantages and disadvantages of XGBoost?,Advantages:Advantages:Disadvantages:Disadvantages:Know more here.
578, How XGBoost Works?,"Solution: When using gradient boosting for regression, where the weak learners are considered to be regression trees, each of the regression trees maps an input data point to one of its leaves that includes a continuous score. XGB minimises a regularised objective function that merges a convex loss function, which is based on the variation between the target outputs and the predicted outputs. The training then proceeds iteratively, adding new trees with the capability to predict the residuals as well as errors of prior trees that are then coupled with the previous trees to make the final prediction. Solution: Click here to learn the step by step process of how XGB works."
579, What does the weight of XGB leaf nodes mean? How to calculate it?,"Solution: The “leaf weight” can be said as the model’s predicted output associated with each leaf (exit) node. Here is an instance of how to calculate the weights of the leaf nodes in XGB-Solution: Consider a test data point, where age=10 and gender=female.To get the prediction for the data point, the tree is traversed from the top to bottom, performing a series of tests. At each of the intermediate nodes, a feature is needed to compare against a threshold. Now, depending on the result of the comparison, one must proceed to either the left or right child node of the tree. In case of (10, female), the test “age < 15” is to be performed first and then proceed to the left branch, because “age < 15” is true. Then, the second test “gender = male?” is performed, which evaluates to false, so we proceed to the right branch. We end up at the Leaf 2, whose output (leaf weight) is 0.1.Click here to know more in detail."
580, What are the data pre-processing steps for XGB?,Solution: The data pre-processing steps for XGB include the following-Solution:Know more here.
581, How does XGB calculate features?,"Solution: XGB automatically provides the estimations of feature importance from a trained predictive model. After a boosting tree is constructed, it retrieves feature importance scores for each attribute. The feature importance contributes a score which indicates how much valuable each feature was in the construction of the boosted decision trees within the model.  Solution:Also, in terms of accuracy, XGB models show better performance for the training phase and comparable performance for the testing phase when compared to SVM models. Besides accuracy, XGB has higher computation speed than SVM.  Know more here."
582, Why does XGBoost perform better than SVM?,"Solution: In case of missing values, XGB is internally designed to handle missing values. The missing values are interpreted in such a way that if there endures any trend in the missing values, it is captured by the model. Users are required to supply a different value than other observations and pass that as a parameter. Solution: XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future. On the other hand, Support Vector Machine (SVM) does not perform well with the missing data and it is always a better option to impute the missing values before running SVM. Know more here."
583, Differences between XGBoost and LightGBM.,Solution: XGBoost and LightGBM are the packages belonging to the family of gradient boosting decision trees (GBDTs). Solution:Know more here.
584, How does XGB handle missing values?,"Solution: XGBoost supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. It is important to note that the gblinear booster treats missing values as zeros. During the training time XGB decides whether the missing values should fall into the right node or left node. This decision is taken to minimise the loss. If there are no missing values during the training time, the tree made a default  decision to send any new missings to the right node.Solution:Know more here."
585, What is the difference between AdaBoost and XGBoost?,"Solution: XGBoost is flexible compared to AdaBoost as XGB is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function."
586,  What is Linear Regression Algorithm?,"In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method."
597,  What are MAE and MAPE?,"MAE stands for Mean Absolute Error, which is defined as the average of absolute or positive errors of all values. In simple words, we can say MAE is an average of absolute or positive differences between predicted values and the actual values.MAPE stands for Mean Absolute Percent Error, which calculates the average absolute error in percentage terms. In simple words, It can be understood as the percentage average of absolute or positive errors.                                                        Image Source: Google Images"
603,  What is Multicollinearity?,"It is a phenomenon where two or more independent variables(predictors) are highly correlated with each other i.e. one variable can be linearly predicted with the help of other variables. It determines the inter-correlations and inter-association among independent variables. Sometimes, multicollinearity can also be known as collinearity. Inaccurate use of dummy variables. Due to a variable that can be computed from the other variable in the dataset.  Impacts regression coefficients i.e, coefficients become indeterminate. Causes high standard errors.  By using the correlation coefficient. With the help of Variance inflation factor (VIF), and Eigenvalues. To learn more about, multicollinearity, refer to the link."
607,  How is Hypothesis testing used in Linear Regression Algorithm?,"For the following purposes, we can carry out the Hypothesis testing in linear regression:"
608,  To check whether an independent variable (predictor) is significant or not for the prediction of the target variable. Two common methods for this are —,"If the p-value of a particular independent variable is greater than a certain threshold (usually 0.05), then that independent variable is insignificant for the prediction of the target variable.If the value of the regression coefficient corresponding to a particular independent variable is zero, then that variable is insignificant for the predictions of the dependent variable and has no linear relationship with it."
610,  Is it possible to apply Linear Regression for Time Series Analysis?,"Yes, we can apply a linear regression algorithm for doing analysis on time series data, but the results are not promising and hence is not advisable to do so.The reasons behind not preferable linear regression on time-series data are as follows: Time series data is mostly used for the prediction of the future but in contrast, linear regression generally seldom gives good results for future prediction as it is basically not meant for extrapolation. Moreover, time-series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis. Unique Data Visualization Techniques To Make Your Plots Stand Out"
637,  How can learning curves help create a better model?,"Learning curves give the indication of the presence of overfitting or underfitting.  In a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:  If the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias.  If there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a high variance.  Machine Learning Engineers: Myths vs. Realities That’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments. Co-authored by – Ojas Agarwal isSingle = true; catStr = 'Artificial Intelligence';  Lead the AI Driven Technological Revolution   										PG Diploma in Machine Learning and Artificial Intelligence									 Learn More   "
638,  What is the difference between a Perceptron and Logistic Regression?,"A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.So, what’s the difference between the two?Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:"
639,  Can we have the same bias for all neurons of a hidden layer?,"Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.A point to note is that both these strategies would give you very different results."
640,"  What if we do not use any activation function(s) in a neural network?
","The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:Z = (weights * X) + biasStep 2: Apply an activation function to calculate the expected output:Y = Activation(Z)Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?Our equation for Y essentially becomes:Y = Z = (weights * X) + biasWait – isn’t this just a simple linear equation? Yes – and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model."
641,"  In a neural network, what if all the weights are initialized with the same value?","In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.Therefore, if all weights have the same initial value, this would lead to underfitting.Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below."
642,  List the supervised and unsupervised tasks in Deep Learning.,"Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:On the other hand, there are some unsupervised deep learning techniques as well:Here is a great article on applications of Deep Learning for unsupervised tasks:"
643,  What is the role of weights and bias in a neural network?,"This is a question best explained with a real-life example. Consider that you want to go out today to play a cricket match with your friends. Now, a number of factors can affect your decision-making, like:And so on. These factors can change your decision greatly or not too much. For example, if it is raining outside, then you cannot go out to play at all. Or if you have only one bat, you can share it while playing as well. The magnitude by which these factors can affect the game is called the weight of that factor.Factors like the weather or temperature might have a higher weight, and other factors like equipment would have a lower weight.However, does this mean that we can play a cricket match with only one bat? No – we would need 1 ball and 6 wickets as well. This is where bias comes into the picture. Bias lets you assign some threshold which helps you activate a decision-point (or a neuron) only when that threshold is crossed."
644,  How does forward propagation and backpropagation work in deep learning?,"Now, this can be answered in two ways. If you are on a phone interview, you cannot perform all the calculus in writing and show the interviewer. In such cases, it best to explain it as such:For an in-person interview, it is best to take up the marker, create a simple neural network with 2 inputs, a hidden layer, and an output layer, and explain it.Forward propagation:Backpropagation:At layer L2, for all weights:At layer L1, for all weights:You need not explain with respect to the bias term as well, though you might need to expand the above equations substituting the actual derivatives."
645,  What are the common data structures used in Deep Learning?,"Deep Learning goes right from the simplest data structures like lists to complicated ones like computation graphs.Here are the most common ones:Once the basics are out of the way, the interview would lead to slightly advanced deep learning concepts. These questions are much easier to answer if you have considerable practice with not only the mathematical concepts but also with coding them.Additionally, these questions can also become more project-specific. As a general rule of thumb, it is best to include examples of how you have used the concept asked in the question in your own projects. This has two advantages:Here, I have given an overview of the key concepts in the questions – you can always customize your answers to add more about your experiences with some of these deep learning algorithms and techniques."
646,  Why should we use Batch Normalization?,"Once the interviewer has asked you about the fundamentals of deep learning architectures, they would move on to the key topic of improving your deep learning model’s performance.Batch Normalization is one of the techniques used for reducing the training time of our deep learning algorithm. Just like normalizing our input helps improve our logistic regression model, we can normalize the activations of the hidden layers in our deep learning model as well:We basically normalize a[1] and a[2] here. This means we normalize the inputs to the layer, and then apply the activation functions to the normalized inputs.Here is an article that explains Batch Normalization and other techniques for improving Neural Networks: Neural Networks – Hyperparameter Tuning, Regularization & Optimization."
648,  Why does a Convolutional Neural Network (CNN) work better with image data?,"The key to this question lies in the Convolution operation. Unlike humans, the machine sees the image as a matrix of pixel values. Instead of interpreting a shape like a petal or an ear, it just identifies curves and edges.Thus, instead of looking at the entire image, it helps to just read the image in parts. Doing this for a 300 x 300 pixel image would mean dividing the matrix into smaller 3 x 3 matrices and dealing with them one by one. This is convolution.Mathematically, we just perform a small operation on the matrix to help us detect features in the image – like boundaries, colors, etc.Z = X * fHere, we are convolving (* operation – not multiplication) the input matrix X with another small matrix f, called the kernel/filter to create a new matrix Z. This matrix is then passed on to the other layers.If you have a board/screen in front of you, you can always illustrate this with a simple example:Learning more about how CNNs work here."
650,"  In a CNN, if the input size 5 X 5 and the filter size is 7 X 7, then what would be the size of the output?","This is a pretty intuitive answer. As we saw above, we perform the convolution on ‘x’ one step at a time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with dimensions 3 X 3.Thus, to make the input size similar to the filter size, we make use of padding – adding 0s to the input matrix such that its new size becomes at least 7 X 7. Thus, the output size would be using the formula:Dimension of image = (n, n) = 5 X 5Dimension of filter = (f,f)  = 7 X 7Padding = 1 (adding 1 pixel with value 0 all around the edges)Dimension of output will be (n+2p-f+1) X (n+2p-f+1) = 1 X 1"
651,  What’s the difference between valid and same padding in a CNN?,"This question has more chances of being a follow-up question to the previous one. Or if you have explained how you used CNNs in a computer vision task, the interviewer might ask this question along with the details of the padding parameters."
652,  What do you mean by exploding and vanishing gradients?,"The key here is to make the explanation as simple as possible. As we know, the gradient descent algorithm tries to minimize the error by taking small steps towards the minimum value. These steps are used to update the weights and biases in a neural network.However, at times, the steps become too large and this results in larger updates to weights and bias terms – so much so as to cause an overflow (or a NaN) value in the weights. This leads to an unstable algorithm and is called an exploding gradient.On the other hand, the steps are too small and this leads to minimal changes in the weights and bias terms – even negligible changes at times. We thus might end up training a deep learning model with almost the same weights and biases each time and never reach the minimum error function. This is called the vanishing gradient.A point to note is that both these issues are specifically evident in Recurrent Neural Networks – so be prepared for follow-up questions on RNN!"
653,  What are the applications of transfer learning in Deep Learning?,"I am sure you would have a doubt as to why a relatively simple question was included in the Intermediate Level. The reason is the sheer volume of subsequent questions it can generate!The use of transfer learning has been one of the key milestones in deep learning. Training a large model on a huge dataset, and then using the final parameters on smaller simpler datasets has led to defining breakthroughs in the form of Pretrained Models. Be it Computer Vision or NLP, pretrained models have become the norm in research and in the industry.Some popular examples include BERT, ResNet, GPT-2, VGG-16, etc and many more.It is here that you can earn brownie points by pointing out specific examples/projects where you used these models and how you used them as well.It is not possible to discuss all of them, so here are a few resources to get started:It is here that questions become really specific to your projects or to what you have discussed in the interview before.Also, depending on the domain – with Computer Vision or Natural Language Processing, these questions can change. While it is not important to know the architecture of each model in detail, you would need to know the intuition behind them and why these models were needed in the first place.Again, just like the intermediate level, it is important to always bring in examples that you have studied or implemented yourself into the discussion."
654,  How backpropagation is different in RNN compared to ANN?,"In Recurrent Neural Networks, we have an additional loop at each node:This loop essentially includes a time component into the network as well. This helps in capturing sequential information from the data, which could not be possible in a generic artificial neural network.This is why the backpropagation in RNN is called Backpropagation through Time, as in backpropagation at each time step.You can find a detailed explanation of RNNs here: Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks."
655,  How does LSTM solve the vanishing gradient challenge?,"The LSTM model is considered a special case of RNNs. The problems of vanishing gradients and exploding gradients we saw earlier are a disadvantage while using the plain RNN model.In LSTMs, we add a forget gate, which is basically a memory unit that retains information that is retained across timesteps and discards the other information that is not needed. This also necessitates the need for input and output gates to include the results of the forget gate as well."
656,  Why is GRU faster as compared to LSTM?,"As you can see, the LSTM model can become quite complex. In order to still retain the functionality of retaining information across time and yet not make a too complex model, we need GRUs.Basically, in GRUs, instead of having an additional Forget gate, we combine the input and Forget gates into a single Update Gate:It is this reduction in the number of gates that makes GRU less complex and faster than LSTM. You can learn about GRUs, LSTMs and other sequence models in detail here: Must-Read Tutorial to Learn Sequence Modeling & Attention Models."
657,  How is the transformer architecture better than RNN?,"Advancements in deep learning have made it possible to solve many tasks in Natural Language Processing. Networks/Sequence models like RNNs, LSTMs, etc. are specifically used for this purpose – so as to capture all possible information from a given sentence, or a paragraph. However, sequential processing comes with its caveats:This gave rise to the Transformer architecture. Transformers use what is called the attention mechanism. This basically means mapping dependencies between all the parts of a sentence.Here is an excellent article explaining transformers: How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models."
658,  Describe a project you worked on and the tools/frameworks you used?,"Now, this is one question that is sure to be asked even if none of the above ones is asked in your deep learning interview. I have included it in the advanced section since you might be grilled on each and every part of the code you have written.Before the interview, make sure to:When you are asked such a question, it is best to give a small 30-second pitch on what was the:After this, you can start going into detail about the model architecture, what preprocessing steps you had to take, and how that changed the data.An important point to be noted is that the project need not be a very complicated or sophisticated one. A well-explained object detection project would earn you more points than a poorly-explained video classification project. Towards this end, I recommend having a README file in the above format for every project that you have implemented.Unique Data Visualization Techniques To Make Your Plots Stand Out"
659, What is deep learning?,"Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc."
660," What are the main differences between AI, Machine Learning, and Deep Learning?"," AI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior. Machine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.  Deep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making. "
661, Differentiate supervised and unsupervised deep learning procedures.," Supervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing. Unsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data. "
662, What are the applications of deep learning?,There are various applications of deep learning: Computer vision Natural language processing and pattern recognition Image recognition and processing Machine translation Sentiment analysis Question Answering system Object Classification and Detection Automatic Handwriting Generation Automatic Text Generation. 
663, Do you think that deep network is better than a shallow one?,"Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input."
664," What do you mean by ""overfitting""?",Overfitting is the most common issue which occurs in deep learning. It usually occurs when a deep learning algorithm apprehends the sound of specific data. It also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.
665, What is Backpropagation?,Backpropagation is a training algorithm which is used for multilayer neural networks. It transfers the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient.Backpropagation can be divided into the following steps: It can forward propagation of training data through the network to generate output. It uses target value and output value to compute error derivative concerning output activations. It can backpropagate to compute the derivative of the error concerning output activations in the previous layer and continue for all hidden layers. It uses the previously calculated derivatives for output and all hidden layers to calculate the error derivative concerning weights. It updates the weights. 
666, What is the function of the Fourier Transform in Deep Learning?,"Fourier transform package is highly efficient for analyzing, maintaining, and managing a large databases. The software is created with a high-quality feature known as the special portrayal. One can effectively utilize it to generate real-time array data, which is extremely helpful for processing all categories of signals."
667, Describe the theory of autonomous form of deep learning in a few words.,"There are several forms and categories available for the particular subject, but the autonomous pattern represents independent or unspecified mathematical bases which are free from any specific categorizer or formula."
668," What is the use of Deep learning in today's age, and how is it adding data scientists?",Deep learning has brought significant changes or revolution in the field of machine learning and data science. The concept of a complex neural network (CNN) is the main center of attention for data scientists. It is widely taken because of its advantages in performing next-level machine learning operations. The advantages of deep learning also include the process of clarifying and simplifying issues based on an algorithm due to its utmost flexible and adaptable nature. It is one of the rare procedures which allow the movement of data in independent pathways. Most of the data scientists are viewing this particular medium as an advanced additive and extended way to the existing process of machine learning and utilizing the same for solving complex day to day issues.
669, What are the deep learning frameworks or tools?,"Deep learning frameworks or tools are:Tensorflow, Keras, Chainer, Pytorch, Theano & Ecosystem, Caffe2, CNTK, DyNetGensim, DSSTNE, Gluon, Paddle, Mxnet, BigDL"
670, What are the disadvantages of deep learning?,"There are some disadvantages of deep learning, which are: Deep learning model takes longer time to execute the model. In some cases, it even takes several days to execute a single model depends on complexity. The deep learning model is not good for small data sets, and it fails here. "
671, What is the meaning of term weight initialization in neural networks?,"In neural networking, weight initialization is one of the essential factors. A bad weight initialization prevents a network from learning. On the other side, a good weight initialization helps in giving a quicker convergence and a better overall error. Biases can be initialized to zero. The standard rule for setting the weights is to be close to zero without being too small."
672, Explain Data Normalization.,"Data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. It assures better convergence during backpropagation. In general, data normalization boils down to subtracting the mean of each data point and dividing by its standard deviation."
673, Why is zero initialization not a good weight initialization process?,"If the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.As a result, the network cannot learn at all because there is no source of asymmetry between neurons. That is the reason why we need to add randomness to the weight initialization process."
674, What are the prerequisites for starting in Deep Learning?,"There are some basic requirements for starting in Deep Learning, which are: Machine Learning Mathematics Python Programming "
675, What are the supervised learning algorithms in Deep learning?, Artificial neural network Convolution neural network Recurrent neural network 
676, What are the unsupervised learning algorithms in Deep learning?, Self Organizing Maps Deep belief networks (Boltzmann Machine) Auto Encoders 
677, How many layers in the neural network?, Input Layer The input layer contains input neurons which send information to the hidden layer. Hidden Layer The hidden layer is used to send data to the output layer. Output Layer The data is made available at the output layer. 
678, What is the use of the Activation function?,"The activation function is used to introduce nonlinearity into the neural network so that it can learn more complex function. Without the Activation function, the neural network would be only able to learn function, which is a linear combination of its input data.Activation function translates the inputs into outputs. The activation function is responsible for deciding whether a neuron should be activated or not. It makes the decision by calculating the weighted sum and further adding bias with it. The basic purpose of the activation function is to introduce non-linearity into the output of a neuron."
679, How many types of activation function are available?, Binary Step Sigmoid Tanh ReLU Leaky ReLU Softmax Swish 
680, What is a binary step function?,"The binary step function is an activation function, which is usually based on a threshold. If the input value is above or below a particular threshold limit, the neuron is activated, then it sends the same signal to the next layer. This function does not allow multi-value outputs."
681, What is the sigmoid function?,"The sigmoid activation function is also called the logistic function. It is traditionally a trendy activation function for neural networks. The input data to the function is transformed into a value between 0.0 and 1.0. Input values that are much larger than 1.0 are transformed to the value 1.0. Similarly, values that are much smaller than 0.0 are transformed into 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. It was the default activation used on neural networks, in the early 1990s."
682, What is Tanh function?,"The hyperbolic tangent function, also known as tanh for short, is a similar shaped nonlinear activation function. It provides output values between -1.0 and 1.0. Later in the 1990s and through the 2000s, this function was preferred over the sigmoid activation function as models. It was easier to train and often had better predictive performance."
683, What is ReLU function?,"A node or unit which implements the activation function is referred to as a rectified linear activation unit or ReLU for short. Generally, networks that use the rectifier function for the hidden layers are referred to as rectified networks.Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution."
684, What is the use of leaky ReLU function?,The Leaky ReLU (LReLU or LReL) manages the function to allow small negative values when the input is less than zero.
685, What is the softmax function?,"The softmax function is used to calculate the probability distribution of the event over 'n' different events. One of the main advantages of using softmax is the output probabilities range. The range will be between 0 to 1, and the sum of all the probabilities will be equal to one. When the softmax function is used for multi-classification model, it returns the probabilities of each class, and the target class will have a high probability."
686, What is a Swish function?,"Swish is a new, self-gated activation function. Researchers at Google discovered the Swish function. According to their paper, it performs better than ReLU with a similar level of computational efficiency. "
687, What is the most used activation function?,Relu function is the most used activation function. It helps us to solve vanishing gradient problems.
688, Can Relu function be used in output layer?,"No, Relu function has to be used in hidden layers."
689, In which layer softmax activation function used?,Softmax activation function has to be used in the output layer.
690, What do you understand by Autoencoder?,"Autoencoder is an artificial neural network. It can learn representation for a set of data without any supervision. The network automatically learns by copying its input to the output; typically,internet representation consists of smaller dimensions than the input vector. As a result, they can learn efficient ways of representing the data. Autoencoder consists of two parts; an encoder tries to fit the inputs to the internal representation, and a decoder converts the internal state to the outputs."
691, What do you mean by Dropout?,"Dropout is a cheap regulation technique used for reducing overfitting in neural networks. We randomly drop out a set of nodes at each training step. As a result, we create a different model for each training case, and all of these models share weights. It's a form of model averaging."
692, What do you understand by Tensors?,"Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, which allows us to represent the data having higher dimensions. In general, we deal with high dimensional data sets where dimensions refer to different features present in the data set."
693, What do you understand by Boltzmann Machine?,"A Boltzmann machine (also known as stochastic Hopfield network with hidden units) is a type of recurrent neural network. In a Boltzmann machine, nodes make binary decisions with some bias. Boltzmann machines can be strung together to create more sophisticated systems such as deep belief networks. Boltzmann Machines can be used to optimize the solution to a problem. Some important points about Boltzmann Machine- It uses a recurrent structure. It consists of stochastic neurons, which include one of the two possible states, either 1 or 0. The neurons present in this are either in an adaptive state (free state) or clamped state (frozen state). If we apply simulated annealing or discrete Hopfield network, then it would become a Boltzmann Machine. "
694, What is Model Capacity?,"The capacity of a deep learning neural network controls the scope of the types of mapping functions that it can learn. Model capacity can approximate any given function. When there is a higher model capacity, it means that the larger amount of information can be stored in the network."
695, What is the cost function?,"A cost function describes us how well the neural network is performing with respect to its given training sample and the expected output. It may depend on variables such as weights and biases.It provides the performance of a neural network as a whole. In deep learning, our priority is to minimize the cost function. That's why we prefer to use the concept of gradient descent."
696, Explain gradient descent?,"An optimization algorithm that is used to minimize some function by repeatedly moving in the direction of steepest descent as specified by the negative of the gradient is known as gradient descent. It's an iteration algorithm, in every iteration algorithm, we compute the gradient of a cost function, concerning each parameter and update the parameter of the function via the following formula:Where,Θ - is the parameter vector, α - learning rate,  J(Θ) - is a cost functionIn machine learning, it is used to update the parameters of our model. Parameters represent the coefficients in linear regression and weights in neural networks."
697," Explain the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch?"," Stochastic Gradient Descent Stochastic gradient descent is used to calculate the gradient and update the parameters by using only a single training example. Batch Gradient Descent Batch gradient descent is used to calculate the gradients for the whole dataset and perform just one update at each iteration. Mini-batch Gradient Descent Mini-batch gradient descent is a variation of stochastic gradient descent. Instead of a single training example, mini-batch of samples is used. Mini-batch gradient descent is one of the most popular optimization algorithms. "
698, What are the main benefits of Mini-batch Gradient Descent?," It is computationally efficient compared to stochastic gradient descent. It improves generalization by finding flat minima. It improves convergence by using mini-batches. We can approximate the gradient of the entire training set, which might help to avoid local minima. "
699, What is matrix element-wise multiplication? Explain with an example.,Element-wise matrix multiplication is used to take two matrices of the same dimensions. It further produces another combined matrix with the elements that are a product of corresponding elements of matrix a and b.
700, What do you understand by a convolutional neural network?,"A convolutional neural network, often called CNN, is a feedforward neural network. It uses convolution in at least one of its layers. The convolutional layer contains a set of filter (kernels). This filter is sliding across the entire input image, computing the dot product between the weights of the filter and the input image. As a result of training, the network automatically learns filters that can detect specific features."
701, Explain the different layers of CNN.,"There are four layered concepts that we should understand in CNN (Convolutional Neural Network): Convolution This layer comprises of a set of independent filters. All these filters are initialized randomly. These filters then become our parameters which will be learned by the network subsequently. ReLU The ReLu layer is used with the convolutional layer. Pooling It reduces the spatial size of the representation to lower the number of parameters and computation in the network. This layer operates on each feature map independently. Full Collectedness Neurons in a completely connected layer have complete connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can be easily computed with a matrix multiplication followed by a bias offset. "
702, What is an RNN?,"RNN stands for Recurrent Neural Networks. These are the artificial neural networks which are designed to recognize patterns in sequences of data such as handwriting, text, the spoken word, genomes, and numerical time series data. RNN use backpropagation algorithm for training because of their internal memory. RNN can remember important things about the input they received, which enables them to be very precise in predicting what's coming next."
703, What are the issues faced while training in Recurrent Networks?,"Recurrent Neural Network uses backpropagation algorithm for training, but it is applied on every timestamp. It is usually known as Back-propagation Through Time (BTT).There are two significant issues with Back-propagation, such as: Vanishing Gradient When we perform Back-propagation, the gradients tend to get smaller and smaller because we keep on moving backward in the Network. As a result, the neurons in the earlier layer learn very slowly if we compare it with the neurons in the later layers.Earlier layers are more valuable because they are responsible for learning and detecting simple patterns. They are the building blocks of the network. If they provide improper or inaccurate results, then how can we expect the next layers and complete network to perform nicely and provide accurate results. The training procedure tales long, and the prediction accuracy of the model decreases. Exploding Gradient Exploding gradients are the main problem when large error gradients accumulate. They provide result in very large updates to neural network model weights during training. Gradient Descent process works best when updates are small and controlled. When the magnitudes of the gradient accumulate, an unstable network is likely to occur. It can cause poor prediction of results or even a model that reports nothing useful. "
704, Explain the importance of LSTM.,"LSTM stands for Long short-term memory. It is an artificial RNN (Recurrent Neural Network) architecture, which is used in the field of deep learning. LSTM has feedback connections which makes it a ""general purpose computer."" It can process not only a single data point but also entire sequences of data.They are a special kind of RNN which are capable of learning long-term dependencies."
705, What are the different layers of Autoencoders? Explain briefly.,An autoencoder contains three layers: Encoder The encoder is used to compress the input into a latent space representation. It encodes the input images as a compressed representation in a reduced dimension. The compressed images are the distorted version of the original image. Code The code layer is used to represent the compressed input which is fed to the decoder. Decoder The decoder layer decodes the encoded image back to its original dimension. The decoded image is a reduced reconstruction of the original image. It is automatically reconstructed from the latent space representation. 
706, What do you understand by Deep Autoencoders?,"Deep Autoencoder is the extension of the simple Autoencoder. The first layer present in DeepAutoencoder is responsible for first-order functions in the raw input. The second layer is responsible for second-order functions corresponding to patterns in the appearance of first-order functions. Deeper layers which are available in the Deep Autoencoder tend to learn even high-order features.A deep autoencoder is the combination of two, symmetrical deep-belief networks: First four or five shallow layers represent the encoding half. The other combination of four or five layers makes up the decoding half. "
707, What are the three steps to developing the necessary assumption structure in Deep learning?,"The procedure of developing an assumption structure involves three specific actions.  The first step contains algorithm development. This particular process is lengthy. The second step contains algorithm analyzing, which represents the in-process methodology.  The third step is about implementing the general algorithm in the final procedure. The entire framework is interlinked and required for throughout the process. "
708," What do you understand by Perceptron? Also, explain its type.",A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features. It is an algorithm for supervised learning of binary classifiers. This algorithm is used to enable neurons to learn and processes elements in the training set one at a time.There are two types of perceptrons: Single-Layer Perceptron Single layer perceptrons can learn only linearly separable patterns. Multilayer Perceptrons Multilayer perceptrons or feedforward neural networks with two or more layers have the higher processing power. © Copyright 2011-2021 www.javatpoint.com. All rights reserved. Developed by JavaTpoint.
710,  What is a Neural Network?,"Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.The most common Neural Networks consist of three network layers: An input layer A hidden layer (this is the most important layer where feature extraction takes place, and adjustments are made to train faster and function better) An output layer Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course"
727,  What Are the Different Layers on CNN?,"There are four layers in CNN: Convolutional Layer -  the layer that performs a convolutional operation, creating several smaller picture windows to go over the data. ReLU Layer - it brings non-linearity to the network and converts all the negative pixels to zero. The output is a rectified feature map. Pooling Layer - pooling is a down-sampling operation that reduces the dimensionality of the feature map. Fully Connected Layer - this layer recognizes and classifies the objects in the image. "
736,  Explain Generative Adversarial Network.,"Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner’s check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic.The forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.Let us understand this example with the help of an image shown above.There is a noise vector coming into the forger who is generating fake wine.Here the forger acts as a Generator.The shop owner acts as a Discriminator.The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake.So, there are two primary components of Generative Adversarial Network (GAN) named: Generator Discriminator The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images."
739,  What is the difference between Machine Learning and Deep Learning?,"Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby helping them improve with experience. Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby forming neural networks.  Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby helping them improve with experience.Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby forming neural networks."
743,  What is the meaning of overfitting?,"Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.  Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented."
747,  What is the use of the loss function?,"The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset. The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.  The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset. The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training."
756,  What are hyperparameters in Deep Learning?,"Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers and more, present in the neural network. Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers and more, present in the neural network."
758,  What is the meaning of dropout in Deep Learning?,"Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby causing lower efficiency. Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby causing lower efficiency."
759,  What are tensors?,"Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors are easily understood and broadly used. Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors are easily understood and broadly used."
763,  What is a computational graph in Deep Learning?,"A computation graph is a series of operations that are performed to take in inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability. If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course. A computation graph is a series of operations that are performed to take in inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course."
765,  What are the various layers present in a CNN?,There are four main layers that form a convolutional neural network:  Convolution: These are layers consisting of entities called filters that are used as parameters to train the network. ReLu: It is used as the activation function and used always with the convolution layer. Pooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage. Connectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.  There are four main layers that form a convolutional neural network: Convolution: These are layers consisting of entities called filters that are used as parameters to train the network. ReLu: It is used as the activation function and used always with the convolution layer. Pooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage. Connectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily. 
767,  What is a vanishing gradient when using RNNs?,"Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby causing efficiency problems in the network. Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby causing efficiency problems in the network."
820,  What is the role of the C hyper-parameter in SVM? Does it affect the bias/variance trade-off?,👉 The balance between keeping the margins as large as possible and limiting the margin violation is controlled by the C parameter: a small value leads to a wider street but more margin violation and a higher value of C makes fewer margin violations but ends up with a smaller margin and overfitting.👉 Here thing becomes a little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make W as small as possible to increase the margin. This is where the role of the C hyperparameter comes in which allows us to define the trade-off between these two objectives.
821,  Explain different types of kernel functions.,"A function is called kernel if there exist a function ϕ that maps a and b into another space such that K(a, b) = ϕ(a)T · ϕ(b). So you can use K as a kernel since you just know that a mapping ϕ exists, even if you don’t know what ϕ function is. These are the very good things about kernels.Some of the kernel functions are as follows:👉 Polynomial Kernel: These are the kernel functions that represent the similarity of vectors in a feature space over polynomials of original variables.👉 Gaussian Radial Basis Function (RBF) kernel:  Gaussian RBF kernel maps each training instance to an infinite-dimensional space, therefore it’s a good thing that you don’t need to perform the mapping.Image Source: link"
839,  Explain about SVM Regression?,"Explanation: The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because the output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM"
840,  Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm.,Explanation:
844,  Can we apply the kernel trick to logistic regression? Why is it not used in practice then?,Explanation:
848,"Generic question for any algorithm: ADA? (Advantages, Disadvantages and Assumptions)","Advantages:Turnkey algorithm, very few parameters to optimize.Optimization is in convex space so a global minimum is reached unlike neural network it doesn’t suffer with problem of stucking at local minima.Memory efficientLinear SVM is more effective than other algorithm if the number of features are highe (>1000)Disadvantages:Feature scalingTends to get slower as number of examples increasesHard margin leads to overfittingAssumptions:None"
877,  How to choose the optimal value of K in the KNN Algorithm?,"There is no straightforward method to find the optimal value of K in the KNN algorithm.You have to play around with different values to choose which value of K should be optimal for my problem statement. Choosing the right value of K is done through a process known as Hyperparameter Tuning.The optimum value of K for KNN is highly dependent on the data itself. In different scenarios, the optimum K may vary. It is more or less a hit and trial method.There is no one proper method of finding the K value in the KNN algorithm. No method is the rule of thumb but you should try the following suggestions:"
878,"  Cross-Validation Method: We should also take the help of cross-validation to find out the optimal value of K in KNN. Start with the minimum value of k i.e, K=1, and run cross-validation, measure the accuracy, and keep repeating till the results become consistent.","As the value of K increases, the error usually goes down after each one-step increase in K, then stabilizes, and then raises again. Finally, pick the optimum K at the beginning of the stable zone. This technique is also known as the Elbow Method. Image Source: Google Images"
879,  Domain Knowledge: Sometimes with the help of domain knowledge for a particular use case we are able to find the optimum value of K (K should be an odd number).,I would therefore suggest trying a mix of all the above points to reach any conclusion.
883,  What are the advantages of the KNN Algorithm?,Some of the advantages of the KNN algorithm are as follows:
884,"  No Training Period: It does not learn anything during the training period since it does not find any discriminative function with the help of the training data. In simple words, actually, there is no training period for the KNN algorithm. It stores the training dataset and learns from it only when we use the algorithm for making the real-time predictions on the test dataset.","As a result, the KNN algorithm is much faster than other algorithms which require training. For Example, SupportVector Machines(SVMs), Linear Regression, etc.Moreover, since the KNN algorithm does not require any training before making predictions as a result new data can be added seamlessly without impacting the accuracy of the algorithm."
885,  What are the disadvantages of the KNN Algorithm?,Some of the disadvantages of the KNN algorithm are as follows:
887,  What are the real-life applications of KNN Algorithms?,The various real-life applications of the KNN Algorithm includes:
888,What is the Central Limit Theorem and why is it important?,"Suppose that we are interested in estimating the average height among all people. Collecting data for  every person in the world is impossible. While we can’t obtain a height measurement from everyone in the  population, we can still sample some people. The question now becomes, what can we say about the  average height of the entire population given a single sample. The Central Limit Theorem addresses this  question exactly."
889,What is sampling? How many sampling methods do you know?,"Data sampling is a statistical analysis technique used to select, manipulate and analyze a representative  subset of data points to identify patterns and trends in the larger data set being examined"
890,What is the difference between type I vs type II error?,"A type I error occurs when the null hypothesis is true, but is rejected. A type II error occurs when the null  hypothesis is false, but erroneously fails to be rejected."
891,"What is linear regression? What do the terms p-value, coefficient, and r-squared  value mean? What is the significance of each of these components? ","A linear regression is a good tool for quick predictive analysis: for example, the price of a house depends  on a myriad of factors, such as its size or its location. In order to see the relationship between these  variables, we need to build a linear regression, which predicts the line of best fit between them and can  help conclude whether or not these two factors have a positive or negative relationship."
892,What are the assumptions required for linear regression?,"There are four major assumptions: 1. There is a linear relationship between the dependent variables and  the regressors, meaning the model you are creating actually fits the data, 2. The errors or residuals of the  data are normally distributed and independent from each other, 3. There is minimal multicollinearity  between explanatory variables, and 4. Homoscedasticity. This means the variance around the regression  line is the same for all values of the predictor variable"
893,What is a statistical interaction?,"Basically, an interaction is when the effect of one factor (input variable) on the dependent variable (output  variable) differs among levels of another factor"
894,What is selection bias?,"Selection (or ‘sampling’) bias occurs in an ‘active,’ sense when the sample data that is gathered and  prepared for modeling has characteristics that are not representative of  . the true, future population of cases"
895,What is the Binomial Probability Formula? ,The binomial distribution consists of the probabilities of each of the possible numbers of successes on N  trials for independent events that each have a probability of ? (the Greek letter pi) of occurring.
896,What is Selection Bias? ,"Selection bias is a kind of error that occurs when the researcher decides who is going to be studied. It is  usually associated with research where the selection of participants  as the selection effect. It is the distortion of statistical analysis, resulting from the method of collecting  samples. If the selection bias is not taken into account, then some conclusions of the study may not be  accurate.  isn’t random. It is sometimes referred to"
897,What is a confusion matrix? ,"The confusion matrix is a 2X2 table that contains 4 outputs provided by the binary classifier. Various  measures, such as error-rate, accuracy, specificity, sensitivity, precision and recall are derived from  it. Confusion Matrix "
898,What do you understand by the term Normal Distribution?,"Data is usually distributed in different ways with a bias to the left or to the right or it can all be jumbled up.However, there are chances that data is distributed around a central value without any bias to the left or right  and reaches normal distribution in the form of a bell-shaped curve. "
899,What is correlation and covariance in statistics?,"Covariance and Correlation are two mathematical concepts; these two approaches are widely used in  statistics. Both Correlation and Covariance establish the relationship and also measure the dependency  between two random variables. Though the work is similar between these two in mathematical terms, they  are different from each other. Covariance: In covariance two items vary together and it’s a measure that indicates the extent to which two  random variables change in cycle. It is a statistical term; it explains the systematic relation between a pair of  random variables, wherein changes in one variable reciprocal by a corresponding change in another  variable."
900,What is an example of a data set with a non-Gaussian distribution?,"The Gaussian distribution is part of the Exponential family of distributions, but there are a lot more of  them, with the same sort of ease of use, in many cases, and if the person doing the machine learning has  a solid grounding in statistics, they can be utilized where appropriate."
901,What is the difference between Point Estimates and Confidence Interval?,"Point Estimation gives us a particular value as an estimate of a population parameter. Method of Moments  and Maximum Likelihood estimator methods are used to derive Point Estimators for population parameters. A confidence interval gives us a range of values which is likely to contain the population parameter. The  confidence interval is generally preferred, as it tells us how likely this interval is to contain the population  parameter. This likeliness or probability is called Confidence Level or Confidence coefficient and represented  by 1 — alpha, where alpha is the level of significance."
902,What is the goal of A/B Testing?,It is a hypothesis testing for a randomized experiment with two variables A and B. The goal of A/B Testing is to identify any changes to the web page to maximize or increase the outcome of  interest. A/B testing is a fantastic method for figuring out the best online promotional and marketing strategies  for your business.
903,What is p-value?,"When you perform a hypothesis test in statistics, a p-value can help you determine the strength of your  results. p-value is a number between 0 and 1. Based on the value it will denote the strength of the results.  The claim which is on trial is called the Null Hypothesis. Low p-value (? 0.05) indicates strength against the null hypothesis which means we can reject the null  Hypothesis. High p-value (? 0.05) indicates strength for the null hypothesis which means we can accept the  null Hypothesis p-value of 0.05 indicates the Hypothesis could go either way. To put it in another way, High P values: your data are likely with a true null. Low P values: your data are unlikely with a true null. "
904,What do you understand by statistical power of sensitivity and how do you calculate it?,"Sensitivity is commonly used to validate the accuracy of a classifier (Logistic, SVM, Random Forest etc.) Sensitivity is nothing but “Predicted True events/ Total events”. True events here are the events which were  true and model also predicted them as true. Calculation of seasonality is pretty straightforward. Seasonality = ( True Positives ) / ( Positives in Actual Dependent Variable ) "
905, Why Is Re-sampling Done? ,"Resampling is done in any of these cases: Estimating the accuracy of sample statistics by using subsets of accessible data or drawing randomly  with replacement from a set of data points. Substituting labels on data points when performing significance tests. Validating models by using random subsets (bootstrapping, cross-validation) "
906,What are the differences between over-fitting and under-fitting?,"In overfitting, a statistical model describes random error or noise instead of the underlying relationship.  Overfitting occurs when a model is excessively complex, such as having too many parameters relative to  the number of observations. A model that has been overfitted, has poor predictive performance, as it  overreacts to minor fluctuations in the training data. Underfitting occurs when a statistical model or machine learning algorithm cannot capture the underlying  trend of the data. Underfitting would occur, for example, when fitting a linear model to non-linear data. Such  a model too would have poor predictive performance."
907, How to combat Overfitting and Underfitting?,"To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold  cross-validation) and by having a validation dataset to evaluate the model. "
908,What is regularisation? Why is it useful?,Regularisation is the process of adding tuning parameter to a model to induce smoothness in order to prevent  overfitting. This is most often done by adding a constant multiple to an existing weight vector. This constant  is often the L1(Lasso) or L2(ridge). The model predictions should then minimize the loss function calculated  on the regularized training set. 
909,What Is the Law of Large Numbers?,"It is a theorem that describes the result of performing the same experiment a large number of times. This  theorem forms the basis of frequency-style thinking. It says that the sample means, the sample variance  and the sample standard deviation converge to what they are trying to estimate."
910,What Are Confounding Variables?,"In statistics, a confounder is a variable that influences both the dependent variable and independent variable.A confounding variable here would be any other variable that affects both of these variables, such as the age  of the subject. "
911, What Are the Types of Biases That Can Occur During Sampling?,"Selection bias, Under coverage bias, Survivorship bias. "
912,What is Survivorship Bias?,It is the logical error of focusing aspects that support surviving some process and casually overlooking those  that did not work because of their lack of prominence. This can lead to wrong conclusions in numerous  different means. 
913,Explain how a ROC curve works?,The ROC curve is a graphical representation of the contrast between true positive rates and false-positive  rates at various thresholds. It is often used as a proxy for the trade-off between the sensitivity(true positive  rate) and false-positive rate.
914,What is TF/IDF vectorization?,"TF–IDF is short for term frequency-inverse document frequency, is a numerical statistic that is intended to  reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor  in information retrieval and text mining. The TF–IDF value increases proportionally to the number of times a word appears in the document but is  offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear  more frequently in general."
915,Python or R – Which one would you prefer for text analytics? ,We will prefer Python because of the following reasons:  Python would be the best option because it has Pandas library that provides easy to use data  structures and high-performance data analysis tools. R is more suitable for machine learning than just text analysis.  Python performs faster for all types of text analytics.
916,How does data cleaning plays a vital role in the analysis?,"Data cleaning can help in analysis because: Cleaning data from multiple sources helps to transform it into a format that data analysts or data  scientists can work with. Data Cleaning helps to increase the accuracy of the model in machine learning. It is a cumbersome process because as the number of data sources increases, the time taken to  clean the data increases exponentially due to the number of sources and the volume of data  generated by these sources. It might take up to 80% of the time for just cleaning data "
917,"Differentiate between univariate, bivariate and multivariate analysis.","Univariate analyses are descriptive statistical analysis techniques which can be differentiated based on the  number of variables involved at a given point of time. For example, the pie charts of sales based on territory  involve only one variable and can the analysis can be referred to as univariate analysis. The bivariate analysis attempts to understand the difference between two variables at a time as in a  scatterplot. For example, analyzing the volume of sale and spending can be considered as an example of  bivariate analysis. Multivariate analysis deals with the study of more than two variables to understand the effect of variables  on the responses."
918,What is Cluster Sampling? ,"Cluster sampling is a technique used when it becomes difficult to study the target population spread across  a wide area and simple random sampling cannot be applied. Cluster Sample is a probability sample where  each sampling unit is a collection or cluster of elements. For eg., A researcher wants to survey the academic performance of high school students in Japan. He can  divide the entire population of Japan into different clusters (cities). Then the researcher selects a number of  clusters depending on his research through simple or systematic random sampling. "
919,What is Systematic Sampling?,"Systematic sampling is a statistical technique where elements are selected from an ordered sampling frame.  In systematic sampling, the list is progressed in a circular manner so once you reach the end of the list, it is  progressed from the top again. The best example of systematic sampling is equal probability method."
920,What are Eigenvectors and Eigenvalues?,"Eigenvectors are used for understanding linear transformations. In data analysis, we usually calculate the  eigenvectors for a correlation or covariance matrix. Eigenvectors are the directions along which a particular  linear transformation acts by flipping, compressing or stretching. Eigenvalue can be referred to as the strength of the transformation in the direction of eigenvector or the  factor by which the compression occurs. "
921,Can you explain the difference between a Validation Set and a Test Set? ,"A Validation set can be considered as a part of the training set as it is used for parameter selection and to  avoid overfitting of the model being built. On the other hand, a Test Set is used for testing or evaluating the performance of a trained machine learning  model. In simple terms, the differences can be summarized as; training set is to fit the parameters i.e. weights and  test set is to assess the performance of the model i.e. evaluating the predictive power and generalization. "
922,Explain cross-validation. ,Cross-validation is a model validation technique for evaluating how the outcomes of statistical analysis  will generalize to an independent dataset. Mainly used in backgrounds where the objective is forecast and  one wants to estimate how accurately a model will accomplish in practice. The goal of cross-validation is to term a data set to test the model in the training phase (i.e. validation data  set) in order to limit problems like overfitting and get an insight on how the model will generalize to an  independent data set. 
923,What is ‘Naive’ in a Naive Bayes?,"The Naive Bayes Algorithm is based on the Bayes Theorem. Bayes’ theorem describes the probability of  an event, based on prior knowledge of conditions that might be related to the event. The Algorithm is ‘naive’ because it makes assumptions that may or may not turn out to be correct."
924,Explain SVM algorithm in detail. ,"SVM stands for support vector machine, it is a supervised machine learning algorithm which can be used  for both Regression and Classification. If you have n features in your training data set, SVM tries to plot it in n-dimensional space with the value of each feature being the value of a particular coordinate. SVM uses hyperplanes to separate out different classes based on the provided kernel function. "
925,What are the different kernels in SVM?,"There are four types of kernels in SVM. Linear Kernel, Polynomial kernel, Radial basis kernel, Sigmoid kernel."
926,Explain Decision Tree algorithm .,A decision tree is a supervised machine learning algorithm mainly used for Regression and  Classification. It breaks down a data set into smaller and smaller subsets while at the same time an  associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf  nodes. A decision tree can handle both categorical and numerical data. 
927,What is pruning in Decision Tree?,"Pruning is a technique in machine learning and search algorithms that reduces the size of decision  trees by removing sections of the tree that provide little power to classify instances. So, when we remove  sub-nodes of a decision node, this process is called pruning or opposite process of splitting. "
928,What is logistic regression?,Logistic Regression often referred to as the logit model is a technique to predict the binary outcome from  a linear combination of predictor variables. 
929, What Are the Drawbacks of the Linear Model? ,"Some drawbacks of the linear model are: The assumption of linearity of the errors,  It can’t be used for count outcomes or binary outcomes, There are overfitting problems that it can’t solve."
930, What is the difference between Regression and classification ML techniques?,"Both Regression and classification machine learning techniques come under Supervised machine  learning algorithms. In Supervised machine learning algorithm, we have to train the model using labelled  data set, While training we have to explicitly provide the correct labels and algorithm tries to learn the pattern  from input to output. If our labels are discrete values then it will a classification problem,"
931,What are Recommender Systems?,"Recommender Systems are a subclass of information filtering systems that are meant to predict the  preferences or ratings that a user would give to a product. Recommender systems are widely used in movies,  news, research articles, products, social tags, music, etc. "
932, What is Collaborative filtering?,"The process of filtering used by most of the recommender systems to find patterns or information by  collaborating viewpoints, various data sources and multiple agents."
933,How can outlier values be treated?,"Outlier values can be identified by using univariate or any other graphical analysis method. If the number of  outlier values is few then they can be assessed individually but for a large number of outliers, the values can  be substituted with either the 99th or the 1st percentile values. All extreme values are not outlier values. The most common ways to treat outlier values. To change the value and bring it within a range. To just remove the value."
934," During analysis, how do you treat missing values?","The extent of the missing values is identified after identifying the variables with missing values. If any  patterns are identified the analyst has to concentrate on them as it could lead to interesting and meaningful  business insights. If there are no patterns identified, then the missing values can be substituted with mean or median values  (imputation) or they can simply be ignored. Assigning a default value which can be mean, minimum or  maximum value. Getting into the data is important. If it is a categorical variable, the default value is assigned. The missing value is assigned a default value. If  you have a distribution of data coming, for normal distribution give the mean value. If 80% of the values for a variable are missing then you can answer that you would be dropping the variable  instead of treating the missing values."
935, What is Ensemble Learning?,Ensemble Learning is basically combining a diverse set of learners(Individual models) together to improvise  on the stability and predictive power of the model. 
936,What is bagging?,"Bagging tries to implement similar learners on small sample populations and then takes a mean of all the  predictions. In generalised bagging, you can use different learners on different population. As you expect  this helps us to reduce the variance error."
937,What is boosting?,"Boosting is an iterative technique which adjusts the weight of an observation based on the last  classification. If an observation was classified incorrectly, it tries to increase the weight of this observation  and vice versa. Boosting in general decreases the bias error and builds strong predictive models. However,  they may over fit on the training data. "
938,What is a Box-Cox Transformation?,"The dependent variable for a regression analysis might not satisfy one or more assumptions of an ordinary  least squares regression. The residuals could either curve as the prediction increases or follow the skewed  distribution. In such scenarios, it is necessary to transform the response variable so that the data meets  the required assumptions. A Box cox transformation is a statistical technique to transform non-normal  dependent variables into a normal shape. If the given data is not normal then most of the statistical techniques assume normality. Applying a box cox transformation means that you can run a broader  number of tests."
939,What do you mean by Deep Learning?,Deep Learning is nothing but a paradigm of machine learning which has shown incredible promise in recent  years. This is because of the fact that Deep Learning shows a great analogy with the functioning of the  human brain. 
940, What are Artificial Neural Networks?,Artificial Neural networks are a specific set of algorithms that have revolutionized machine learning. They  are inspired by biological neural networks. Neural Networks can adapt to changing the input so the network  generates the best possible result without needing to redesign the output criteria. 
941,Describe the structure of Artificial Neural Networks?,"Artificial Neural Networks works on the same principle as a biological Neural Network. It consists of inputs  which get processed with weighted sums and Bias, with the help of Activation Functions."
942,How Are Weights Initialized in a Network?,"There are two methods here: we can either initialize the weights to zero or assign them randomly.Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer  perform the same operation, giving the same output and making the deep net useless. Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to  0. It gives better accuracy to the model since every neuron performs different computations. This is the most  commonly used method. "
943,What Are Hyperparameters? ,"With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A  hyperparameter is a parameter whose value is set before the learning process begins. It determines how a  network is trained and the structure of the network (such as the number of hidden units, the learning rate,  epochs, etc.)."
944,What is Epoch?,It represent one iteration over the entire dataset (everything put into the training model). 
945,What is Batch?,"It refers to when we cannot pass the entire dataset into the neural network at once, so we  divide the dataset into several batches."
946,What are Recurrent Neural Networks(RNNs)?,"RNNs are a type of artificial neural networks designed to recognise the pattern from the sequence of data  such as Time series, stock market and government agencies etc. To understand recurrent nets, first, you  have to understand the basics of feedforward nets. Both these networks RNN and feed-forward named after the way they channel information through a series  of mathematical orations performed at the nodes of the network. One feeds information through  straight(never touching the same node twice), while the other cycles it through a loop, and the latter are  called recurrent."
947,What Is a Multi-layer Perceptron(MLP)?,"As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the  same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can  classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes. Except for the input layer, each node in the other  layers uses a nonlinear activation function. This means the input layers, the data coming in, and the  activation function is based upon all nodes and weights being added together, producing the output. MLP  uses a supervised learning method called “backpropagation.” In backpropagation, the neural network  calculates the error with the help of cost function. It propagates this error backward from where it came  (adjusts the weights to train the model more accurately)."
948,What is  exploding gradients?,"While training an RNN, if you see exponentially growing (very large) error gradients which accumulate  and result in very large updates to neural network model weights during training, they’re known as  exploding gradients. At an extreme, the values of weights can become so large as to overflow and result in  NaN values. This has the effect of your model is unstable and unable to learn from your training data. "
949,What is vanishing gradients? ,"While training an RNN, your slope can become either too small; this makes the training difficult. When the  slope is too small, the problem is known as a Vanishing Gradient. It leads to long training times, poor performance, and low accuracy. "
950,What is Back Propagation?,"Backpropagation is a training algorithm used for multilayer neural network. In this method, we move the  error from an end of the network to all weights inside the network and thus allowing efficient computation of  the gradient. "
951,What is the role of the Activation Function?,The Activation function is used to introduce non-linearity into the neural network helping it to learn more  complex function. Without which the neural network would be only able to learn linear function which is a  linear combination of its input data. An activation function is a function in an artificial neuron that delivers an  output based on inputs.
952,What is an Auto-Encoder? ,"Auto-encoders are simple learning networks that aim to transform inputs into outputs with the minimum  possible error. This means that we want the output to be as close to input as possible. We add a couple of  layers between the input and the output, and the sizes of these layers are smaller than the input layer. The  auto-encoder receives unlabelled input which is then encoded to reconstruct the input. "
953,What is a Boltzmann Machine?,Boltzmann machines have a simple learning algorithm that allows them to discover interesting features that  represent complex regularities in the training data. The Boltzmann machine is basically used to optimise the  weights and the quantity for the given problem. The learning algorithm is very slow in networks with many  layers of feature detectors. “Restricted Boltzmann Machines” algorithm has a single layer of feature  detectors which makes it faster than the rest.
954,What Is Dropout and Batch Normalization? ,Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting  of data (typically dropping 20 per cent of the nodes). It doubles the number of iterations needed to converge  the network. 
955, Why Is Tensorflow the Most Preferred Library in Deep Learning? ,"Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation  time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and  GPU computing devices. "
956,What Do You Mean by Tensor in Tensorflow?,A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with  different dimensions and ranks fed as input to the neural network are called “Tensors.”
957, What is the Computational Graph? ,"Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each  node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows  in the form of a graph, it is also called a “DataFlow Graph.”"
958,What are dimensionality reduction and its benefits?,"Dimensionality reduction refers to the process of converting a data set with vast dimensions into data  with fewer dimensions (fields) to convey similar information concisely. This reduction helps in compressing data and reducing storage space. It also reduces computation time as  fewer dimensions lead to less computing. It removes redundant features; for example, there's no point in  storing a value in two different units (meters and inches). "
959,How can you select k for k-means?,"We use the elbow method to select k for k-means clustering. The idea of the elbow method is to run k means clustering on the data set where 'k' is the number of clusters.Within the sum of squares (WSS), it is defined as the sum of the squared distance between each member  of the cluster and its centroid."
960,What are the feature vectors?,"A feature vector is an n-dimensional vector of numerical features that represent an object. In machine  learning, feature vectors are used to represent numeric or symbolic characteristics (called features) of an  object in a mathematical way that's easy to analyze."
961,What is root cause analysis?,Root cause analysis was initially developed to analyze industrial accidents but is now widely used in other  areas. It is a problem-solving technique used for isolating the root causes of faults or problems. A factor is  called a root cause if its deduction from the problem-fault-sequence averts the final undesirable event from  recurring. 
962,What is collaborative filtering?,"Most recommender systems use this filtering process to find patterns and information by collaborating  perspectives, numerous data sources, and several agents."
