{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[df1](https://www.analyticsvidhya.com/blog/2021/06/25-questions-to-test-your-skills-on-linear-regression-algorithm/)\n",
    "[df2](https://www.proschoolonline.com/blog/data-science-interview-questions-on-linear-regression)\n",
    "[df3](https://www.upgrad.com/blog/machine-learning-interview-questions-answers-ii/)\n",
    "[df4](https://www.analyticsvidhya.com/blog/2020/04/comprehensive-popular-deep-learning-interview-questions-answers/)\n",
    "[df5](https://www.javatpoint.com/deep-learning-interview-questions)\n",
    "[df6](https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-interview-questions)\n",
    "[df7](https://intellipaat.com/blog/interview-question/deep-learning-interview-questions/)\n",
    "[df8](https://towardsdatascience.com/50-deep-learning-interview-questions-part-1-2-8bbc8a00ec61)\n",
    "[df9](https://www.naukri.com/learning/articles/data-science-interview-questions-answers/)\n",
    "[df10](https://intellipaat.com/blog/interview-question/data-science-interview-questions/)\n",
    "[df11](https://www.edureka.co/blog/interview-questions/data-science-interview-questions/)\n",
    "[df12](https://www.projectpro.io/article/logistic-regression-interview-questions-/448)\n",
    "[df13](https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-logistic-regression/)\n",
    "[df14](https://towardsdatascience.com/five-data-science-interview-questions-that-you-must-be-able-to-answer-8f2ec53b409a)\n",
    "[df15](https://www.analyticsvidhya.com/blog/2021/05/25-questions-to-test-your-skills-on-decision-trees/)\n",
    "[df16](https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-k-nearest-neighbour/)\n",
    "[df17](https://www.analyticsvidhya.com/blog/2021/05/top-15-questions-to-test-your-data-science-skills-on-svm/)\n",
    "[df18](https://alekhyo.medium.com/interview-questions-on-svm-bf13e5fbcca8)\n",
    "[df19](https://towardsdatascience.com/support-vector-machine-svm-719e530a725f)\n",
    "[df20](https://medium.datadriveninvestor.com/support-vector-machines-important-questions-a47224692495)\n",
    "[df21](https://analyticsindiamag.com/top-xgboost-interview-questions-for-data-scientists/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\nStart Here\\n\\n\\n\\n\\nMachine Learning\\n\\n\\nDeep Learning\\n\\n\\nNLP\\n\\n\\n\\n\\n\\nArticles\\n\\n\\n\\nMachine Learning\\n\\n\\nDeep Learning\\n\\n\\nNLP\\n\\n\\nComputer Vision\\n\\n\\nData Visualization\\n\\n\\nCareers\\n\\n\\nMore\\n\\nInfographics\\nJobs\\nPodcasts\\nE-Books\\nFor Companies\\nDatahack Summit\\nDSAT\\nGlossary\\nArchive\\n\\n\\n Write an Article\\n\\n\\n\\n\\nCourses\\n\\n\\n\\nCertified AI & ML BlackBelt Plus\\n\\n\\nAscend Pro\\n\\n\\nData Science Immersive Bootcamp\\n\\n All Courses\\n\\n\\n\\n\\nJob-A-Thon\\n\\n',\n",
       " '\\n\\nMachine Learning\\n\\n\\nDeep Learning\\n\\n\\nNLP\\n\\n',\n",
       " '\\n\\nMachine Learning\\n\\n\\nDeep Learning\\n\\n\\nNLP\\n\\n\\nComputer Vision\\n\\n\\nData Visualization\\n\\n\\nCareers\\n\\n\\nMore\\n\\nInfographics\\nJobs\\nPodcasts\\nE-Books\\nFor Companies\\nDatahack Summit\\nDSAT\\nGlossary\\nArchive\\n\\n\\n Write an Article\\n',\n",
       " '\\n\\nCertified AI & ML BlackBelt Plus\\n\\n\\nAscend Pro\\n\\n\\nData Science Immersive Bootcamp\\n\\n All Courses\\n',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign in\\n\\n\\nJoin Now\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nManage your AV Account\\nMy Hackathons\\nMy Bookmarks\\nMy Courses\\nMy Applied Jobs\\n\\n\\nSign Out\\n \\n\\n\\n',\n",
       " '',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign in\\n\\n\\nJoin Now\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nManage your AV Account\\nMy Hackathons\\nMy Bookmarks\\nMy Courses\\nMy Applied Jobs\\n\\n\\nSign Out\\n \\n\\n\\n',\n",
       " '\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nTwitter\\n\\n\\n\\n\\n\\nLinkedin\\n\\n\\n\\n\\n\\nYoutube\\n\\n\\n',\n",
       " 'This article was published as a part of the\\xa0Data Science Blogathon',\n",
       " 'Introduction',\n",
       " 'Linear Regression, a supervised technique is one of the simplest Machine Learning algorithms. It\\xa0is a linear approach to modeling the relationship between a scalar response and one or more explanatory variables.',\n",
       " 'Therefore it becomes necessary for every aspiring Data Scientist and Machine Learning Engineer to have a good knowledge of the Linear Regression Algorithm.',\n",
       " 'In this article, we will discuss the most important questions on the Linear Regression Algorithm which is helpful to get you a clear understanding of the Algorithm, and also for Data Science Interviews, which covers its very fundamental level to complex concepts.',\n",
       " 'Let’s get started,',\n",
       " '\\xa0',\n",
       " '1. What is Linear Regression Algorithm?',\n",
       " 'In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.',\n",
       " 'In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Image Source: Google Images',\n",
       " '\\xa0',\n",
       " '2. How do you interpret a linear regression model?',\n",
       " 'As we know that the linear regression model is of the form:',\n",
       " '',\n",
       " 'The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).',\n",
       " 'Therefore, a linear regression model is quite easy to interpret.',\n",
       " 'For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered.',\n",
       " '3. What are the basic assumptions of the Linear Regression Algorithm?',\n",
       " 'The basic assumptions of the Linear regression algorithm are as follows:',\n",
       " '\\nLinearity: The relationship between the features and target.\\nHomoscedasticity: The error term has a constant variance.\\nMulticollinearity: There is no multicollinearity between the features.\\nIndependence: Observations are independent of each other.\\nNormality: The error(residuals) follows a normal distribution.\\n',\n",
       " 'Now, let’s break these assumptions into different categories:',\n",
       " 'It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’.',\n",
       " '\\nNormality assumption: The error terms, ε(i), are normally distributed.\\nZero mean assumption: The residuals have a mean value of zero.\\nConstant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity.\\nIndependent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.\\n',\n",
       " '\\nThe independent variables are measured without error.\\nThere does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data.\\n',\n",
       " '4. Explain the difference between Correlation and Regression.',\n",
       " 'Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.',\n",
       " 'Regression:\\xa0It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.',\n",
       " '5. Explain the Gradient Descent algorithm with respect to linear regression.',\n",
       " 'Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.',\n",
       " 'The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.',\n",
       " '',\n",
       " 'Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,',\n",
       " 'ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :',\n",
       " '',\n",
       " 'Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,',\n",
       " 'y is the target column or output, and m is the number of data points in the training set.',\n",
       " 'Step-1: Gradient Descent starts with a random solution,',\n",
       " 'Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.',\n",
       " 'The updated value for the parameter is given by the formulae:',\n",
       " 'Repeat until convergence(upto minimum loss function)',\n",
       " '',\n",
       " '6. Justify the cases where the linear regression algorithm is suitable for a given dataset.',\n",
       " 'Generally, a Scatter plot is used to see if linear regression is suitable for any given data. So, we can go for a linear model if the relationship looks somewhat linear. Plotting the scatter plots is easy in the case of simple or univariate linear regression.',\n",
       " 'But if we have more than one independent variable i.e, the case of multivariate linear regression, then two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted to find the suitableness.',\n",
       " 'On the contrary, to make the relationship linear we have to apply some transformations.',\n",
       " '7. List down some of the metrics used to evaluate a Regression Model.',\n",
       " 'Mainly, there are five metrics that are commonly used to evaluate the regression models:',\n",
       " '\\nMean Absolute Error(MAE)\\nMean Squared Error(MSE)\\nRoot Mean Squared Error(RMSE)\\nR-Squared(Coefficient of Determination)\\nAdjusted R-Squared\\n',\n",
       " '8. For a linear regression model, how do we interpret a Q-Q plot?',\n",
       " 'The Q-Q plot represents a graphical plotting of the quantiles of two distributions with respect to each other. In simple words, we plot quantiles against quantiles in the Q-Q plot which is used to check the normality of errors.',\n",
       " 'Whenever we interpret a Q-Q plot, we should concentrate on the ‘y = x’ line, which corresponds to a normal distribution. Sometimes, this line is also known as the 45-degree line in statistics.',\n",
       " 'It implies that each of the distributions has the same quantiles. In case you witness a deviation from this line, one of the distributions could be skewed when compared to the other i.e, normal distribution.',\n",
       " '9. In linear regression, what is the value of the sum of the residuals for a given dataset? Explain with proper justification.',\n",
       " 'The sum of the residuals in a linear regression model is 0 since it assumes that the errors (residuals) are normally distributed with an expected value or mean equal to 0, i.e.',\n",
       " 'Y = βT X + ε',\n",
       " 'Here, Y is the dependent variable or the target column, and β is the vector of the estimates of the regression coefficient,',\n",
       " 'X is the feature matrix containing all the features as the columns, ε is the residual term such that ε ~ N(0, σ2).',\n",
       " 'Moreover, the sum of all the residuals is calculated as the expected value of the residuals times the total number of observations in our dataset. Since the expectation of residuals is 0, therefore the sum of all the residual terms is zero.',\n",
       " 'Note: N(μ, σ2)\\xa0denotes the standard notation for a normal distribution having mean μ and standard deviation σ2.',\n",
       " '10. What are RMSE and MSE? How to calculate it?',\n",
       " 'RMSE and MSE are the two of the most common measures of accuracy for linear regression.',\n",
       " 'MSE (Mean Squared Error) is defined as the average of all the squared errors(residuals) for all data points. In simple words, we can say it is an average of squared differences between predicted and actual values.',\n",
       " 'RMSE (Root Mean Squared Error) is the square root of the average of squared differences between predicted and actual values.',\n",
       " 'RMSE stands for Root mean square error, which represented by the formulae:',\n",
       " '',\n",
       " 'MSE stands for Mean square error, which represented by the formulae:',\n",
       " '',\n",
       " 'Increment in RMSE is larger than MAE as the test sample size increases. In general, as the variance of error magnitudes increase, MAE remains steady but RMSE increases.',\n",
       " '11. What is OLS?',\n",
       " 'OLS stands for Ordinary Least Squares. The main objective of the linear regression algorithm is to find coefficients or estimates by minimizing the error term i.e, the sum of squared errors. This process is known as OLS.',\n",
       " 'This method finds the best fit line, known as regression line by minimizing the sum of square differences between the observed and predicted values.',\n",
       " '12. What are MAE and MAPE?',\n",
       " 'MAE stands for Mean Absolute Error, which is defined as the average of absolute or positive errors of all values. In simple words, we can say MAE is an average of absolute or positive differences between predicted values and the actual values.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Image Source: Google Images',\n",
       " 'MAPE stands for Mean Absolute Percent Error, which calculates the average absolute error in percentage terms. In simple words, It can be understood as the percentage average of absolute or positive errors.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 Image Source: Google Images',\n",
       " '\\xa0',\n",
       " '13. Why do we square the residuals instead of using modulus?',\n",
       " 'This question can be understood that why one should prefer the absolute error instead of the squared error.',\n",
       " '1. In fact, the absolute error is often closer to what we want when making predictions from our model. But, if we want to penalize those predictions that are contributing to the maximum value of error.',\n",
       " '2. Moreover in mathematical terms, the squared function is differentiable everywhere, while the absolute error is not differentiable at all the points in its domain(its derivative is undefined at 0). This makes the squared error more preferable to the techniques of mathematical optimization. To optimize the squared error, we can compute the derivative and set its expression equal to 0, and solve. But to optimize the absolute error, we require more complex techniques having more computations.',\n",
       " '3. Actually, we use the Root Mean Squared Error instead of Mean squared error so that the unit of RMSE and the dependent variable are equal and results are interpretable.',\n",
       " '14. List down the techniques that are adopted to find the parameters of the linear regression line which best fits the model.',\n",
       " 'There are mainly two methods used for linear regression:',\n",
       " '1. Ordinary Least Squares(Statistics domain): ',\n",
       " 'To implement this in Scikit-learn we have to use the LinearRegression() class.',\n",
       " '2. Gradient Descent(Calculus family):',\n",
       " 'To implement this in Scikit-learn we have to use the SGDRegressor() class.',\n",
       " '\\xa0',\n",
       " '15. Which evaluation metric should you prefer to use for a dataset having a lot of outliers in it?',\n",
       " 'Mean Absolute Error(MAE) is preferred when we have too many outliers present in the dataset because MAE is robust to outliers whereas MSE and RMSE are very susceptible to outliers and these start penalizing the outliers by squaring the error terms, commonly known as residuals.',\n",
       " '16. Explain the normal form equation of the linear regression.',\n",
       " 'The normal equation for linear regression is :',\n",
       " 'β=(XTX)-1XTY',\n",
       " 'This is also known as the closed-form solution for a linear regression model.',\n",
       " 'where,',\n",
       " 'Y=βTX is the equation that represents the model for the linear regression,',\n",
       " 'Y is the dependent variable or target column,',\n",
       " 'β is the vector of the estimates of the regression coefficient, which is arrived at using the normal equation,',\n",
       " 'X is the feature matrix that contains all the features in the form of columns. The thing to note down here is that the first column in the X matrix consists of all 1s, to incorporate the offset value for the regression line.',\n",
       " '17. When should it be preferred to the Gradient Descent method instead of the Normal Equation in Linear Regression Algorithm?',\n",
       " 'To answer the given question, let’s first understand the difference between the Normal equation and Gradient descent method for linear regression:',\n",
       " '\\nNeeds hyper-parameter tuning for alpha (learning parameter).\\nIt is an iterative process.\\nTime complexity- O(kn2)\\nPreferred when n is extremely large.\\n',\n",
       " '\\nNo such need for any hyperparameter.\\nIt is a non-iterative process.\\nTime complexity- O(n3) due to evaluation of XTX.\\nBecomes quite slow for large values of n.\\n',\n",
       " 'where,',\n",
       " '‘k’ represents the maximum number of iterations used for the gradient descent algorithm, and',\n",
       " '‘n’ is the total number of observations present in the training dataset.',\n",
       " 'Clearly, if we have large training data, a normal equation is not preferred for use due to very high time complexity but for small values of ‘n’, the normal equation is faster than gradient descent.',\n",
       " '18. What are R-squared and Adjusted R-squared?',\n",
       " 'R-square (R2), also known as the coefficient of determination measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model.',\n",
       " '',\n",
       " 'The main problem with the R-squared is that it will always remain the same or increases as we are adding more independent variables. Therefore, to overcome this problem, an Adjusted-R2 square comes into the picture by penalizing those adding independent variables that do not improve your existing model.',\n",
       " 'To learn more about, R2 and adjusted-R2, refer to the link.',\n",
       " '19. What are the flaws in R-squared?',\n",
       " 'There are two major flaws of R-squared:',\n",
       " 'Problem- 1: As we are adding more and more predictors, R² always increases irrespective of the impact of the predictor on the model. As R² always increases and never decreases, it can always appear to be a better fit with the more independent variables(predictors) we add to the model. This can be completely misleading.',\n",
       " 'Problem- 2: Similarly, if our model has too many independent variables and too many high-order polynomials, we can also face the problem of over-fitting the data. Whenever the data is over-fitted, it can lead to a misleadingly high R² value which eventually can lead to misleading predictions.',\n",
       " 'To learn more about, flaws of R2, refer to the link.',\n",
       " '20. What is Multicollinearity?',\n",
       " 'It is a phenomenon where two or more independent variables(predictors) are highly correlated with each other i.e. one variable can be linearly predicted with the help of other variables. It determines the inter-correlations and inter-association among independent variables. Sometimes, multicollinearity can also be known as collinearity.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Image Source: Google Images',\n",
       " '\\nInaccurate use of dummy variables.\\nDue to a variable that can be computed from the other variable in the dataset.\\n',\n",
       " '\\nImpacts regression coefficients i.e, coefficients become indeterminate.\\nCauses high standard errors.\\n',\n",
       " '\\nBy using the correlation coefficient.\\nWith the help of Variance inflation factor (VIF), and Eigenvalues.\\n',\n",
       " 'To learn more about, multicollinearity, refer to the link.',\n",
       " '21. What is Heteroscedasticity? How to detect it?',\n",
       " 'It refers to the situation where the variations in a particular independent variable are unequal across the range of values of a second variable that tries to predict it.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 Image Source: Google Images',\n",
       " 'To detect heteroscedasticity, we can use graphs or statistical tests such as the Breush-Pagan test and NCV test, etc.',\n",
       " '22. What are the disadvantages of the linear regression Algorithm?',\n",
       " 'The main disadvantages of linear regression are as follows:',\n",
       " '\\nAssumption of linearity: It assumes that there exists a linear relationship between the independent variables(input) and dependent variables (output), therefore we are not able to fit the complex problems with the help of a linear regression algorithm.\\nOutliers: It is sensitive to noise and outliers.\\nMulticollinearity: It gets affected by multicollinearity.\\n',\n",
       " '23. What is VIF? How do you calculate it?',\n",
       " 'VIF stands for Variance inflation factor, which measures how much variance of an estimated regression coefficient is increased due to the presence of collinearity between the variables. It also determines how much multicollinearity exists in a particular regression model.',\n",
       " 'Firstly, it applies the ordinary least square method of regression that has Xi as a function of all the other explanatory or independent variables and then calculates VIF using the given below mathematical formula:',\n",
       " '',\n",
       " '',\n",
       " '24. How is Hypothesis testing used in Linear Regression Algorithm?',\n",
       " 'For the following purposes, we can carry out the Hypothesis testing in linear regression:',\n",
       " '1. To check whether an independent variable (predictor) is significant or not for the prediction of the target variable. Two common methods for this are —',\n",
       " 'If the p-value of a particular independent variable is greater than a certain threshold (usually 0.05), then that independent variable is insignificant for the prediction of the target variable.',\n",
       " 'If the value of the regression coefficient corresponding to a particular independent variable is zero, then that variable is insignificant for the predictions of the dependent variable and has no linear relationship with it.',\n",
       " '2. To verify whether the calculated regression coefficients i.e, with the help of linear regression algorithm, are good estimators or not of the actual coefficients.',\n",
       " '25. Is it possible to apply Linear Regression for Time Series Analysis?',\n",
       " 'Yes, we can apply a linear regression algorithm for doing analysis on time series data, but the results are not promising and hence is not advisable to do so.',\n",
       " 'The reasons behind not preferable linear regression on time-series data are as follows:',\n",
       " '\\nTime series data is mostly used for the prediction of the future but in contrast, linear regression generally seldom gives good results for future prediction as it is basically not meant for extrapolation.\\nMoreover, time-series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis.\\n',\n",
       " 'End Notes',\n",
       " 'Thanks for reading!',\n",
       " 'I hope you enjoyed the questions and were able to test your knowledge about Linear Regression Algorithm.',\n",
       " 'If you liked this and want to know more, go visit my other articles on Data Science and Machine Learning by clicking on the Link',\n",
       " 'Please feel free to contact me on Linkedin, Email.',\n",
       " 'Something not mentioned or want to share your thoughts? Feel free to comment below And I’ll get back to you.',\n",
       " 'Chirag Goyal',\n",
       " 'Currently, I am pursuing my Bachelor of Technology (B.Tech) in Computer Science and Engineering from the Indian Institute of Technology Jodhpur(IITJ). I am very enthusiastic about Machine learning, Deep Learning, and Artificial Intelligence.',\n",
       " 'The media shown in this article on Sign Language Recognition are not owned by Analytics Vidhya and are used at the Author’s discretion.',\n",
       " '\\n',\n",
       " '',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " ' Notify me of follow-up comments by email.',\n",
       " ' Notify me of new posts by email.',\n",
       " '',\n",
       " '',\n",
       " 'Top Resources',\n",
       " 'Basic Concepts of Object-Oriented Programming in Python',\n",
       " 'Python Tutorial: Working with CSV file for Data Science',\n",
       " 'Commonly used Machine Learning Algorithms (with Python and R Codes)',\n",
       " '3 Interesting Python Projects With Code for Beginners!',\n",
       " '\\n\\n\\n×\\n\\n',\n",
       " '\\n\\nAnalytics Vidhya\\nAbout Us\\nOur Team\\nCareers\\nContact us\\n\\n\\nData Scientists\\nBlog\\nHackathon\\nDiscussions\\nApply Jobs\\n\\n\\nCompanies\\nPost Jobs\\nTrainings\\nHiring Hackathons\\nAdvertising\\n\\n\\nVisit us\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '© Copyright 2013-2021 Analytics Vidhya.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#linear-regression\n",
    "lst1 = []\n",
    "url = \"https://www.analyticsvidhya.com/blog/2021/06/25-questions-to-test-your-skills-on-linear-regression-algorithm/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h2','p','ul'])\n",
    "for answer in answers:\n",
    "    lst1.append(answer.text)\n",
    "lst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is Linear Regression Algorithm?',\n",
       " 'In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.',\n",
       " 'In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Image Source: Google Images',\n",
       " '\\xa0',\n",
       " '2. How do you interpret a linear regression model?',\n",
       " 'As we know that the linear regression model is of the form:',\n",
       " '',\n",
       " 'The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).',\n",
       " 'Therefore, a linear regression model is quite easy to interpret.',\n",
       " 'For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered.',\n",
       " '3. What are the basic assumptions of the Linear Regression Algorithm?',\n",
       " 'The basic assumptions of the Linear regression algorithm are as follows:',\n",
       " '\\nLinearity: The relationship between the features and target.\\nHomoscedasticity: The error term has a constant variance.\\nMulticollinearity: There is no multicollinearity between the features.\\nIndependence: Observations are independent of each other.\\nNormality: The error(residuals) follows a normal distribution.\\n',\n",
       " 'Now, let’s break these assumptions into different categories:',\n",
       " 'It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’.',\n",
       " '\\nNormality assumption: The error terms, ε(i), are normally distributed.\\nZero mean assumption: The residuals have a mean value of zero.\\nConstant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity.\\nIndependent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.\\n',\n",
       " '\\nThe independent variables are measured without error.\\nThere does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data.\\n',\n",
       " '4. Explain the difference between Correlation and Regression.',\n",
       " 'Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.',\n",
       " 'Regression:\\xa0It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.',\n",
       " '5. Explain the Gradient Descent algorithm with respect to linear regression.',\n",
       " 'Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.',\n",
       " 'The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.',\n",
       " '',\n",
       " 'Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,',\n",
       " 'ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :',\n",
       " '',\n",
       " 'Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,',\n",
       " 'y is the target column or output, and m is the number of data points in the training set.',\n",
       " 'Step-1: Gradient Descent starts with a random solution,',\n",
       " 'Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.',\n",
       " 'The updated value for the parameter is given by the formulae:',\n",
       " 'Repeat until convergence(upto minimum loss function)',\n",
       " '',\n",
       " '6. Justify the cases where the linear regression algorithm is suitable for a given dataset.',\n",
       " 'Generally, a Scatter plot is used to see if linear regression is suitable for any given data. So, we can go for a linear model if the relationship looks somewhat linear. Plotting the scatter plots is easy in the case of simple or univariate linear regression.',\n",
       " 'But if we have more than one independent variable i.e, the case of multivariate linear regression, then two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted to find the suitableness.',\n",
       " 'On the contrary, to make the relationship linear we have to apply some transformations.',\n",
       " '7. List down some of the metrics used to evaluate a Regression Model.',\n",
       " 'Mainly, there are five metrics that are commonly used to evaluate the regression models:',\n",
       " '\\nMean Absolute Error(MAE)\\nMean Squared Error(MSE)\\nRoot Mean Squared Error(RMSE)\\nR-Squared(Coefficient of Determination)\\nAdjusted R-Squared\\n',\n",
       " '8. For a linear regression model, how do we interpret a Q-Q plot?',\n",
       " 'The Q-Q plot represents a graphical plotting of the quantiles of two distributions with respect to each other. In simple words, we plot quantiles against quantiles in the Q-Q plot which is used to check the normality of errors.',\n",
       " 'Whenever we interpret a Q-Q plot, we should concentrate on the ‘y = x’ line, which corresponds to a normal distribution. Sometimes, this line is also known as the 45-degree line in statistics.',\n",
       " 'It implies that each of the distributions has the same quantiles. In case you witness a deviation from this line, one of the distributions could be skewed when compared to the other i.e, normal distribution.',\n",
       " '9. In linear regression, what is the value of the sum of the residuals for a given dataset? Explain with proper justification.',\n",
       " 'The sum of the residuals in a linear regression model is 0 since it assumes that the errors (residuals) are normally distributed with an expected value or mean equal to 0, i.e.',\n",
       " 'Y = βT X + ε',\n",
       " 'Here, Y is the dependent variable or the target column, and β is the vector of the estimates of the regression coefficient,',\n",
       " 'X is the feature matrix containing all the features as the columns, ε is the residual term such that ε ~ N(0, σ2).',\n",
       " 'Moreover, the sum of all the residuals is calculated as the expected value of the residuals times the total number of observations in our dataset. Since the expectation of residuals is 0, therefore the sum of all the residual terms is zero.',\n",
       " 'Note: N(μ, σ2)\\xa0denotes the standard notation for a normal distribution having mean μ and standard deviation σ2.',\n",
       " '10. What are RMSE and MSE? How to calculate it?',\n",
       " 'RMSE and MSE are the two of the most common measures of accuracy for linear regression.',\n",
       " 'MSE (Mean Squared Error) is defined as the average of all the squared errors(residuals) for all data points. In simple words, we can say it is an average of squared differences between predicted and actual values.',\n",
       " 'RMSE (Root Mean Squared Error) is the square root of the average of squared differences between predicted and actual values.',\n",
       " 'RMSE stands for Root mean square error, which represented by the formulae:',\n",
       " '',\n",
       " 'MSE stands for Mean square error, which represented by the formulae:',\n",
       " '',\n",
       " 'Increment in RMSE is larger than MAE as the test sample size increases. In general, as the variance of error magnitudes increase, MAE remains steady but RMSE increases.',\n",
       " '11. What is OLS?',\n",
       " 'OLS stands for Ordinary Least Squares. The main objective of the linear regression algorithm is to find coefficients or estimates by minimizing the error term i.e, the sum of squared errors. This process is known as OLS.',\n",
       " 'This method finds the best fit line, known as regression line by minimizing the sum of square differences between the observed and predicted values.',\n",
       " '12. What are MAE and MAPE?',\n",
       " 'MAE stands for Mean Absolute Error, which is defined as the average of absolute or positive errors of all values. In simple words, we can say MAE is an average of absolute or positive differences between predicted values and the actual values.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Image Source: Google Images',\n",
       " 'MAPE stands for Mean Absolute Percent Error, which calculates the average absolute error in percentage terms. In simple words, It can be understood as the percentage average of absolute or positive errors.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 Image Source: Google Images',\n",
       " '\\xa0',\n",
       " '13. Why do we square the residuals instead of using modulus?',\n",
       " 'This question can be understood that why one should prefer the absolute error instead of the squared error.',\n",
       " '1. In fact, the absolute error is often closer to what we want when making predictions from our model. But, if we want to penalize those predictions that are contributing to the maximum value of error.',\n",
       " '2. Moreover in mathematical terms, the squared function is differentiable everywhere, while the absolute error is not differentiable at all the points in its domain(its derivative is undefined at 0). This makes the squared error more preferable to the techniques of mathematical optimization. To optimize the squared error, we can compute the derivative and set its expression equal to 0, and solve. But to optimize the absolute error, we require more complex techniques having more computations.',\n",
       " '3. Actually, we use the Root Mean Squared Error instead of Mean squared error so that the unit of RMSE and the dependent variable are equal and results are interpretable.',\n",
       " '14. List down the techniques that are adopted to find the parameters of the linear regression line which best fits the model.',\n",
       " 'There are mainly two methods used for linear regression:',\n",
       " '1. Ordinary Least Squares(Statistics domain): ',\n",
       " 'To implement this in Scikit-learn we have to use the LinearRegression() class.',\n",
       " '2. Gradient Descent(Calculus family):',\n",
       " 'To implement this in Scikit-learn we have to use the SGDRegressor() class.',\n",
       " '\\xa0',\n",
       " '15. Which evaluation metric should you prefer to use for a dataset having a lot of outliers in it?',\n",
       " 'Mean Absolute Error(MAE) is preferred when we have too many outliers present in the dataset because MAE is robust to outliers whereas MSE and RMSE are very susceptible to outliers and these start penalizing the outliers by squaring the error terms, commonly known as residuals.',\n",
       " '16. Explain the normal form equation of the linear regression.',\n",
       " 'The normal equation for linear regression is :',\n",
       " 'β=(XTX)-1XTY',\n",
       " 'This is also known as the closed-form solution for a linear regression model.',\n",
       " 'where,',\n",
       " 'Y=βTX is the equation that represents the model for the linear regression,',\n",
       " 'Y is the dependent variable or target column,',\n",
       " 'β is the vector of the estimates of the regression coefficient, which is arrived at using the normal equation,',\n",
       " 'X is the feature matrix that contains all the features in the form of columns. The thing to note down here is that the first column in the X matrix consists of all 1s, to incorporate the offset value for the regression line.',\n",
       " '17. When should it be preferred to the Gradient Descent method instead of the Normal Equation in Linear Regression Algorithm?',\n",
       " 'To answer the given question, let’s first understand the difference between the Normal equation and Gradient descent method for linear regression:',\n",
       " '\\nNeeds hyper-parameter tuning for alpha (learning parameter).\\nIt is an iterative process.\\nTime complexity- O(kn2)\\nPreferred when n is extremely large.\\n',\n",
       " '\\nNo such need for any hyperparameter.\\nIt is a non-iterative process.\\nTime complexity- O(n3) due to evaluation of XTX.\\nBecomes quite slow for large values of n.\\n',\n",
       " 'where,',\n",
       " '‘k’ represents the maximum number of iterations used for the gradient descent algorithm, and',\n",
       " '‘n’ is the total number of observations present in the training dataset.',\n",
       " 'Clearly, if we have large training data, a normal equation is not preferred for use due to very high time complexity but for small values of ‘n’, the normal equation is faster than gradient descent.',\n",
       " '18. What are R-squared and Adjusted R-squared?',\n",
       " 'R-square (R2), also known as the coefficient of determination measures the proportion of the variation in your dependent variable (Y) explained by your independent variables (X) for a linear regression model.',\n",
       " '',\n",
       " 'The main problem with the R-squared is that it will always remain the same or increases as we are adding more independent variables. Therefore, to overcome this problem, an Adjusted-R2 square comes into the picture by penalizing those adding independent variables that do not improve your existing model.',\n",
       " 'To learn more about, R2 and adjusted-R2, refer to the link.',\n",
       " '19. What are the flaws in R-squared?',\n",
       " 'There are two major flaws of R-squared:',\n",
       " 'Problem- 1: As we are adding more and more predictors, R² always increases irrespective of the impact of the predictor on the model. As R² always increases and never decreases, it can always appear to be a better fit with the more independent variables(predictors) we add to the model. This can be completely misleading.',\n",
       " 'Problem- 2: Similarly, if our model has too many independent variables and too many high-order polynomials, we can also face the problem of over-fitting the data. Whenever the data is over-fitted, it can lead to a misleadingly high R² value which eventually can lead to misleading predictions.',\n",
       " 'To learn more about, flaws of R2, refer to the link.',\n",
       " '20. What is Multicollinearity?',\n",
       " 'It is a phenomenon where two or more independent variables(predictors) are highly correlated with each other i.e. one variable can be linearly predicted with the help of other variables. It determines the inter-correlations and inter-association among independent variables. Sometimes, multicollinearity can also be known as collinearity.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0Image Source: Google Images',\n",
       " '\\nInaccurate use of dummy variables.\\nDue to a variable that can be computed from the other variable in the dataset.\\n',\n",
       " '\\nImpacts regression coefficients i.e, coefficients become indeterminate.\\nCauses high standard errors.\\n',\n",
       " '\\nBy using the correlation coefficient.\\nWith the help of Variance inflation factor (VIF), and Eigenvalues.\\n',\n",
       " 'To learn more about, multicollinearity, refer to the link.',\n",
       " '21. What is Heteroscedasticity? How to detect it?',\n",
       " 'It refers to the situation where the variations in a particular independent variable are unequal across the range of values of a second variable that tries to predict it.',\n",
       " '',\n",
       " '\\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 \\xa0 Image Source: Google Images',\n",
       " 'To detect heteroscedasticity, we can use graphs or statistical tests such as the Breush-Pagan test and NCV test, etc.',\n",
       " '22. What are the disadvantages of the linear regression Algorithm?',\n",
       " 'The main disadvantages of linear regression are as follows:',\n",
       " '\\nAssumption of linearity: It assumes that there exists a linear relationship between the independent variables(input) and dependent variables (output), therefore we are not able to fit the complex problems with the help of a linear regression algorithm.\\nOutliers: It is sensitive to noise and outliers.\\nMulticollinearity: It gets affected by multicollinearity.\\n',\n",
       " '23. What is VIF? How do you calculate it?',\n",
       " 'VIF stands for Variance inflation factor, which measures how much variance of an estimated regression coefficient is increased due to the presence of collinearity between the variables. It also determines how much multicollinearity exists in a particular regression model.',\n",
       " 'Firstly, it applies the ordinary least square method of regression that has Xi as a function of all the other explanatory or independent variables and then calculates VIF using the given below mathematical formula:',\n",
       " '',\n",
       " '',\n",
       " '24. How is Hypothesis testing used in Linear Regression Algorithm?',\n",
       " 'For the following purposes, we can carry out the Hypothesis testing in linear regression:',\n",
       " '1. To check whether an independent variable (predictor) is significant or not for the prediction of the target variable. Two common methods for this are —',\n",
       " 'If the p-value of a particular independent variable is greater than a certain threshold (usually 0.05), then that independent variable is insignificant for the prediction of the target variable.',\n",
       " 'If the value of the regression coefficient corresponding to a particular independent variable is zero, then that variable is insignificant for the predictions of the dependent variable and has no linear relationship with it.',\n",
       " '2. To verify whether the calculated regression coefficients i.e, with the help of linear regression algorithm, are good estimators or not of the actual coefficients.',\n",
       " '25. Is it possible to apply Linear Regression for Time Series Analysis?',\n",
       " 'Yes, we can apply a linear regression algorithm for doing analysis on time series data, but the results are not promising and hence is not advisable to do so.',\n",
       " 'The reasons behind not preferable linear regression on time-series data are as follows:',\n",
       " '\\nTime series data is mostly used for the prediction of the future but in contrast, linear regression generally seldom gives good results for future prediction as it is basically not meant for extrapolation.\\nMoreover, time-series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis.\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new1 = lst1[15:161]\n",
    "lst_new1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.                                                       Image Source: Google Images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is of the form:The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).Therefore, a linear regression model is quite easy to interpret.For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the basic assumptions of the Linear Regression Algorithm?</td>\n",
       "      <td>The basic assumptions of the Linear regression algorithm are as follows: Linearity: The relationship between the features and target. Homoscedasticity: The error term has a constant variance. Multicollinearity: There is no multicollinearity between the features. Independence: Observations are independent of each other. Normality: The error(residuals) follows a normal distribution. Now, let’s break these assumptions into different categories:It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’. Normality assumption: The error terms, ε(i), are normally distributed. Zero mean assumption: The residuals have a mean value of zero. Constant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity. Independent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.  The independent variables are measured without error. There does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the difference between Correlation and Regression.</td>\n",
       "      <td>Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.Regression: It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the Gradient Descent algorithm with respect to linear regression.</td>\n",
       "      <td>Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,y is the target column or output, and m is the number of data points in the training set.Step-1: Gradient Descent starts with a random solution,Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.The updated value for the parameter is given by the formulae:Repeat until convergence(upto minimum loss function)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Questions  \\\n",
       "0                                         What is Linear Regression Algorithm?   \n",
       "1                              How do you interpret a linear regression model?   \n",
       "2           What are the basic assumptions of the Linear Regression Algorithm?   \n",
       "3                   Explain the difference between Correlation and Regression.   \n",
       "4    Explain the Gradient Descent algorithm with respect to linear regression.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.                                                       Image Source: Google Images   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            As we know that the linear regression model is of the form:The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).Therefore, a linear regression model is quite easy to interpret.For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered.  \n",
       "2  The basic assumptions of the Linear regression algorithm are as follows: Linearity: The relationship between the features and target. Homoscedasticity: The error term has a constant variance. Multicollinearity: There is no multicollinearity between the features. Independence: Observations are independent of each other. Normality: The error(residuals) follows a normal distribution. Now, let’s break these assumptions into different categories:It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’. Normality assumption: The error terms, ε(i), are normally distributed. Zero mean assumption: The residuals have a mean value of zero. Constant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity. Independent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.  The independent variables are measured without error. There does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.Regression: It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.  \n",
       "4                                                                                   Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,y is the target column or output, and m is the number of data points in the training set.Step-1: Gradient Descent starts with a random solution,Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.The updated value for the parameter is given by the formulae:Repeat until convergence(upto minimum loss function)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new1:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df1=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df1[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.drop(df1.index[[12,13,14,15,16,28,29,30]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[5:]\n",
    "# df1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'It’s completely unacceptable to overlook the significance of data and our ability to analyze, combine, and contextualize it. Data scientists are relied upon to satisfy this need, but there is a lack of qualified candidates.',\n",
       " 'If you want to be a data scientist, you are required to be prepared to impress considered companies with your knowledge. In addition to describing why data science is so valuable, you need to explain that you are technically skilled with all the concepts, frameworks, and applications. Below is the list of eight of the most common questions you can foresee in an interview and how to compose your answers.',\n",
       " 'It’s completely unacceptable to overlook the significance of data and our ability to analyze, combine, and contextualize it. Data scientists are relied upon to satisfy this need, but there is a lack of qualified candidates.',\n",
       " 'If you want to be a data scientist, you are required to be prepared to impress considered companies with your knowledge. In addition to describing why data science is so valuable, you need to explain that you are technically skilled with all the concepts, frameworks, and applications. Below is the list of eight of the most common questions you can foresee in an interview.',\n",
       " '1. What are the important assumptions of Linear regression?',\n",
       " 'A linear relationship',\n",
       " 'Restricted Multi-collinearity value',\n",
       " 'Homoscedasticity',\n",
       " 'Firstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.',\n",
       " 'Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.',\n",
       " 'The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed.',\n",
       " '2. What is heteroscedasticity?',\n",
       " 'Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used.',\n",
       " '3. What is the difference between R square and adjusted R square?',\n",
       " 'R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. i.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables(P values less than 0.05) to indicate the percentage of variation in the model.',\n",
       " '4. How to find RMSE and MSE?',\n",
       " 'RMSE and MSE are the two of the most common measures of accuracy for a linear regression.',\n",
       " 'RMSE indicates the Root mean square error, which indicated by the formulae:',\n",
       " '',\n",
       " 'Where MSE indicates the Mean square error represented by the formulae:',\n",
       " '5. What are the possible ways of improving the accuracy of a linear regression model?',\n",
       " 'There could be multiple ways of improving the accuracy of a linear regression, most commonly used ways are as follows:',\n",
       " '-Regression is sensitive to outliers, hence it becomes very important to treat the outliers with appropriate values. Replacing the values with mean, median, mode or percentile depending on the distribution can prove to be useful.',\n",
       " '6. How to interpret a Q-Q plot in a Linear regression model?',\n",
       " 'A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors.',\n",
       " '7. What is the significance of an F-test in a linear model?',\n",
       " '– The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression.',\n",
       " '8.\\xa0What are the disadvantages of the linear model?',\n",
       " '– Linear regression is sensitive to outliers which may affect the result.',\n",
       " '– Over-fitting',\n",
       " '– Under-fitting',\n",
       " '\\xa0',\n",
       " '\\xa0',\n",
       " 'Check your inbox or spam folder to confirm your subscription.\\n        ',\n",
       " ' ',\n",
       " 'Proschool, is an Initiative of IMS. At IMS our goal for the past 40 years has been simple- “To Build a Long Term Successful Career”. Proschool is an Extension of the same mission. Our endeavor is to get students into jobs that gives them useful experience for a long term successful career. We currently provide training programs in Finance, Accounting, Analytics and Marketing with 15+ centres all over India.',\n",
       " '\\xa07710044425',\n",
       " '\\xa0info@proschoolonline.com',\n",
       " '\\xa0\\xa0\\xa0',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Select your CityBangaloreChennaiDelhiGurgaonHyderabadKochiMumbaiPuneOthers',\n",
       " '',\n",
       " 'Submit',\n",
       " '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst2 = []\n",
    "url = \"https://www.proschoolonline.com/blog/data-science-interview-questions-on-linear-regression\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['p'])\n",
    "for answer in answers:\n",
    "    lst2.append(answer.text)\n",
    "lst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are the important assumptions of Linear regression?',\n",
       " 'A linear relationship',\n",
       " 'Restricted Multi-collinearity value',\n",
       " 'Homoscedasticity',\n",
       " 'Firstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.',\n",
       " 'Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.',\n",
       " 'The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed.',\n",
       " '2. What is heteroscedasticity?',\n",
       " 'Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used.',\n",
       " '3. What is the difference between R square and adjusted R square?',\n",
       " 'R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. i.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables(P values less than 0.05) to indicate the percentage of variation in the model.',\n",
       " '4. How to find RMSE and MSE?',\n",
       " 'RMSE and MSE are the two of the most common measures of accuracy for a linear regression.',\n",
       " 'RMSE indicates the Root mean square error, which indicated by the formulae:',\n",
       " '',\n",
       " 'Where MSE indicates the Mean square error represented by the formulae:',\n",
       " '5. What are the possible ways of improving the accuracy of a linear regression model?',\n",
       " 'There could be multiple ways of improving the accuracy of a linear regression, most commonly used ways are as follows:',\n",
       " '-Regression is sensitive to outliers, hence it becomes very important to treat the outliers with appropriate values. Replacing the values with mean, median, mode or percentile depending on the distribution can prove to be useful.',\n",
       " '6. How to interpret a Q-Q plot in a Linear regression model?',\n",
       " 'A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors.',\n",
       " '7. What is the significance of an F-test in a linear model?',\n",
       " '– The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression.',\n",
       " '8.\\xa0What are the disadvantages of the linear model?',\n",
       " '– Linear regression is sensitive to outliers which may affect the result.',\n",
       " '– Over-fitting',\n",
       " '– Under-fitting']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new2 = lst2[5:32]\n",
    "lst_new2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the important assumptions of Linear regression?</td>\n",
       "      <td>A linear relationshipRestricted Multi-collinearity valueHomoscedasticityFirstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is heteroscedasticity?</td>\n",
       "      <td>Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the difference between R square and adjusted R square?</td>\n",
       "      <td>R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. i.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables(P values less than 0.05) to indicate the percentage of variation in the model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How to find RMSE and MSE?</td>\n",
       "      <td>RMSE and MSE are the two of the most common measures of accuracy for a linear regression.RMSE indicates the Root mean square error, which indicated by the formulae:Where MSE indicates the Mean square error represented by the formulae:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the possible ways of improving the accuracy of a linear regression model?</td>\n",
       "      <td>There could be multiple ways of improving the accuracy of a linear regression, most commonly used ways are as follows:-Regression is sensitive to outliers, hence it becomes very important to treat the outliers with appropriate values. Replacing the values with mean, median, mode or percentile depending on the distribution can prove to be useful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How to interpret a Q-Q plot in a Linear regression model?</td>\n",
       "      <td>A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the significance of an F-test in a linear model?</td>\n",
       "      <td>– The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the disadvantages of the linear model?</td>\n",
       "      <td>– Linear regression is sensitive to outliers which may affect the result.– Over-fitting– Under-fitting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                              Questions  \\\n",
       "0                              What are the important assumptions of Linear regression?   \n",
       "1                                                           What is heteroscedasticity?   \n",
       "2                        What is the difference between R square and adjusted R square?   \n",
       "3                                                             How to find RMSE and MSE?   \n",
       "4    What are the possible ways of improving the accuracy of a linear regression model?   \n",
       "5                             How to interpret a Q-Q plot in a Linear regression model?   \n",
       "6                              What is the significance of an F-test in a linear model?   \n",
       "7                                       What are the disadvantages of the linear model?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Answer  \n",
       "0  A linear relationshipRestricted Multi-collinearity valueHomoscedasticityFirstly, there has to be a linear relationship between the dependent and the independent variables. To check this relationship, a scatter plot proves to be useful.Secondly, there must no or very little multi-collinearity between the independent variables in the dataset. The value needs to be restricted, which depends on the domain requirement.The third is the homoscedasticity. It is one of the most important assumptions which states that the errors are equally distributed.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                             Heteroscedasticity is exactly the opposite of homoscedasticity, which means that the error terms are not equally distributed. To correct this phenomenon, usually, a log function is used.  \n",
       "2                                                                                                                                      R square and adjusted R square values are used for model validation in case of linear regression. R square indicates the variation of all the independent variables on the dependent variable. i.e. it considers all the independent variable to explain the variation. In the case of Adjusted R squared, it considers only significant variables(P values less than 0.05) to indicate the percentage of variation in the model.  \n",
       "3                                                                                                                                                                                                                                                                                                                             RMSE and MSE are the two of the most common measures of accuracy for a linear regression.RMSE indicates the Root mean square error, which indicated by the formulae:Where MSE indicates the Mean square error represented by the formulae:  \n",
       "4                                                                                                                                                                                                            There could be multiple ways of improving the accuracy of a linear regression, most commonly used ways are as follows:-Regression is sensitive to outliers, hence it becomes very important to treat the outliers with appropriate values. Replacing the values with mean, median, mode or percentile depending on the distribution can prove to be useful.  \n",
       "5                                                                                                                                                                                                                                       A Q-Q plot is used to check the normality of errors. In the above chart mentioned, Majority of the data follows a normal distribution with tails curled. This shows that the errors are mostly normally distributed but some observations may be due to significantly higher/lower values are affecting the normality of errors.  \n",
       "6                                                                                                                                                                                                                                                                                                                                         – The use of F-test is to test the goodness of the model. When the model is re-iterated to improve the accuracy with changes, the F-test values prove to be useful in terms of understanding the effect of overall regression.  \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                 – Linear regression is sensitive to outliers which may affect the result.– Over-fitting– Under-fitting  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new2:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df2=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tThulasiram is a veteran with 20 years of experience in production planning, supply chain management, quality assurance, Information Technology, and training. Trained in Data Analysis from IIIT Bangalore and UpGrad,…\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t',\n",
       " 'It is a common practice to test data science aspirants on commonly used machine learning algorithms in interviews. These conventional algorithms being linear regression, logistic regression, clustering, decision trees etc. Data scientists are expected to possess an in-depth knowledge of these algorithms. ',\n",
       " 'We consulted hiring managers and data scientists from various organisations to know about the typical ML questions which they ask in an interview. Based on their extensive feedback a set of question and answers were prepared to help aspiring data scientists in their conversations. Q&As on these algorithms will be provided in a series of four blog posts. ',\n",
       " 'Each blog post will cover the following topic:-',\n",
       " 'Table of Contents',\n",
       " '1. What is linear regression?',\n",
       " 'In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables. \\nIn technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.',\n",
       " '2. State the assumptions in a linear regression model.',\n",
       " 'There are three main assumptions in a linear regression model:',\n",
       " 'Explanation:',\n",
       " '3. What is feature engineering? How do you apply it in the process of modelling?',\n",
       " 'Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\\nIn layman terms, feature engineering means the development of new features that may help you understand and model the problem in a better way. Feature engineering is of two kinds — business driven and data-driven. Business-driven feature engineering revolves around the inclusion of features from a business point of view. The job here is to transform the business variables into features of the problem. In case of data-driven feature engineering, the features you add do not have any significant physical interpretation, but they help the model in the prediction of the target variable. \\nTo apply feature engineering, one must be fully acquainted with the dataset. This involves knowing what the given data is, what it signifies, what the raw features are, etc. You must also have a crystal clear idea of the problem, such as what factors affect the target variable, what the physical interpretation of the variable is, etc.\\n 5 Breakthrough Applications of Machine Learning\\n4. What is the use of regularisation? Explain L1 and L2 regularisations.\\nRegularisation is a technique that is used to tackle the problem of overfitting of the model. When a very complex model is implemented on the training data, it overfits. At times, the simple model might not be able to generalise the data and the complex model overfits. To address this problem, regularisation is used. \\nRegularisation is nothing but adding the coefficient terms (betas) to the cost function so that the terms are penalised and are small in magnitude. This essentially helps in capturing the trends in the data and at the same time prevents overfitting by not letting the model become too complex. \\n\\nL1 or LASSO regularisation: Here, the absolute values of the coefficients are added to the cost function. This can be seen in the following equation; the highlighted part corresponds to the L1 or LASSO regularisation. This regularisation technique gives sparse results, which lead to feature selection as well.\\n\\n\\n\\nL2 or Ridge regularisation: Here, the squares of the coefficients are added to the cost function. This can be seen in the following equation, where the highlighted part corresponds to the L2 or Ridge regularisation.\\n\\n\\n5. How to choose the value of the parameter learning rate (α)?\\nSelecting the value of learning rate is a tricky business. If the value is too small, the gradient descent algorithm takes ages to converge to the optimal solution. On the other hand, if the value of the learning rate is high, the gradient descent will overshoot the optimal solution and most likely never converge to the optimal solution.\\nTo overcome this problem, you can try different values of alpha over a range of values and plot the cost vs the number of iterations. Then, based on the graphs, the value corresponding to the graph showing the rapid decrease can be chosen.\\n\\nThe aforementioned graph is an ideal cost vs the number of iterations curve. Note that the cost initially decreases as the number of iterations increases, but after certain iterations, the gradient descent converges and the cost does not decrease anymore. \\nIf you see that the cost is increasing with the number of iterations, your learning rate parameter is high and it needs to be decreased. \\n6. How to choose the value of the regularisation parameter (λ)?\\nSelecting the regularisation parameter is a tricky business. If the value of\\xa0λ\\xa0is too high, it will lead to extremely small values of the regression coefficient\\xa0β, which will lead to the model underfitting (high bias – low variance). On the other hand, if the value of\\xa0λ\\xa0is 0 (very small), the model will tend to overfit the training data (low bias – high variance).\\nThere is no proper way to select the value of λ. What you can do is have a sub-sample of data and run the algorithm multiple times on different sets. Here, the person has to decide how much variance can be tolerated. Once the user is satisfied with the variance, that value of\\xa0λ\\xa0can be chosen for the full dataset. \\nOne thing to be noted is that the value of λ\\xa0selected here was optimal for that subset, not for the entire training data. \\n7. Can we use linear regression for time series analysis?\\nOne can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so. The reasons behind this are — \\n\\nTime series data is mostly used for the prediction of the future, but linear regression seldom gives good results for future prediction as it is not meant for extrapolation. \\nMostly, time series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis. \\n\\n8. What value is the sum of the residuals of a linear regression close to? Justify.\\nAns The sum of the residuals of a linear regression is 0. Linear regression works on the assumption that the errors (residuals) are normally distributed with a mean of 0, i.e. \\nY = βT\\xa0X + ε\\nHere, Y is the target or dependent variable,\\n β\\xa0is the vector of the regression coefficient,\\nX is the feature matrix containing all the features as the columns, \\nε is the residual term such that\\xa0ε ~ N(0,σ2). \\nSo, the sum of all the residuals is the expected value of the residuals times the total number of data points. Since the expectation of residuals is 0, the sum of all the residual terms is zero. \\nNote: N(μ,σ2) is the standard notation for a normal distribution having mean μ and standard deviation σ2.\\n9. How does multicollinearity affect the linear regression?\\nAns Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, let’s say, some business model, it may cause problems.\\nOne of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business. \\nA highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset. \\n10. What is the normal form (equation) of linear regression? When should it be preferred to the gradient descent method?\\nThe normal equation for linear regression is — \\nβ=(XTX)-1.XTY\\nHere, Y=βTX is the model for the linear regression, \\nY is the target or dependent variable,\\n β is the vector of the regression coefficient, which is arrived at using the normal equation, \\nX is the feature matrix containing all the features as the columns. \\nNote here that the first column in the X matrix consists of all 1s. This is to incorporate the offset value for the regression line.\\nComparison between gradient descent and normal equation:\\n\\n\\n\\nGradient Descent\\nNormal Equation\\n\\n\\nNeeds hyper-parameter tuning for alpha (learning parameter)\\nNo such need\\n\\n\\nIt is an iterative process\\nIt is a non-iterative process\\n\\n\\nO(kn2) time complexity\\nO(n3) time complexity due to evaluation of XTX\\n\\n\\nPrefered when n is extremely large\\nBecomes quite slow for large values of n\\n\\n\\n\\nHere, ‘k’ is the maximum number of iterations for gradient descent, and ‘n’ is the total number of data points in the training set. \\nClearly, if we have large training data, normal equation is not prefered for use. For small values of ‘n’, normal equation is faster than gradient descent.\\n What is Machine Learning and Why it matters\\n11. You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?\\nThis case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.\\n12. Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?\\nThis condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.\\n13. What do you mean by adjusted R2? How is it different from R2?\\nAdjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2\\xa0\\xa0is — \\n\\nHere, n is the number of data points, and k is the number of features.\\nOne drawback of R2\\xa0is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2\\xa0overcomes this drawback. The value of the adjusted R2\\xa0increases only if the newly added feature plays a significant role in the model.\\n14. How do you interpret the residual vs fitted value curve?\\nThe residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model.\\nThe most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.\\n15. What is heteroscedasticity? What are the consequences, and how can you overcome it?\\nA random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation). \\nThe existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values. \\nOne of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the\\xa0least variance than other Linear Unbiased Estimators (LUEs).\\nThere is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — \\n\\nLogarithmising the data: A series that is increasing exponentially often results in increased variability. This can be overcome using the log transformation.\\nUsing weighted linear regression: Here, the OLS method is applied to the weighted values of X and Y. One way is to attach weights directly related to the magnitude of the dependent variable.\\n\\n How does Unsupervised Machine Learning Work?\\n16. What is VIF? How do you calculate it?\\nVariance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—\\xa0\\n\\nHere, VIFj \\xa0is the value of VIF for the jth\\xa0variable,\\nRj2\\xa0is the R2\\xa0value of the model when that variable is regressed against all the other independent variables. \\nIf the value of VIF is high for a variable, it implies that the R2\\xa0\\xa0value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.\\n17. How do you know that linear regression is suitable for any given data?\\nTo see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.\\n18. How is hypothesis testing used in linear regression?\\nHypothesis testing can be carried out in linear regression for the following purposes:\\n\\nTo check whether a predictor is significant for the prediction of the target variable. Two common methods for this are — \\n\\nBy the use of p-values:\\n If the p-value of a variable is greater than a certain limit (usually 0.05), the variable is insignificant in the prediction of the target variable.\\nBy checking the values of the regression coefficient:\\n If the value of regression coefficient corresponding to a predictor is zero, that variable is insignificant in the prediction of the target variable and has no linear relationship with it.\\n\\n\\nTo check whether the calculated regression coefficients are good estimators of the actual coefficients. \\xa0\\n\\n19. Explain gradient descent with respect to linear regression.\\nGradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.\\nGradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).\\n\\nMathematically, the aim of gradient descent for linear regression is to find the solution of\\nArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by — \\xa0\\n\\nHere, h is the linear hypothesis model, h=Θ0\\xa0+ Θ1x,\\xa0y is the true output,\\xa0and m is the number of the data points in the training set.\\nGradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.\\nThe update is:\\nRepeat until convergence\\n\\n20. How do you interpret a linear regression model?\\nA linear regression model is quite easy to interpret. The model is of the following form:\\n\\nThe significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of x0\\xa0increases by 1 unit, keeping other variables constant, the total increase in the value of y\\xa0will be βi. Mathematically, the intercept term (β0) is the response when all the predictor terms are set to zero or not considered.\\n These 6 Machine Learning Techniques are Improving Healthcare\\n21. What is robust regression?\\nA regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. \\n22. Which graphs are suggested to be observed before model fitting?\\nBefore fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.\\n23. What is the generalized linear model?\\nThe generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.\\n24. Explain the bias-variance trade-off.\\nBias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.\\n\\nDecreasing the bias increases the variance.\\nDecreasing the variance increases the bias.\\n\\nSo, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.\\n25. How can learning curves help create a better model?\\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " '4. What is the use of regularisation? Explain L1 and L2 regularisations.',\n",
       " 'Regularisation is a technique that is used to tackle the problem of overfitting of the model. When a very complex model is implemented on the training data, it overfits. At times, the simple model might not be able to generalise the data and the complex model overfits. To address this problem, regularisation is used. \\nRegularisation is nothing but adding the coefficient terms (betas) to the cost function so that the terms are penalised and are small in magnitude. This essentially helps in capturing the trends in the data and at the same time prevents overfitting by not letting the model become too complex. ',\n",
       " '',\n",
       " '',\n",
       " '5. How to choose the value of the parameter learning rate (α)?',\n",
       " 'Selecting the value of learning rate is a tricky business. If the value is too small, the gradient descent algorithm takes ages to converge to the optimal solution. On the other hand, if the value of the learning rate is high, the gradient descent will overshoot the optimal solution and most likely never converge to the optimal solution.\\nTo overcome this problem, you can try different values of alpha over a range of values and plot the cost vs the number of iterations. Then, based on the graphs, the value corresponding to the graph showing the rapid decrease can be chosen.\\n\\nThe aforementioned graph is an ideal cost vs the number of iterations curve. Note that the cost initially decreases as the number of iterations increases, but after certain iterations, the gradient descent converges and the cost does not decrease anymore. \\nIf you see that the cost is increasing with the number of iterations, your learning rate parameter is high and it needs to be decreased. ',\n",
       " '6. How to choose the value of the regularisation parameter (λ)?',\n",
       " 'Selecting the regularisation parameter is a tricky business. If the value of\\xa0λ\\xa0is too high, it will lead to extremely small values of the regression coefficient\\xa0β, which will lead to the model underfitting (high bias – low variance). On the other hand, if the value of\\xa0λ\\xa0is 0 (very small), the model will tend to overfit the training data (low bias – high variance).\\nThere is no proper way to select the value of λ. What you can do is have a sub-sample of data and run the algorithm multiple times on different sets. Here, the person has to decide how much variance can be tolerated. Once the user is satisfied with the variance, that value of\\xa0λ\\xa0can be chosen for the full dataset. \\nOne thing to be noted is that the value of λ\\xa0selected here was optimal for that subset, not for the entire training data. ',\n",
       " '7. Can we use linear regression for time series analysis?',\n",
       " 'One can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so. The reasons behind this are — ',\n",
       " '8. What value is the sum of the residuals of a linear regression close to? Justify.',\n",
       " 'Ans The sum of the residuals of a linear regression is 0. Linear regression works on the assumption that the errors (residuals) are normally distributed with a mean of 0, i.e. ',\n",
       " 'Y = βT\\xa0X + ε',\n",
       " 'Here, Y is the target or dependent variable,\\n β\\xa0is the vector of the regression coefficient,\\nX is the feature matrix containing all the features as the columns, \\nε is the residual term such that\\xa0ε ~ N(0,σ2). \\nSo, the sum of all the residuals is the expected value of the residuals times the total number of data points. Since the expectation of residuals is 0, the sum of all the residual terms is zero. \\nNote: N(μ,σ2) is the standard notation for a normal distribution having mean μ and standard deviation σ2.',\n",
       " '9. How does multicollinearity affect the linear regression?',\n",
       " 'Ans Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, let’s say, some business model, it may cause problems.\\nOne of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business. \\nA highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset. ',\n",
       " '10. What is the normal form (equation) of linear regression? When should it be preferred to the gradient descent method?',\n",
       " 'The normal equation for linear regression is — ',\n",
       " 'β=(XTX)-1.XTY',\n",
       " 'Here, Y=βTX is the model for the linear regression, \\nY is the target or dependent variable,\\n β is the vector of the regression coefficient, which is arrived at using the normal equation, \\nX is the feature matrix containing all the features as the columns. \\nNote here that the first column in the X matrix consists of all 1s. This is to incorporate the offset value for the regression line.\\nComparison between gradient descent and normal equation:',\n",
       " 'Here, ‘k’ is the maximum number of iterations for gradient descent, and ‘n’ is the total number of data points in the training set. \\nClearly, if we have large training data, normal equation is not prefered for use. For small values of ‘n’, normal equation is faster than gradient descent.\\n What is Machine Learning and Why it matters\\n11. You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?\\nThis case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.\\n12. Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?\\nThis condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.\\n13. What do you mean by adjusted R2? How is it different from R2?\\nAdjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2\\xa0\\xa0is — \\n\\nHere, n is the number of data points, and k is the number of features.\\nOne drawback of R2\\xa0is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2\\xa0overcomes this drawback. The value of the adjusted R2\\xa0increases only if the newly added feature plays a significant role in the model.\\n14. How do you interpret the residual vs fitted value curve?\\nThe residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model.\\nThe most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.\\n15. What is heteroscedasticity? What are the consequences, and how can you overcome it?\\nA random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation). \\nThe existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values. \\nOne of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the\\xa0least variance than other Linear Unbiased Estimators (LUEs).\\nThere is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — \\n\\nLogarithmising the data: A series that is increasing exponentially often results in increased variability. This can be overcome using the log transformation.\\nUsing weighted linear regression: Here, the OLS method is applied to the weighted values of X and Y. One way is to attach weights directly related to the magnitude of the dependent variable.\\n\\n How does Unsupervised Machine Learning Work?\\n16. What is VIF? How do you calculate it?\\nVariance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—\\xa0\\n\\nHere, VIFj \\xa0is the value of VIF for the jth\\xa0variable,\\nRj2\\xa0is the R2\\xa0value of the model when that variable is regressed against all the other independent variables. \\nIf the value of VIF is high for a variable, it implies that the R2\\xa0\\xa0value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.\\n17. How do you know that linear regression is suitable for any given data?\\nTo see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.\\n18. How is hypothesis testing used in linear regression?\\nHypothesis testing can be carried out in linear regression for the following purposes:\\n\\nTo check whether a predictor is significant for the prediction of the target variable. Two common methods for this are — \\n\\nBy the use of p-values:\\n If the p-value of a variable is greater than a certain limit (usually 0.05), the variable is insignificant in the prediction of the target variable.\\nBy checking the values of the regression coefficient:\\n If the value of regression coefficient corresponding to a predictor is zero, that variable is insignificant in the prediction of the target variable and has no linear relationship with it.\\n\\n\\nTo check whether the calculated regression coefficients are good estimators of the actual coefficients. \\xa0\\n\\n19. Explain gradient descent with respect to linear regression.\\nGradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.\\nGradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).\\n\\nMathematically, the aim of gradient descent for linear regression is to find the solution of\\nArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by — \\xa0\\n\\nHere, h is the linear hypothesis model, h=Θ0\\xa0+ Θ1x,\\xa0y is the true output,\\xa0and m is the number of the data points in the training set.\\nGradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.\\nThe update is:\\nRepeat until convergence\\n\\n20. How do you interpret a linear regression model?\\nA linear regression model is quite easy to interpret. The model is of the following form:\\n\\nThe significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of x0\\xa0increases by 1 unit, keeping other variables constant, the total increase in the value of y\\xa0will be βi. Mathematically, the intercept term (β0) is the response when all the predictor terms are set to zero or not considered.\\n These 6 Machine Learning Techniques are Improving Healthcare\\n21. What is robust regression?\\nA regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. \\n22. Which graphs are suggested to be observed before model fitting?\\nBefore fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.\\n23. What is the generalized linear model?\\nThe generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.\\n24. Explain the bias-variance trade-off.\\nBias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.\\n\\nDecreasing the bias increases the variance.\\nDecreasing the variance increases the bias.\\n\\nSo, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.\\n25. How can learning curves help create a better model?\\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " '11. You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?',\n",
       " 'This case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.',\n",
       " '12. Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?',\n",
       " 'This condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.',\n",
       " '13. What do you mean by adjusted R2? How is it different from R2?',\n",
       " 'Adjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2\\xa0\\xa0is — \\n\\nHere, n is the number of data points, and k is the number of features.\\nOne drawback of R2\\xa0is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2\\xa0overcomes this drawback. The value of the adjusted R2\\xa0increases only if the newly added feature plays a significant role in the model.',\n",
       " '14. How do you interpret the residual vs fitted value curve?',\n",
       " 'The residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model.\\nThe most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.',\n",
       " '15. What is heteroscedasticity? What are the consequences, and how can you overcome it?',\n",
       " 'A random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation). \\nThe existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values. \\nOne of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the\\xa0least variance than other Linear Unbiased Estimators (LUEs).\\nThere is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — ',\n",
       " '16. What is VIF? How do you calculate it?',\n",
       " 'Variance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—\\xa0\\n\\nHere, VIFj \\xa0is the value of VIF for the jth\\xa0variable,\\nRj2\\xa0is the R2\\xa0value of the model when that variable is regressed against all the other independent variables. \\nIf the value of VIF is high for a variable, it implies that the R2\\xa0\\xa0value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.',\n",
       " '17. How do you know that linear regression is suitable for any given data?',\n",
       " 'To see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.',\n",
       " '18. How is hypothesis testing used in linear regression?',\n",
       " 'Hypothesis testing can be carried out in linear regression for the following purposes:',\n",
       " '19. Explain gradient descent with respect to linear regression.',\n",
       " 'Gradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.\\nGradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).\\n\\nMathematically, the aim of gradient descent for linear regression is to find the solution of\\nArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by — \\xa0\\n\\nHere, h is the linear hypothesis model, h=Θ0\\xa0+ Θ1x,\\xa0y is the true output,\\xa0and m is the number of the data points in the training set.\\nGradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.\\nThe update is:\\nRepeat until convergence\\n',\n",
       " '20. How do you interpret a linear regression model?',\n",
       " 'A linear regression model is quite easy to interpret. The model is of the following form:\\n\\nThe significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of x0\\xa0increases by 1 unit, keeping other variables constant, the total increase in the value of y\\xa0will be βi. Mathematically, the intercept term (β0) is the response when all the predictor terms are set to zero or not considered.\\n These 6 Machine Learning Techniques are Improving Healthcare\\n21. What is robust regression?\\nA regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. \\n22. Which graphs are suggested to be observed before model fitting?\\nBefore fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.\\n23. What is the generalized linear model?\\nThe generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.\\n24. Explain the bias-variance trade-off.\\nBias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.\\n\\nDecreasing the bias increases the variance.\\nDecreasing the variance increases the bias.\\n\\nSo, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.\\n25. How can learning curves help create a better model?\\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " '21. What is robust regression?',\n",
       " 'A regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. ',\n",
       " '22. Which graphs are suggested to be observed before model fitting?',\n",
       " 'Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.',\n",
       " '23. What is the generalized linear model?',\n",
       " 'The generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.',\n",
       " '24. Explain the bias-variance trade-off.',\n",
       " 'Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.',\n",
       " 'So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.',\n",
       " '25. How can learning curves help create a better model?',\n",
       " 'Learning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " 'That’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal',\n",
       " ' Cancel reply',\n",
       " 'Your email address will not be published.',\n",
       " 'Comment ',\n",
       " 'Name ',\n",
       " 'Email ',\n",
       " 'Website ',\n",
       " ' \\n\\n',\n",
       " '',\n",
       " 'Our Trending Machine Learning Courses',\n",
       " 'Accelerate Your Career with upGrad',\n",
       " 'Our Popular Machine Learning Course',\n",
       " 'Building Careers of Tomorrow',\n",
       " 'Register for a Demo Course',\n",
       " 'Register for a Demo Course',\n",
       " 'Register for a Demo Course',\n",
       " 'Talk to our Counselor to find a best course suitable to your Career Growth']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst3 = []\n",
    "url = \"https://www.upgrad.com/blog/machine-learning-interview-questions-answers-ii/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h3','p'])\n",
    "for answer in answers:\n",
    "    lst3.append(answer.text)\n",
    "lst3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is linear regression?',\n",
       " 'In simple terms, linear regression is a method of finding the best straight line fitting to the given data, i.e. finding the best linear relationship between the independent and dependent variables. \\nIn technical terms, linear regression is a machine learning algorithm that finds the best linear-fit relationship on any given data, between independent and dependent variables. It is mostly done by the Sum of Squared Residuals Method.',\n",
       " '2. State the assumptions in a linear regression model.',\n",
       " 'There are three main assumptions in a linear regression model:',\n",
       " 'Explanation:',\n",
       " '3. What is feature engineering? How do you apply it in the process of modelling?',\n",
       " 'Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.\\nIn layman terms, feature engineering means the development of new features that may help you understand and model the problem in a better way. Feature engineering is of two kinds — business driven and data-driven. Business-driven feature engineering revolves around the inclusion of features from a business point of view. The job here is to transform the business variables into features of the problem. In case of data-driven feature engineering, the features you add do not have any significant physical interpretation, but they help the model in the prediction of the target variable. \\nTo apply feature engineering, one must be fully acquainted with the dataset. This involves knowing what the given data is, what it signifies, what the raw features are, etc. You must also have a crystal clear idea of the problem, such as what factors affect the target variable, what the physical interpretation of the variable is, etc.\\n 5 Breakthrough Applications of Machine Learning\\n4. What is the use of regularisation? Explain L1 and L2 regularisations.\\nRegularisation is a technique that is used to tackle the problem of overfitting of the model. When a very complex model is implemented on the training data, it overfits. At times, the simple model might not be able to generalise the data and the complex model overfits. To address this problem, regularisation is used. \\nRegularisation is nothing but adding the coefficient terms (betas) to the cost function so that the terms are penalised and are small in magnitude. This essentially helps in capturing the trends in the data and at the same time prevents overfitting by not letting the model become too complex. \\n\\nL1 or LASSO regularisation: Here, the absolute values of the coefficients are added to the cost function. This can be seen in the following equation; the highlighted part corresponds to the L1 or LASSO regularisation. This regularisation technique gives sparse results, which lead to feature selection as well.\\n\\n\\n\\nL2 or Ridge regularisation: Here, the squares of the coefficients are added to the cost function. This can be seen in the following equation, where the highlighted part corresponds to the L2 or Ridge regularisation.\\n\\n\\n5. How to choose the value of the parameter learning rate (α)?\\nSelecting the value of learning rate is a tricky business. If the value is too small, the gradient descent algorithm takes ages to converge to the optimal solution. On the other hand, if the value of the learning rate is high, the gradient descent will overshoot the optimal solution and most likely never converge to the optimal solution.\\nTo overcome this problem, you can try different values of alpha over a range of values and plot the cost vs the number of iterations. Then, based on the graphs, the value corresponding to the graph showing the rapid decrease can be chosen.\\n\\nThe aforementioned graph is an ideal cost vs the number of iterations curve. Note that the cost initially decreases as the number of iterations increases, but after certain iterations, the gradient descent converges and the cost does not decrease anymore. \\nIf you see that the cost is increasing with the number of iterations, your learning rate parameter is high and it needs to be decreased. \\n6. How to choose the value of the regularisation parameter (λ)?\\nSelecting the regularisation parameter is a tricky business. If the value of\\xa0λ\\xa0is too high, it will lead to extremely small values of the regression coefficient\\xa0β, which will lead to the model underfitting (high bias – low variance). On the other hand, if the value of\\xa0λ\\xa0is 0 (very small), the model will tend to overfit the training data (low bias – high variance).\\nThere is no proper way to select the value of λ. What you can do is have a sub-sample of data and run the algorithm multiple times on different sets. Here, the person has to decide how much variance can be tolerated. Once the user is satisfied with the variance, that value of\\xa0λ\\xa0can be chosen for the full dataset. \\nOne thing to be noted is that the value of λ\\xa0selected here was optimal for that subset, not for the entire training data. \\n7. Can we use linear regression for time series analysis?\\nOne can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so. The reasons behind this are — \\n\\nTime series data is mostly used for the prediction of the future, but linear regression seldom gives good results for future prediction as it is not meant for extrapolation. \\nMostly, time series data have a pattern, such as during peak hours, festive seasons, etc., which would most likely be treated as outliers in the linear regression analysis. \\n\\n8. What value is the sum of the residuals of a linear regression close to? Justify.\\nAns The sum of the residuals of a linear regression is 0. Linear regression works on the assumption that the errors (residuals) are normally distributed with a mean of 0, i.e. \\nY = βT\\xa0X + ε\\nHere, Y is the target or dependent variable,\\n β\\xa0is the vector of the regression coefficient,\\nX is the feature matrix containing all the features as the columns, \\nε is the residual term such that\\xa0ε ~ N(0,σ2). \\nSo, the sum of all the residuals is the expected value of the residuals times the total number of data points. Since the expectation of residuals is 0, the sum of all the residual terms is zero. \\nNote: N(μ,σ2) is the standard notation for a normal distribution having mean μ and standard deviation σ2.\\n9. How does multicollinearity affect the linear regression?\\nAns Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, let’s say, some business model, it may cause problems.\\nOne of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business. \\nA highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset. \\n10. What is the normal form (equation) of linear regression? When should it be preferred to the gradient descent method?\\nThe normal equation for linear regression is — \\nβ=(XTX)-1.XTY\\nHere, Y=βTX is the model for the linear regression, \\nY is the target or dependent variable,\\n β is the vector of the regression coefficient, which is arrived at using the normal equation, \\nX is the feature matrix containing all the features as the columns. \\nNote here that the first column in the X matrix consists of all 1s. This is to incorporate the offset value for the regression line.\\nComparison between gradient descent and normal equation:\\n\\n\\n\\nGradient Descent\\nNormal Equation\\n\\n\\nNeeds hyper-parameter tuning for alpha (learning parameter)\\nNo such need\\n\\n\\nIt is an iterative process\\nIt is a non-iterative process\\n\\n\\nO(kn2) time complexity\\nO(n3) time complexity due to evaluation of XTX\\n\\n\\nPrefered when n is extremely large\\nBecomes quite slow for large values of n\\n\\n\\n\\nHere, ‘k’ is the maximum number of iterations for gradient descent, and ‘n’ is the total number of data points in the training set. \\nClearly, if we have large training data, normal equation is not prefered for use. For small values of ‘n’, normal equation is faster than gradient descent.\\n What is Machine Learning and Why it matters\\n11. You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?\\nThis case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.\\n12. Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?\\nThis condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.\\n13. What do you mean by adjusted R2? How is it different from R2?\\nAdjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2\\xa0\\xa0is — \\n\\nHere, n is the number of data points, and k is the number of features.\\nOne drawback of R2\\xa0is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2\\xa0overcomes this drawback. The value of the adjusted R2\\xa0increases only if the newly added feature plays a significant role in the model.\\n14. How do you interpret the residual vs fitted value curve?\\nThe residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model.\\nThe most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.\\n15. What is heteroscedasticity? What are the consequences, and how can you overcome it?\\nA random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation). \\nThe existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values. \\nOne of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the\\xa0least variance than other Linear Unbiased Estimators (LUEs).\\nThere is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — \\n\\nLogarithmising the data: A series that is increasing exponentially often results in increased variability. This can be overcome using the log transformation.\\nUsing weighted linear regression: Here, the OLS method is applied to the weighted values of X and Y. One way is to attach weights directly related to the magnitude of the dependent variable.\\n\\n How does Unsupervised Machine Learning Work?\\n16. What is VIF? How do you calculate it?\\nVariance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—\\xa0\\n\\nHere, VIFj \\xa0is the value of VIF for the jth\\xa0variable,\\nRj2\\xa0is the R2\\xa0value of the model when that variable is regressed against all the other independent variables. \\nIf the value of VIF is high for a variable, it implies that the R2\\xa0\\xa0value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.\\n17. How do you know that linear regression is suitable for any given data?\\nTo see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.\\n18. How is hypothesis testing used in linear regression?\\nHypothesis testing can be carried out in linear regression for the following purposes:\\n\\nTo check whether a predictor is significant for the prediction of the target variable. Two common methods for this are — \\n\\nBy the use of p-values:\\n If the p-value of a variable is greater than a certain limit (usually 0.05), the variable is insignificant in the prediction of the target variable.\\nBy checking the values of the regression coefficient:\\n If the value of regression coefficient corresponding to a predictor is zero, that variable is insignificant in the prediction of the target variable and has no linear relationship with it.\\n\\n\\nTo check whether the calculated regression coefficients are good estimators of the actual coefficients. \\xa0\\n\\n19. Explain gradient descent with respect to linear regression.\\nGradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.\\nGradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).\\n\\nMathematically, the aim of gradient descent for linear regression is to find the solution of\\nArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by — \\xa0\\n\\nHere, h is the linear hypothesis model, h=Θ0\\xa0+ Θ1x,\\xa0y is the true output,\\xa0and m is the number of the data points in the training set.\\nGradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.\\nThe update is:\\nRepeat until convergence\\n\\n20. How do you interpret a linear regression model?\\nA linear regression model is quite easy to interpret. The model is of the following form:\\n\\nThe significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of x0\\xa0increases by 1 unit, keeping other variables constant, the total increase in the value of y\\xa0will be βi. Mathematically, the intercept term (β0) is the response when all the predictor terms are set to zero or not considered.\\n These 6 Machine Learning Techniques are Improving Healthcare\\n21. What is robust regression?\\nA regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. \\n22. Which graphs are suggested to be observed before model fitting?\\nBefore fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.\\n23. What is the generalized linear model?\\nThe generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.\\n24. Explain the bias-variance trade-off.\\nBias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.\\n\\nDecreasing the bias increases the variance.\\nDecreasing the variance increases the bias.\\n\\nSo, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.\\n25. How can learning curves help create a better model?\\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " '4. What is the use of regularisation? Explain L1 and L2 regularisations.',\n",
       " 'Regularisation is a technique that is used to tackle the problem of overfitting of the model. When a very complex model is implemented on the training data, it overfits. At times, the simple model might not be able to generalise the data and the complex model overfits. To address this problem, regularisation is used. \\nRegularisation is nothing but adding the coefficient terms (betas) to the cost function so that the terms are penalised and are small in magnitude. This essentially helps in capturing the trends in the data and at the same time prevents overfitting by not letting the model become too complex. ',\n",
       " '',\n",
       " '',\n",
       " '5. How to choose the value of the parameter learning rate (α)?',\n",
       " 'Selecting the value of learning rate is a tricky business. If the value is too small, the gradient descent algorithm takes ages to converge to the optimal solution. On the other hand, if the value of the learning rate is high, the gradient descent will overshoot the optimal solution and most likely never converge to the optimal solution.\\nTo overcome this problem, you can try different values of alpha over a range of values and plot the cost vs the number of iterations. Then, based on the graphs, the value corresponding to the graph showing the rapid decrease can be chosen.\\n\\nThe aforementioned graph is an ideal cost vs the number of iterations curve. Note that the cost initially decreases as the number of iterations increases, but after certain iterations, the gradient descent converges and the cost does not decrease anymore. \\nIf you see that the cost is increasing with the number of iterations, your learning rate parameter is high and it needs to be decreased. ',\n",
       " '6. How to choose the value of the regularisation parameter (λ)?',\n",
       " 'Selecting the regularisation parameter is a tricky business. If the value of\\xa0λ\\xa0is too high, it will lead to extremely small values of the regression coefficient\\xa0β, which will lead to the model underfitting (high bias – low variance). On the other hand, if the value of\\xa0λ\\xa0is 0 (very small), the model will tend to overfit the training data (low bias – high variance).\\nThere is no proper way to select the value of λ. What you can do is have a sub-sample of data and run the algorithm multiple times on different sets. Here, the person has to decide how much variance can be tolerated. Once the user is satisfied with the variance, that value of\\xa0λ\\xa0can be chosen for the full dataset. \\nOne thing to be noted is that the value of λ\\xa0selected here was optimal for that subset, not for the entire training data. ',\n",
       " '7. Can we use linear regression for time series analysis?',\n",
       " 'One can use linear regression for time series analysis, but the results are not promising. So, it is generally not advisable to do so. The reasons behind this are — ',\n",
       " '8. What value is the sum of the residuals of a linear regression close to? Justify.',\n",
       " 'Ans The sum of the residuals of a linear regression is 0. Linear regression works on the assumption that the errors (residuals) are normally distributed with a mean of 0, i.e. ',\n",
       " 'Y = βT\\xa0X + ε',\n",
       " 'Here, Y is the target or dependent variable,\\n β\\xa0is the vector of the regression coefficient,\\nX is the feature matrix containing all the features as the columns, \\nε is the residual term such that\\xa0ε ~ N(0,σ2). \\nSo, the sum of all the residuals is the expected value of the residuals times the total number of data points. Since the expectation of residuals is 0, the sum of all the residual terms is zero. \\nNote: N(μ,σ2) is the standard notation for a normal distribution having mean μ and standard deviation σ2.',\n",
       " '9. How does multicollinearity affect the linear regression?',\n",
       " 'Ans Multicollinearity occurs when some of the independent variables are highly correlated (positively or negatively) with each other. This multicollinearity causes a problem as it is against the basic assumption of linear regression. The presence of multicollinearity does not affect the predictive capability of the model. So, if you just want predictions, the presence of multicollinearity does not affect your output. However, if you want to draw some insights from the model and apply them in, let’s say, some business model, it may cause problems.\\nOne of the major problems caused by multicollinearity is that it leads to incorrect interpretations and provides wrong insights. The coefficients of linear regression suggest the mean change in the target value if a feature is changed by one unit. So, if multicollinearity exists, this does not hold true as changing one feature will lead to changes in the correlated variable and consequent changes in the target variable. This leads to wrong insights and can produce hazardous results for a business. \\nA highly effective way of dealing with multicollinearity is the use of VIF (Variance Inflation Factor). Higher the value of VIF for a feature, more linearly correlated is that feature. Simply remove the feature with very high VIF value and re-train the model on the remaining dataset. ',\n",
       " '10. What is the normal form (equation) of linear regression? When should it be preferred to the gradient descent method?',\n",
       " 'The normal equation for linear regression is — ',\n",
       " 'β=(XTX)-1.XTY',\n",
       " 'Here, Y=βTX is the model for the linear regression, \\nY is the target or dependent variable,\\n β is the vector of the regression coefficient, which is arrived at using the normal equation, \\nX is the feature matrix containing all the features as the columns. \\nNote here that the first column in the X matrix consists of all 1s. This is to incorporate the offset value for the regression line.\\nComparison between gradient descent and normal equation:',\n",
       " 'Here, ‘k’ is the maximum number of iterations for gradient descent, and ‘n’ is the total number of data points in the training set. \\nClearly, if we have large training data, normal equation is not prefered for use. For small values of ‘n’, normal equation is faster than gradient descent.\\n What is Machine Learning and Why it matters\\n11. You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?\\nThis case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.\\n12. Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?\\nThis condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.\\n13. What do you mean by adjusted R2? How is it different from R2?\\nAdjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2\\xa0\\xa0is — \\n\\nHere, n is the number of data points, and k is the number of features.\\nOne drawback of R2\\xa0is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2\\xa0overcomes this drawback. The value of the adjusted R2\\xa0increases only if the newly added feature plays a significant role in the model.\\n14. How do you interpret the residual vs fitted value curve?\\nThe residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model.\\nThe most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.\\n15. What is heteroscedasticity? What are the consequences, and how can you overcome it?\\nA random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation). \\nThe existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values. \\nOne of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the\\xa0least variance than other Linear Unbiased Estimators (LUEs).\\nThere is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — \\n\\nLogarithmising the data: A series that is increasing exponentially often results in increased variability. This can be overcome using the log transformation.\\nUsing weighted linear regression: Here, the OLS method is applied to the weighted values of X and Y. One way is to attach weights directly related to the magnitude of the dependent variable.\\n\\n How does Unsupervised Machine Learning Work?\\n16. What is VIF? How do you calculate it?\\nVariance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—\\xa0\\n\\nHere, VIFj \\xa0is the value of VIF for the jth\\xa0variable,\\nRj2\\xa0is the R2\\xa0value of the model when that variable is regressed against all the other independent variables. \\nIf the value of VIF is high for a variable, it implies that the R2\\xa0\\xa0value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.\\n17. How do you know that linear regression is suitable for any given data?\\nTo see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.\\n18. How is hypothesis testing used in linear regression?\\nHypothesis testing can be carried out in linear regression for the following purposes:\\n\\nTo check whether a predictor is significant for the prediction of the target variable. Two common methods for this are — \\n\\nBy the use of p-values:\\n If the p-value of a variable is greater than a certain limit (usually 0.05), the variable is insignificant in the prediction of the target variable.\\nBy checking the values of the regression coefficient:\\n If the value of regression coefficient corresponding to a predictor is zero, that variable is insignificant in the prediction of the target variable and has no linear relationship with it.\\n\\n\\nTo check whether the calculated regression coefficients are good estimators of the actual coefficients. \\xa0\\n\\n19. Explain gradient descent with respect to linear regression.\\nGradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.\\nGradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).\\n\\nMathematically, the aim of gradient descent for linear regression is to find the solution of\\nArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by — \\xa0\\n\\nHere, h is the linear hypothesis model, h=Θ0\\xa0+ Θ1x,\\xa0y is the true output,\\xa0and m is the number of the data points in the training set.\\nGradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.\\nThe update is:\\nRepeat until convergence\\n\\n20. How do you interpret a linear regression model?\\nA linear regression model is quite easy to interpret. The model is of the following form:\\n\\nThe significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of x0\\xa0increases by 1 unit, keeping other variables constant, the total increase in the value of y\\xa0will be βi. Mathematically, the intercept term (β0) is the response when all the predictor terms are set to zero or not considered.\\n These 6 Machine Learning Techniques are Improving Healthcare\\n21. What is robust regression?\\nA regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. \\n22. Which graphs are suggested to be observed before model fitting?\\nBefore fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.\\n23. What is the generalized linear model?\\nThe generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.\\n24. Explain the bias-variance trade-off.\\nBias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.\\n\\nDecreasing the bias increases the variance.\\nDecreasing the variance increases the bias.\\n\\nSo, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.\\n25. How can learning curves help create a better model?\\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " '11. You run your regression on different subsets of your data, and in each subset, the beta value for a certain variable varies wildly. What could be the issue here?',\n",
       " 'This case implies that the dataset is heterogeneous. So, to overcome this problem, the dataset should be clustered into different subsets, and then separate models should be built for each cluster. Another way to deal with this problem is to use non-parametric models, such as decision trees, which can deal with heterogeneous data quite efficiently.',\n",
       " '12. Your linear regression doesn’t run and communicates that there is an infinite number of best estimates for the regression coefficients. What could be wrong?',\n",
       " 'This condition arises when there is a perfect correlation (positive or negative) between some variables. In this case, there is no unique value for the coefficients, and hence, the given condition arises.',\n",
       " '13. What do you mean by adjusted R2? How is it different from R2?',\n",
       " 'Adjusted R2, just like R2, is a representative of the number of points lying around the regression line. That is, it shows how well the model is fitting the training data. The formula for adjusted R2\\xa0\\xa0is — \\n\\nHere, n is the number of data points, and k is the number of features.\\nOne drawback of R2\\xa0is that it will always increase with the addition of a new feature, whether the new feature is useful or not. The adjusted R2\\xa0overcomes this drawback. The value of the adjusted R2\\xa0increases only if the newly added feature plays a significant role in the model.',\n",
       " '14. How do you interpret the residual vs fitted value curve?',\n",
       " 'The residual vs fitted value plot is used to see whether the predicted values and residuals have a correlation or not. If the residuals are distributed normally, with a mean around the fitted value and a constant variance, our model is working fine; otherwise, there is some issue with the model.\\nThe most common problem that can be found when training the model over a large range of a dataset is heteroscedasticity(this is explained in the answer below). The presence of heteroscedasticity can be easily seen by plotting the residual vs fitted value curve.',\n",
       " '15. What is heteroscedasticity? What are the consequences, and how can you overcome it?',\n",
       " 'A random variable is said to be heteroscedastic when different subpopulations have different variabilities (standard deviation). \\nThe existence of heteroscedasticity gives rise to certain problems in the regression analysis as the assumption says that error terms are uncorrelated and, hence, the variance is constant. The presence of heteroscedasticity can often be seen in the form of a cone-like scatter plot for residual vs fitted values. \\nOne of the basic assumptions of linear regression is that heteroscedasticity is not present in the data. Due to the violation of assumptions, the Ordinary Least Squares (OLS) estimators are not the Best Linear Unbiased Estimators (BLUE). Hence, they do not give the\\xa0least variance than other Linear Unbiased Estimators (LUEs).\\nThere is no fixed procedure to overcome heteroscedasticity. However, there are some ways that may lead to a reduction of heteroscedasticity. They are — ',\n",
       " '16. What is VIF? How do you calculate it?',\n",
       " 'Variance Inflation Factor (VIF) is used to check the presence of multicollinearity in a dataset. It is calculated as—\\xa0\\n\\nHere, VIFj \\xa0is the value of VIF for the jth\\xa0variable,\\nRj2\\xa0is the R2\\xa0value of the model when that variable is regressed against all the other independent variables. \\nIf the value of VIF is high for a variable, it implies that the R2\\xa0\\xa0value of the corresponding model is high, i.e. other independent variables are able to explain that variable. In simple terms, the variable is linearly dependent on some other variables.',\n",
       " '17. How do you know that linear regression is suitable for any given data?',\n",
       " 'To see if linear regression is suitable for any given data, a scatter plot can be used. If the relationship looks linear, we can go for a linear model. But if it is not the case, we have to apply some transformations to make the relationship linear. Plotting the scatter plots is easy in case of simple or univariate linear regression. But in case of multivariate linear regression, two-dimensional pairwise scatter plots, rotating plots, and dynamic graphs can be plotted.',\n",
       " '18. How is hypothesis testing used in linear regression?',\n",
       " 'Hypothesis testing can be carried out in linear regression for the following purposes:',\n",
       " '19. Explain gradient descent with respect to linear regression.',\n",
       " 'Gradient descent is an optimisation algorithm. In linear regression, it is used to optimise the cost function and find the values of the βs (estimators) corresponding to the optimised value of the cost function.\\nGradient descent works like a ball rolling down a graph (ignoring the inertia). The ball moves along the direction of the greatest gradient and comes to rest at the flat surface (minima).\\n\\nMathematically, the aim of gradient descent for linear regression is to find the solution of\\nArgMin J(Θ0,Θ1), where J(Θ0,Θ1) is the cost function of the linear regression. It is given by — \\xa0\\n\\nHere, h is the linear hypothesis model, h=Θ0\\xa0+ Θ1x,\\xa0y is the true output,\\xa0and m is the number of the data points in the training set.\\nGradient Descent starts with a random solution, and then based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.\\nThe update is:\\nRepeat until convergence\\n',\n",
       " '20. How do you interpret a linear regression model?',\n",
       " 'A linear regression model is quite easy to interpret. The model is of the following form:\\n\\nThe significance of this model lies in the fact that one can easily interpret and understand the marginal changes and their consequences. For example, if the value of x0\\xa0increases by 1 unit, keeping other variables constant, the total increase in the value of y\\xa0will be βi. Mathematically, the intercept term (β0) is the response when all the predictor terms are set to zero or not considered.\\n These 6 Machine Learning Techniques are Improving Healthcare\\n21. What is robust regression?\\nA regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. \\n22. Which graphs are suggested to be observed before model fitting?\\nBefore fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.\\n23. What is the generalized linear model?\\nThe generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.\\n24. Explain the bias-variance trade-off.\\nBias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.\\n\\nDecreasing the bias increases the variance.\\nDecreasing the variance increases the bias.\\n\\nSo, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.\\n25. How can learning curves help create a better model?\\nLearning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n',\n",
       " '21. What is robust regression?',\n",
       " 'A regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers. \\nA regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful. ',\n",
       " '22. Which graphs are suggested to be observed before model fitting?',\n",
       " 'Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.',\n",
       " '23. What is the generalized linear model?',\n",
       " 'The generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.',\n",
       " '24. Explain the bias-variance trade-off.',\n",
       " 'Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a\\xa0low bias.\\nVariance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance.\\nFor a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance.\\nThere is no escaping\\xa0the relationship between bias and variance in machine learning.',\n",
       " 'So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.',\n",
       " '25. How can learning curves help create a better model?',\n",
       " 'Learning curves give the indication of the presence of overfitting or underfitting. \\nIn a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:\\n\\nIf the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias. \\nIf there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a\\xa0high variance.\\n Machine Learning Engineers: Myths vs. Realities\\nThat’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments.\\nCo-authored by – Ojas Agarwal\\n\\n\\nLead the AI Driven Technological Revolution\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t\\nLearn More\\n\\n\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new3 = lst3[5:64]\n",
    "lst_new3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>What is robust regression?</td>\n",
       "      <td>A regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers.  A regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Which graphs are suggested to be observed before model fitting?</td>\n",
       "      <td>Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What is the generalized linear model?</td>\n",
       "      <td>The generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Explain the bias-variance trade-off.</td>\n",
       "      <td>Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a low bias. Variance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance. For a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance. There is no escaping the relationship between bias and variance in machine learning.So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>How can learning curves help create a better model?</td>\n",
       "      <td>Learning curves give the indication of the presence of overfitting or underfitting.  In a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:  If the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias.  If there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a high variance.  Machine Learning Engineers: Myths vs. Realities That’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments. Co-authored by – Ojas Agarwal   Lead the AI Driven Technological Revolution   \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t Learn More</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Questions  \\\n",
       "20                                         What is robust regression?   \n",
       "21    Which graphs are suggested to be observed before model fitting?   \n",
       "22                              What is the generalized linear model?   \n",
       "23                               Explain the bias-variance trade-off.   \n",
       "24                How can learning curves help create a better model?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Answer  \n",
       "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             A regression model should be robust in nature. This means that with changes in a few observations, the model should not change drastically. Also, it should not be much affected by the outliers.  A regression model with OLS (Ordinary Least Squares) is quite sensitive to the outliers. To overcome this problem, we can use the WLS (Weighted Least Squares) method to determine the estimators of the regression coefficients. Here, less weights are given to the outliers or high leverage points in the fitting, making these points less impactful.   \n",
       "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Before fitting the model, one must be well aware of the data, such as what the trends, distribution, skewness, etc. in the variables are. Graphs such as histograms, box plots, and dot plots can be used to observe the distribution of the variables. Apart from this, one must also analyse what the relationship between dependent and independent variables is. This can be done by scatter plots (in case of univariate problems), rotating plots, dynamic plots, etc.  \n",
       "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The generalized linear model is the derivative of the ordinary linear regression model. GLM is more flexible in terms of residuals and can be used where linear regression does not seem appropriate. GLM allows the distribution of residuals to be other than a normal distribution. It generalizes the linear regression by allowing the linear model to link to the target variable using the linking function. Model estimation is done using the method of maximum likelihood estimation.  \n",
       "23                                                                                                                                                                                                                                                                                                                                                   Bias refers to the difference between the values predicted by the model and the real values. It is an error. One of the goals of an ML algorithm is to have a low bias. Variance refers to the sensitivity of the model to small fluctuations in the training dataset. Another goal of an ML algorithm is to have low variance. For a dataset that is not exactly linear, it is not possible to have both bias and variance low at the same time. A straight line model will have low variance but high bias, whereas a high-degree polynomial will have low bias but high variance. There is no escaping the relationship between bias and variance in machine learning.So, there is a trade-off between the two; the ML specialist has to decide, based on the assigned problem, how much bias and variance can be tolerated. Based on this, the final model is built.  \n",
       "24  Learning curves give the indication of the presence of overfitting or underfitting.  In a learning curve, the training error and cross-validating error are plotted against the number of training data points. A typical learning curve looks like this:  If the training error and true error (cross-validating error) converge to the same value and the corresponding value of the error is high, it indicates that the model is underfitting and is suffering from high bias.  If there is a significant gap between the converging values of the training and cross-validating errors, i.e. the cross-validating error is significantly higher than the training error, it suggests that the model is overfitting the training data and is suffering from a high variance.  Machine Learning Engineers: Myths vs. Realities That’s the end of the first section of this series. Stick around for the next part of the series which consist of questions based on Logistic Regression. Feel free to post your comments. Co-authored by – Ojas Agarwal   Lead the AI Driven Technological Revolution   \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tPG Diploma in Machine Learning and Artificial Intelligence\\t\\t\\t\\t\\t\\t\\t\\t\\t Learn More     "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new3:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df3=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df3[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df3.drop(df3.index[[1,2,9,17,19]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df3[:]\n",
    "# df3.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '\\xa0',\n",
       " 'Are you planning to sit for deep learning interviews? Have you perhaps already taken the first step, applied, and sat through the ordeal of several rounds of interviews for a deep learning role and not made the cut?',\n",
       " 'Cracking an interview, especially for a complex role like a deep learning specialist, is a daunting task for most people. Deep learning is a vast field with an ever-changing nature as new developments are rolled out on a regular basis. How can you keep up with the pace? What should you focus on?',\n",
       " 'These are questions every deep learning enthusiast, fresher, and even expert with the best deep learning course learning has asked themselves at some point.',\n",
       " 'That was a key reason behind penning down this article, a comprehensive list of the popular deep learning interview questions and answers. But let me expand on that a bit more.',\n",
       " 'Note: Make sure you check out the popular Fundamentals of Deep Learning course if you harbor any deep learning career ambitions!',\n",
       " '\\xa0',\n",
       " 'There are plenty of resources on the internet about Machine Learning and Data Science Interviews. The rapid rise of Machine Learning and the solutions it provides to complex tasks has been remarkable and the industry has responded to this rise really well. Almost every big company has a Data Science Team, and almost every other startup leverages Machine Learning for its product.',\n",
       " 'However, we are now seeing a tectonic shift in the industry. As the need for more complex solutions grows around the world, organizations are turning to deep learning frameworks. Advances in computer vision and natural language processing (NLP) have created a need to adopt deep learning or stay behind the curve.',\n",
       " 'The demand for deep learning folks is growing every month! This is a great time to polish your skills and start climbing the deep learning hill.',\n",
       " 'My aim here is to make this article simple and to-the-point while explaining the core ideas behind the questions. I have also listed resources where you can learn more about the deep learning topics in the questions.',\n",
       " 'Also, this does not mean that your interview will not contain a single question on Machine Learning. There are some concepts that are common in both fields and are extremely crucial for you to know. These include topics like:',\n",
       " 'So, here is a definitive interview guide that covers all the topics in details, in the form of MCQs and long-form resources: The Most Comprehensive Data Science & Machine Learning Interview Guide You’ll Ever Need. If you’re looking for a structured and granular guide including tips, tricks and case studies on how to crack interviews, I highly recommend taking the Ace Data Science Interviews course.',\n",
       " 'In this comprehensive guide, I have organized the deep learning questions into three levels:',\n",
       " 'There’s something here for everyone! So get your pen and paper ready, strap in, and prepare to learn.',\n",
       " '\\xa0',\n",
       " 'These questions are typically asked to make the candidate familiar with the interviewer and the interview setting. While the questions themselves may not be very difficult to answer, this level is your best chance to convince the interviewer that your fundamental concepts around deep learning are clear.',\n",
       " 'Your answers to these questions need not be too detailed but do keep in mind that the interviewer might recall your answer while asking more advanced questions later.',\n",
       " '\\xa0',\n",
       " '1. What is the difference between a Perceptron and Logistic Regression?',\n",
       " 'A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.',\n",
       " 'So, what’s the difference between the two?',\n",
       " 'Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:',\n",
       " '',\n",
       " '\\xa0',\n",
       " '2. Can we have the same bias for all neurons of a hidden layer?',\n",
       " 'Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.',\n",
       " 'A point to note is that both these strategies would give you very different results.',\n",
       " '\\xa0',\n",
       " '3. What if we do not use any activation function(s) in a neural network?\\n',\n",
       " 'The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:',\n",
       " 'Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:',\n",
       " 'Z = (weights * X) + bias',\n",
       " 'Step 2: Apply an activation function to calculate the expected output:',\n",
       " 'Y = Activation(Z)',\n",
       " 'Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?',\n",
       " 'Our equation for Y essentially becomes:',\n",
       " 'Y = Z = (weights * X) + bias',\n",
       " 'Wait – isn’t this just a simple linear equation? Yes – and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.',\n",
       " 'In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model.',\n",
       " '\\xa0',\n",
       " '4. In a neural network, what if all the weights are initialized with the same value?',\n",
       " 'In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.',\n",
       " 'In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.',\n",
       " 'Therefore, if all weights have the same initial value, this would lead to underfitting.',\n",
       " 'Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below.',\n",
       " '\\xa0',\n",
       " '5. List the supervised and unsupervised tasks in Deep Learning.',\n",
       " 'Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:',\n",
       " 'On the other hand, there are some unsupervised deep learning techniques as well:',\n",
       " 'Here is a great article on applications of Deep Learning for unsupervised tasks:',\n",
       " '\\xa0',\n",
       " '6. What is the role of weights and bias in a neural network?',\n",
       " 'This is a question best explained with a real-life example. Consider that you want to go out today to play a cricket match with your friends. Now, a number of factors can affect your decision-making, like:',\n",
       " 'And so on. These factors can change your decision greatly or not too much. For example, if it is raining outside, then you cannot go out to play at all. Or if you have only one bat, you can share it while playing as well. The magnitude by which these factors can affect the game is called the weight of that factor.',\n",
       " 'Factors like the weather or temperature might have a higher weight, and other factors like equipment would have a lower weight.',\n",
       " 'However, does this mean that we can play a cricket match with only one bat? No – we would need 1 ball and 6 wickets as well. This is where bias comes into the picture. Bias lets you assign some threshold which helps you activate a decision-point (or a neuron) only when that threshold is crossed.',\n",
       " '\\xa0',\n",
       " '7. How does forward propagation and backpropagation work in deep learning?',\n",
       " 'Now, this can be answered in two ways. If you are on a phone interview, you cannot perform all the calculus in writing and show the interviewer. In such cases, it best to explain it as such:',\n",
       " 'For an in-person interview, it is best to take up the marker, create a simple neural network with 2 inputs, a hidden layer, and an output layer, and explain it.',\n",
       " '',\n",
       " 'Forward propagation:',\n",
       " '',\n",
       " 'Backpropagation:',\n",
       " 'At layer L2, for all weights:',\n",
       " '',\n",
       " 'At layer L1, for all weights:',\n",
       " '',\n",
       " 'You need not explain with respect to the bias term as well, though you might need to expand the above equations substituting the actual derivatives.',\n",
       " '\\xa0',\n",
       " '8. What are the common data structures used in Deep Learning?',\n",
       " 'Deep Learning goes right from the simplest data structures like lists to complicated ones like computation graphs.',\n",
       " 'Here are the most common ones:',\n",
       " '\\xa0',\n",
       " 'Once the basics are out of the way, the interview would lead to slightly advanced deep learning concepts. These questions are much easier to answer if you have considerable practice with not only the mathematical concepts but also with coding them.',\n",
       " 'Additionally, these questions can also become more project-specific. As a general rule of thumb, it is best to include examples of how you have used the concept asked in the question in your own projects. This has two advantages:',\n",
       " 'Here, I have given an overview of the key concepts in the questions – you can always customize your answers to add more about your experiences with some of these deep learning algorithms and techniques.',\n",
       " '\\xa0',\n",
       " '9. Why should we use Batch Normalization?',\n",
       " 'Once the interviewer has asked you about the fundamentals of deep learning architectures, they would move on to the key topic of improving your deep learning model’s performance.',\n",
       " 'Batch Normalization is one of the techniques used for reducing the training time of our deep learning algorithm. Just like normalizing our input helps improve our logistic regression model, we can normalize the activations of the hidden layers in our deep learning model as well:',\n",
       " '',\n",
       " 'We basically normalize a[1] and a[2] here. This means we normalize the inputs to the layer, and then apply the activation functions to the normalized inputs.',\n",
       " 'Here is an article that explains Batch Normalization and other techniques for improving Neural Networks: Neural Networks – Hyperparameter Tuning, Regularization & Optimization.',\n",
       " '\\xa0',\n",
       " '10. List the activation functions you have used so far in your projects and how you would choose one.',\n",
       " 'The most common activation functions are:',\n",
       " 'While it is not important to know all the activation functions, you can always score points by knowing the range of these functions and how they are used. Here is a handy table for you to follow:',\n",
       " '',\n",
       " 'Here is a great guide on how to use these and other activations functions: Fundamentals of Deep Learning – Activation Functions and When to Use Them?.',\n",
       " '',\n",
       " '11. Why does a Convolutional Neural Network (CNN) work better with image data?',\n",
       " 'The key to this question lies in the Convolution operation. Unlike humans, the machine sees the image as a matrix of pixel values. Instead of interpreting a shape like a petal or an ear, it just identifies curves and edges.',\n",
       " 'Thus, instead of looking at the entire image, it helps to just read the image in parts. Doing this for a 300 x 300 pixel image would mean dividing the matrix into smaller 3 x 3 matrices and dealing with them one by one. This is convolution.',\n",
       " 'Mathematically, we just perform a small operation on the matrix to help us detect features in the image – like boundaries, colors, etc.',\n",
       " 'Z = X * f',\n",
       " 'Here, we are convolving (* operation – not multiplication) the input matrix X with another small matrix f, called the kernel/filter to create a new matrix Z. This matrix is then passed on to the other layers.',\n",
       " 'If you have a board/screen in front of you, you can always illustrate this with a simple example:',\n",
       " '',\n",
       " 'Learning more about how CNNs work here.',\n",
       " '\\xa0',\n",
       " '12. Why do RNNs work better with text data?',\n",
       " 'The main component that differentiates Recurrent Neural Networks (RNN) from the other models is the addition of a loop at each node. This loop brings the recurrence mechanism in RNNs. In a basic Artificial Neural Network (ANN), each input is given the same weight and fed to the network at the same time. So, for a sentence like “I saw the movie and hated it”, it would be difficult to capture the information which associates “it” with the “movie”.',\n",
       " '',\n",
       " 'The addition of a loop is to denote preserving the previous node’s information for the next node, and so on. This is why RNNs are much better for sequential data, and since text data also is sequential in nature, they are an improvement over ANNs.',\n",
       " '',\n",
       " '13. In a CNN, if the input size 5 X 5 and the filter size is 7 X 7, then what would be the size of the output?',\n",
       " 'This is a pretty intuitive answer. As we saw above, we perform the convolution on ‘x’ one step at a time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with dimensions 3 X 3.',\n",
       " 'Thus, to make the input size similar to the filter size, we make use of padding – adding 0s to the input matrix such that its new size becomes at least 7 X 7. Thus, the output size would be using the formula:',\n",
       " 'Dimension of image = (n, n) = 5 X 5',\n",
       " 'Dimension of filter = (f,f)\\xa0 = 7 X 7',\n",
       " 'Padding = 1 (adding 1 pixel with value 0 all around the edges)',\n",
       " 'Dimension of output will be (n+2p-f+1) X (n+2p-f+1) = 1 X 1',\n",
       " '\\xa0',\n",
       " '14. What’s the difference between valid and same padding in a CNN?',\n",
       " 'This question has more chances of being a follow-up question to the previous one. Or if you have explained how you used CNNs in a computer vision task, the interviewer might ask this question along with the details of the padding parameters.',\n",
       " '\\xa0',\n",
       " '15. What do you mean by exploding and vanishing gradients?',\n",
       " 'The key here is to make the explanation as simple as possible. As we know, the gradient descent algorithm tries to minimize the error by taking small steps towards the minimum value. These steps are used to update the weights and biases in a neural network.',\n",
       " 'However, at times, the steps become too large and this results in larger updates to weights and bias terms – so much so as to cause an overflow (or a NaN) value in the weights. This leads to an unstable algorithm and is called an exploding gradient.',\n",
       " 'On the other hand, the steps are too small and this leads to minimal changes in the weights and bias terms – even negligible changes at times. We thus might end up training a deep learning model with almost the same weights and biases each time and never reach the minimum error function. This is called the vanishing gradient.',\n",
       " 'A point to note is that both these issues are specifically evident in Recurrent Neural Networks – so be prepared for follow-up questions on RNN!',\n",
       " '\\xa0',\n",
       " '16. What are the applications of transfer learning in Deep Learning?',\n",
       " 'I am sure you would have a doubt as to why a relatively simple question was included in the Intermediate Level. The reason is the sheer volume of subsequent questions it can generate!',\n",
       " 'The use of transfer learning has been one of the key milestones in deep learning. Training a large model on a huge dataset, and then using the final parameters on smaller simpler datasets has led to defining breakthroughs in the form of Pretrained Models. Be it Computer Vision or NLP, pretrained models have become the norm in research and in the industry.',\n",
       " 'Some popular examples include BERT, ResNet, GPT-2, VGG-16, etc and many more.',\n",
       " 'It is here that you can earn brownie points by pointing out specific examples/projects where you used these models and how you used them as well.',\n",
       " 'It is not possible to discuss all of them, so here are a few resources to get started:',\n",
       " '\\xa0',\n",
       " 'It is here that questions become really specific to your projects or to what you have discussed in the interview before.',\n",
       " 'Also, depending on the domain – with Computer Vision or Natural Language Processing, these questions can change. While it is not important to know the architecture of each model in detail, you would need to know the intuition behind them and why these models were needed in the first place.',\n",
       " 'Again, just like the intermediate level, it is important to always bring in examples that you have studied or implemented yourself into the discussion.',\n",
       " '\\xa0',\n",
       " '17. How backpropagation is different in RNN compared to ANN?',\n",
       " 'In Recurrent Neural Networks, we have an additional loop at each node:',\n",
       " '',\n",
       " 'This loop essentially includes a time component into the network as well. This helps in capturing sequential information from the data, which could not be possible in a generic artificial neural network.',\n",
       " 'This is why the backpropagation in RNN is called Backpropagation through Time, as in backpropagation at each time step.',\n",
       " 'You can find a detailed explanation of RNNs here: Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks.',\n",
       " '\\xa0',\n",
       " '18. How does LSTM solve the vanishing gradient challenge?',\n",
       " 'The LSTM model is considered a special case of RNNs. The problems of vanishing gradients and exploding gradients we saw earlier are a disadvantage while using the plain RNN model.',\n",
       " 'In LSTMs, we add a forget gate, which is basically a memory unit that retains information that is retained across timesteps and discards the other information that is not needed. This also necessitates the need for input and output gates to include the results of the forget gate as well.',\n",
       " '',\n",
       " '\\xa0',\n",
       " '19. Why is GRU faster as compared to LSTM?',\n",
       " 'As you can see, the LSTM model can become quite complex. In order to still retain the functionality of retaining information across time and yet not make a too complex model, we need GRUs.',\n",
       " 'Basically, in GRUs, instead of having an additional Forget gate, we combine the input and Forget gates into a single Update Gate:',\n",
       " '',\n",
       " 'It is this reduction in the number of gates that makes GRU less complex and faster than LSTM. You can learn about GRUs, LSTMs and other sequence models in detail here: Must-Read Tutorial to Learn Sequence Modeling & Attention Models.',\n",
       " '\\xa0',\n",
       " '20. How is the transformer architecture better than RNN?',\n",
       " 'Advancements in deep learning have made it possible to solve many tasks in Natural Language Processing. Networks/Sequence models like RNNs, LSTMs, etc. are specifically used for this purpose – so as to capture all possible information from a given sentence, or a paragraph. However, sequential processing comes with its caveats:',\n",
       " 'This gave rise to the Transformer architecture. Transformers use what is called the attention mechanism. This basically means mapping dependencies between all the parts of a sentence.',\n",
       " 'Here is an excellent article explaining transformers: How do Transformers Work in NLP? A Guide to the Latest State-of-the-Art Models.',\n",
       " '\\xa0',\n",
       " '21. Describe a project you worked on and the tools/frameworks you used?',\n",
       " 'Now, this is one question that is sure to be asked even if none of the above ones is asked in your deep learning interview. I have included it in the advanced section since you might be grilled on each and every part of the code you have written.',\n",
       " 'Before the interview, make sure to:',\n",
       " 'When you are asked such a question, it is best to give a small 30-second pitch on what was the:',\n",
       " 'After this, you can start going into detail about the model architecture, what preprocessing steps you had to take, and how that changed the data.',\n",
       " 'An important point to be noted is that the project need not be a very complicated or sophisticated one. A well-explained object detection project would earn you more points than a poorly-explained video classification project. Towards this end, I recommend having a README file in the above format for every project that you have implemented.',\n",
       " '\\xa0',\n",
       " 'These are a list of a few key questions that you would come across in a deep learning interview. I have tried to cover more generic topics rather than go into the details of how deep learning is used in fields like NLP or computer vision.',\n",
       " 'I would always recommend to have a clear knowledge of the job description and prepare accordingly. If the role is more geared towards Computer Vision, I would recommend studying more on Convolutional Neural Networks and OpenCV, while Natural Language Processing roles would veer more towards RNNs, Transformers, etc.',\n",
       " 'I would love to hear your own interview experiences. Share them with me and the community in the comments section below.',\n",
       " 'About the Author',\n",
       " 'Trainee Data Scientist at Analytics Vidhya. Pursuing Masters in Data Science from the University of Mumbai, Dept. of Computer Science. ML and NLP enthusiast.',\n",
       " 'Our Top Authors',\n",
       " '\\nDownload\\nAnalytics Vidhya App for the Latest blog/Article\\n',\n",
       " 'Leave a Reply Your email address will not be published. Required fields are marked *',\n",
       " ' Notify me of follow-up comments by email.',\n",
       " ' Notify me of new posts by email.',\n",
       " '',\n",
       " '',\n",
       " 'Basic Concepts of Object-Oriented Programming in Python',\n",
       " 'Python Tutorial: Working with CSV file for Data Science',\n",
       " 'Commonly used Machine Learning Algorithms (with Python and R Codes)',\n",
       " '3 Interesting Python Projects With Code for Beginners!',\n",
       " '© Copyright 2013-2021 Analytics Vidhya.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Networks nd deep learning\n",
    "lst4 = []\n",
    "url = \"https://www.analyticsvidhya.com/blog/2020/04/comprehensive-popular-deep-learning-interview-questions-answers/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h3','p'])\n",
    "for answer in answers:\n",
    "    lst4.append(answer.text)\n",
    "lst4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the difference between a Perceptron and Logistic Regression?',\n",
       " 'A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.',\n",
       " 'So, what’s the difference between the two?',\n",
       " 'Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:',\n",
       " '',\n",
       " '\\xa0',\n",
       " '2. Can we have the same bias for all neurons of a hidden layer?',\n",
       " 'Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.',\n",
       " 'A point to note is that both these strategies would give you very different results.',\n",
       " '\\xa0',\n",
       " '3. What if we do not use any activation function(s) in a neural network?\\n',\n",
       " 'The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:',\n",
       " 'Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:',\n",
       " 'Z = (weights * X) + bias',\n",
       " 'Step 2: Apply an activation function to calculate the expected output:',\n",
       " 'Y = Activation(Z)',\n",
       " 'Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?',\n",
       " 'Our equation for Y essentially becomes:',\n",
       " 'Y = Z = (weights * X) + bias',\n",
       " 'Wait – isn’t this just a simple linear equation? Yes – and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.',\n",
       " 'In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model.',\n",
       " '\\xa0',\n",
       " '4. In a neural network, what if all the weights are initialized with the same value?',\n",
       " 'In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.',\n",
       " 'In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.',\n",
       " 'Therefore, if all weights have the same initial value, this would lead to underfitting.',\n",
       " 'Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below.',\n",
       " '\\xa0',\n",
       " '5. List the supervised and unsupervised tasks in Deep Learning.',\n",
       " 'Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:',\n",
       " 'On the other hand, there are some unsupervised deep learning techniques as well:',\n",
       " 'Here is a great article on applications of Deep Learning for unsupervised tasks:',\n",
       " '\\xa0',\n",
       " '6. What is the role of weights and bias in a neural network?',\n",
       " 'This is a question best explained with a real-life example. Consider that you want to go out today to play a cricket match with your friends. Now, a number of factors can affect your decision-making, like:',\n",
       " 'And so on. These factors can change your decision greatly or not too much. For example, if it is raining outside, then you cannot go out to play at all. Or if you have only one bat, you can share it while playing as well. The magnitude by which these factors can affect the game is called the weight of that factor.',\n",
       " 'Factors like the weather or temperature might have a higher weight, and other factors like equipment would have a lower weight.',\n",
       " 'However, does this mean that we can play a cricket match with only one bat? No – we would need 1 ball and 6 wickets as well. This is where bias comes into the picture. Bias lets you assign some threshold which helps you activate a decision-point (or a neuron) only when that threshold is crossed.',\n",
       " '\\xa0',\n",
       " '7. How does forward propagation and backpropagation work in deep learning?',\n",
       " 'Now, this can be answered in two ways. If you are on a phone interview, you cannot perform all the calculus in writing and show the interviewer. In such cases, it best to explain it as such:',\n",
       " 'For an in-person interview, it is best to take up the marker, create a simple neural network with 2 inputs, a hidden layer, and an output layer, and explain it.',\n",
       " '',\n",
       " 'Forward propagation:',\n",
       " '',\n",
       " 'Backpropagation:',\n",
       " 'At layer L2, for all weights:',\n",
       " '',\n",
       " 'At layer L1, for all weights:',\n",
       " '',\n",
       " 'You need not explain with respect to the bias term as well, though you might need to expand the above equations substituting the actual derivatives.',\n",
       " '\\xa0',\n",
       " '8. What are the common data structures used in Deep Learning?',\n",
       " 'Deep Learning goes right from the simplest data structures like lists to complicated ones like computation graphs.',\n",
       " 'Here are the most common ones:',\n",
       " '\\xa0',\n",
       " 'Once the basics are out of the way, the interview would lead to slightly advanced deep learning concepts. These questions are much easier to answer if you have considerable practice with not only the mathematical concepts but also with coding them.',\n",
       " 'Additionally, these questions can also become more project-specific. As a general rule of thumb, it is best to include examples of how you have used the concept asked in the question in your own projects. This has two advantages:',\n",
       " 'Here, I have given an overview of the key concepts in the questions – you can always customize your answers to add more about your experiences with some of these deep learning algorithms and techniques.',\n",
       " '\\xa0',\n",
       " '9. Why should we use Batch Normalization?',\n",
       " 'Once the interviewer has asked you about the fundamentals of deep learning architectures, they would move on to the key topic of improving your deep learning model’s performance.',\n",
       " 'Batch Normalization is one of the techniques used for reducing the training time of our deep learning algorithm. Just like normalizing our input helps improve our logistic regression model, we can normalize the activations of the hidden layers in our deep learning model as well:',\n",
       " '',\n",
       " 'We basically normalize a[1] and a[2] here. This means we normalize the inputs to the layer, and then apply the activation functions to the normalized inputs.',\n",
       " 'Here is an article that explains Batch Normalization and other techniques for improving Neural Networks: Neural Networks – Hyperparameter Tuning, Regularization & Optimization.',\n",
       " '\\xa0',\n",
       " '10. List the activation functions you have used so far in your projects and how you would choose one.',\n",
       " 'The most common activation functions are:',\n",
       " 'While it is not important to know all the activation functions, you can always score points by knowing the range of these functions and how they are used. Here is a handy table for you to follow:',\n",
       " '',\n",
       " 'Here is a great guide on how to use these and other activations functions: Fundamentals of Deep Learning – Activation Functions and When to Use Them?.',\n",
       " '',\n",
       " '11. Why does a Convolutional Neural Network (CNN) work better with image data?',\n",
       " 'The key to this question lies in the Convolution operation. Unlike humans, the machine sees the image as a matrix of pixel values. Instead of interpreting a shape like a petal or an ear, it just identifies curves and edges.',\n",
       " 'Thus, instead of looking at the entire image, it helps to just read the image in parts. Doing this for a 300 x 300 pixel image would mean dividing the matrix into smaller 3 x 3 matrices and dealing with them one by one. This is convolution.',\n",
       " 'Mathematically, we just perform a small operation on the matrix to help us detect features in the image – like boundaries, colors, etc.',\n",
       " 'Z = X * f',\n",
       " 'Here, we are convolving (* operation – not multiplication) the input matrix X with another small matrix f, called the kernel/filter to create a new matrix Z. This matrix is then passed on to the other layers.',\n",
       " 'If you have a board/screen in front of you, you can always illustrate this with a simple example:',\n",
       " '',\n",
       " 'Learning more about how CNNs work here.',\n",
       " '\\xa0',\n",
       " '12. Why do RNNs work better with text data?',\n",
       " 'The main component that differentiates Recurrent Neural Networks (RNN) from the other models is the addition of a loop at each node. This loop brings the recurrence mechanism in RNNs. In a basic Artificial Neural Network (ANN), each input is given the same weight and fed to the network at the same time. So, for a sentence like “I saw the movie and hated it”, it would be difficult to capture the information which associates “it” with the “movie”.',\n",
       " '',\n",
       " 'The addition of a loop is to denote preserving the previous node’s information for the next node, and so on. This is why RNNs are much better for sequential data, and since text data also is sequential in nature, they are an improvement over ANNs.',\n",
       " '',\n",
       " '13. In a CNN, if the input size 5 X 5 and the filter size is 7 X 7, then what would be the size of the output?',\n",
       " 'This is a pretty intuitive answer. As we saw above, we perform the convolution on ‘x’ one step at a time, to the right, and in the end, we got Z with dimensions 2 X 2, for X with dimensions 3 X 3.',\n",
       " 'Thus, to make the input size similar to the filter size, we make use of padding – adding 0s to the input matrix such that its new size becomes at least 7 X 7. Thus, the output size would be using the formula:',\n",
       " 'Dimension of image = (n, n) = 5 X 5',\n",
       " 'Dimension of filter = (f,f)\\xa0 = 7 X 7',\n",
       " 'Padding = 1 (adding 1 pixel with value 0 all around the edges)',\n",
       " 'Dimension of output will be (n+2p-f+1) X (n+2p-f+1) = 1 X 1',\n",
       " '\\xa0',\n",
       " '14. What’s the difference between valid and same padding in a CNN?',\n",
       " 'This question has more chances of being a follow-up question to the previous one. Or if you have explained how you used CNNs in a computer vision task, the interviewer might ask this question along with the details of the padding parameters.',\n",
       " '\\xa0',\n",
       " '15. What do you mean by exploding and vanishing gradients?',\n",
       " 'The key here is to make the explanation as simple as possible. As we know, the gradient descent algorithm tries to minimize the error by taking small steps towards the minimum value. These steps are used to update the weights and biases in a neural network.',\n",
       " 'However, at times, the steps become too large and this results in larger updates to weights and bias terms – so much so as to cause an overflow (or a NaN) value in the weights. This leads to an unstable algorithm and is called an exploding gradient.',\n",
       " 'On the other hand, the steps are too small and this leads to minimal changes in the weights and bias terms – even negligible changes at times. We thus might end up training a deep learning model with almost the same weights and biases each time and never reach the minimum error function. This is called the vanishing gradient.',\n",
       " 'A point to note is that both these issues are specifically evident in Recurrent Neural Networks – so be prepared for follow-up questions on RNN!',\n",
       " '\\xa0',\n",
       " '16. What are the applications of transfer learning in Deep Learning?',\n",
       " 'I am sure you would have a doubt as to why a relatively simple question was included in the Intermediate Level. The reason is the sheer volume of subsequent questions it can generate!',\n",
       " 'The use of transfer learning has been one of the key milestones in deep learning. Training a large model on a huge dataset, and then using the final parameters on smaller simpler datasets has led to defining breakthroughs in the form of Pretrained Models. Be it Computer Vision or NLP, pretrained models have become the norm in research and in the industry.',\n",
       " 'Some popular examples include BERT, ResNet, GPT-2, VGG-16, etc and many more.',\n",
       " 'It is here that you can earn brownie points by pointing out specific examples/projects where you used these models and how you used them as well.',\n",
       " 'It is not possible to discuss all of them, so here are a few resources to get started:',\n",
       " '\\xa0',\n",
       " 'It is here that questions become really specific to your projects or to what you have discussed in the interview before.',\n",
       " 'Also, depending on the domain – with Computer Vision or Natural Language Processing, these questions can change. While it is not important to know the architecture of each model in detail, you would need to know the intuition behind them and why these models were needed in the first place.',\n",
       " 'Again, just like the intermediate level, it is important to always bring in examples that you have studied or implemented yourself into the discussion.',\n",
       " '\\xa0',\n",
       " '17. How backpropagation is different in RNN compared to ANN?',\n",
       " 'In Recurrent Neural Networks, we have an additional loop at each node:',\n",
       " '',\n",
       " 'This loop essentially includes a time component into the network as well. This helps in capturing sequential information from the data, which could not be possible in a generic artificial neural network.',\n",
       " 'This is why the backpropagation in RNN is called Backpropagation through Time, as in backpropagation at each time step.',\n",
       " 'You can find a detailed explanation of RNNs here: Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks.',\n",
       " '\\xa0',\n",
       " '18. How does LSTM solve the vanishing gradient challenge?',\n",
       " 'The LSTM model is considered a special case of RNNs. The problems of vanishing gradients and exploding gradients we saw earlier are a disadvantage while using the plain RNN model.',\n",
       " 'In LSTMs, we add a forget gate, which is basically a memory unit that retains information that is retained across timesteps and discards the other information that is not needed. This also necessitates the need for input and output gates to include the results of the forget gate as well.',\n",
       " '',\n",
       " '\\xa0',\n",
       " '19. Why is GRU faster as compared to LSTM?',\n",
       " 'As you can see, the LSTM model can become quite complex. In order to still retain the functionality of retaining information across time and yet not make a too complex model, we need GRUs.',\n",
       " 'Basically, in GRUs, instead of having an additional Forget gate, we combine the input and Forget gates into a single Update Gate:',\n",
       " '',\n",
       " 'It is this reduction in the number of gates that makes GRU less complex and faster than LSTM. You can learn about GRUs, LSTMs and other sequence models in detail here: Must-Read Tutorial to Learn Sequence Modeling & Attention Models.',\n",
       " '\\xa0',\n",
       " '20. How is the transformer architecture better than RNN?',\n",
       " 'Advancements in deep learning have made it possible to solve many tasks in Natural Language Processing. Networks/Sequence models like RNNs, LSTMs, etc. are specifically used for this purpose – so as to capture all possible information from a given sentence, or a paragraph. However, sequential processing comes with its caveats:',\n",
       " 'This gave rise to the Transformer architecture. Transformers use what is called the attention mechanism. This basically means mapping dependencies between all the parts of a sentence.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new4 = lst4[20:157]\n",
    "lst_new4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between a Perceptron and Logistic Regression?</td>\n",
       "      <td>A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.So, what’s the difference between the two?Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can we have the same bias for all neurons of a hidden layer?</td>\n",
       "      <td>Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.A point to note is that both these strategies would give you very different results.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What if we do not use any activation function(s) in a neural network?\\n</td>\n",
       "      <td>The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:Z = (weights * X) + biasStep 2: Apply an activation function to calculate the expected output:Y = Activation(Z)Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?Our equation for Y essentially becomes:Y = Z = (weights * X) + biasWait – isn’t this just a simple linear equation? Yes – and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In a neural network, what if all the weights are initialized with the same value?</td>\n",
       "      <td>In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.Therefore, if all weights have the same initial value, this would lead to underfitting.Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List the supervised and unsupervised tasks in Deep Learning.</td>\n",
       "      <td>Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:On the other hand, there are some unsupervised deep learning techniques as well:Here is a great article on applications of Deep Learning for unsupervised tasks:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             Questions  \\\n",
       "0                 What is the difference between a Perceptron and Logistic Regression?   \n",
       "1                         Can we have the same bias for all neurons of a hidden layer?   \n",
       "2              What if we do not use any activation function(s) in a neural network?\\n   \n",
       "3    In a neural network, what if all the weights are initialized with the same value?   \n",
       "4                         List the supervised and unsupervised tasks in Deep Learning.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   A Multi-Layer Perceptron (MLP) is one of the most basic neural networks that we use for classification. For a binary classification problem, we know that the output can be either 0 or 1. This is just like our simple logistic regression, where we use a logit function to generate a probability between 0 and 1.So, what’s the difference between the two?Simply put, it is just the difference in the threshold function! When we restrict the logistic regression model to give us either exactly 1 or exactly 0, we get a Perceptron model:   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Essentially, you can have a different bias value at each layer or at each neuron as well. However, it is best if we have a bias matrix for all the neurons in the hidden layers as well.A point to note is that both these strategies would give you very different results.   \n",
       "2  The main aim of this question is to understand why we need activation functions in a neural network. You can start off by giving a simple explanation of how neural networks are built:Step 1: Calculate the sum of all the inputs (X) according to their weights and include the bias term:Z = (weights * X) + biasStep 2: Apply an activation function to calculate the expected output:Y = Activation(Z)Steps 1 and 2 are performed at each layer. If you recollect, this is nothing but forward propagation! Now, what if there is no activation function?Our equation for Y essentially becomes:Y = Z = (weights * X) + biasWait – isn’t this just a simple linear equation? Yes – and that is why we need activation functions. A linear equation will not be able to capture the complex patterns in the data – this is even more evident in the case of deep learning problems.In order to capture non-linear relationships, we use activation functions, and that is why a neural network without an activation function is just a linear regression model.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                        In simplest terms, if all the neurons have the same value of weights, each hidden unit will get exactly the same signal. While this might work during forward propagation, the derivative of the cost function during backward propagation would be the same every time.In short, there is no learning happening by the network! What do you call the phenomenon of the model being unable to learn any patterns from the data? Yes, underfitting.Therefore, if all weights have the same initial value, this would lead to underfitting.Note: This question might further lead to questions on exploding and vanishing gradients, which I have covered below.   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Now, this can be one tricky question. There might be a misconception that deep learning can only solve unsupervised learning problems. This is not the case. Some example of Supervised Learning and Deep learning include:On the other hand, there are some unsupervised deep learning techniques as well:Here is a great article on applications of Deep Learning for unsupervised tasks:   "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new4:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df4=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n Home\\n Interview Questions\\n Java\\n SQL\\n Python\\n JavaScript\\n Angular\\n Selenium\\n Spring Boot\\n HR\\n C\\n Data Structure\\n DBMS\\n HTML\\n C#\\n C++\\n',\n",
       " 'A list of top frequently asked Deep Learning Interview Questions and answers are given below. ',\n",
       " '1) What is deep learning?',\n",
       " 'Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.',\n",
       " '2) What are the main differences between AI, Machine Learning, and Deep Learning?',\n",
       " '\\nAI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior.\\nMachine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.\\n',\n",
       " '\\nDeep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making.\\n',\n",
       " '3) Differentiate supervised and unsupervised deep learning procedures.',\n",
       " '\\nSupervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing.\\nUnsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data.\\n',\n",
       " '4) What are the applications of deep learning?',\n",
       " 'There are various applications of deep learning:',\n",
       " '\\nComputer vision\\nNatural language processing and pattern recognition\\nImage recognition and processing\\nMachine translation\\nSentiment analysis\\nQuestion Answering system\\nObject Classification and Detection\\nAutomatic Handwriting Generation\\nAutomatic Text Generation.\\n',\n",
       " '5) Do you think that deep network is better than a shallow one?',\n",
       " 'Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input.',\n",
       " '6) What do you mean by \"overfitting\"?',\n",
       " 'Overfitting is the most common issue which occurs in deep learning. It usually occurs when a deep learning algorithm apprehends the sound of specific data. It also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.',\n",
       " '7) What is Backpropagation?',\n",
       " 'Backpropagation is a training algorithm which is used for multilayer neural networks. It transfers the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient.',\n",
       " 'Backpropagation can be divided into the following steps:',\n",
       " '\\nIt can forward propagation of training data through the network to generate output.\\nIt uses target value and output value to compute error derivative concerning output activations.\\nIt can backpropagate to compute the derivative of the error concerning output activations in the previous layer and continue for all hidden layers.\\nIt uses the previously calculated derivatives for output and all hidden layers to calculate the error derivative concerning weights.\\nIt updates the weights.\\n',\n",
       " '8) What is the function of the Fourier Transform in Deep Learning?',\n",
       " 'Fourier transform package is highly efficient for analyzing, maintaining, and managing a large databases. The software is created with a high-quality feature known as the special portrayal. One can effectively utilize it to generate real-time array data, which is extremely helpful for processing all categories of signals.',\n",
       " '9) Describe the theory of autonomous form of deep learning in a few words.',\n",
       " 'There are several forms and categories available for the particular subject, but the autonomous pattern represents independent or unspecified mathematical bases which are free from any specific categorizer or formula.',\n",
       " \"10) What is the use of Deep learning in today's age, and how is it adding data scientists?\",\n",
       " 'Deep learning has brought significant changes or revolution in the field of machine learning and data science. The concept of a complex neural network (CNN) is the main center of attention for data scientists. It is widely taken because of its advantages in performing next-level machine learning operations. The advantages of deep learning also include the process of clarifying and simplifying issues based on an algorithm due to its utmost flexible and adaptable nature. It is one of the rare procedures which allow the movement of data in independent pathways. Most of the data scientists are viewing this particular medium as an advanced additive and extended way to the existing process of machine learning and utilizing the same for solving complex day to day issues.',\n",
       " '11) What are the deep learning frameworks or tools?',\n",
       " 'Deep learning frameworks or tools are:',\n",
       " 'Tensorflow, Keras, Chainer, Pytorch, Theano & Ecosystem, Caffe2, CNTK, DyNetGensim, DSSTNE, Gluon, Paddle, Mxnet, BigDL',\n",
       " '12) What are the disadvantages of deep learning?',\n",
       " 'There are some disadvantages of deep learning, which are:',\n",
       " '\\nDeep learning model takes longer time to execute the model. In some cases, it even takes several days to execute a single model depends on complexity.\\nThe deep learning model is not good for small data sets, and it fails here.\\n',\n",
       " '13) What is the meaning of term weight initialization in neural networks?',\n",
       " 'In neural networking, weight initialization is one of the essential factors. A bad weight initialization prevents a network from learning. On the other side, a good weight initialization helps in giving a quicker convergence and a better overall error. Biases can be initialized to zero. The standard rule for setting the weights is to be close to zero without being too small.',\n",
       " '14) Explain Data Normalization.',\n",
       " 'Data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. It assures better convergence during backpropagation. In general, data normalization boils down to subtracting the mean of each data point and dividing by its standard deviation.',\n",
       " '15) Why is zero initialization not a good weight initialization process?',\n",
       " 'If the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.',\n",
       " 'As a result, the network cannot learn at all because there is no source of asymmetry between neurons. That is the reason why we need to add randomness to the weight initialization process.',\n",
       " '16) What are the prerequisites for starting in Deep Learning?',\n",
       " 'There are some basic requirements for starting in Deep Learning, which are:',\n",
       " '\\nMachine Learning\\nMathematics\\nPython Programming\\n',\n",
       " '17) What are the supervised learning algorithms in Deep learning?',\n",
       " '\\nArtificial neural network\\nConvolution neural network\\nRecurrent neural network\\n',\n",
       " '18) What are the unsupervised learning algorithms in Deep learning?',\n",
       " '\\nSelf Organizing Maps\\nDeep belief networks (Boltzmann Machine)\\nAuto Encoders\\n',\n",
       " '19) How many layers in the neural network?',\n",
       " '\\nInput Layer\\nThe input layer contains input neurons which send information to the hidden layer.\\nHidden Layer\\nThe hidden layer is used to send data to the output layer.\\nOutput Layer\\nThe data is made available at the output layer.\\n',\n",
       " '20) What is the use of the Activation function?',\n",
       " 'The activation function is used to introduce nonlinearity into the neural network so that it can learn more complex function. Without the Activation function, the neural network would be only able to learn function, which is a linear combination of its input data.',\n",
       " 'Activation function translates the inputs into outputs. The activation function is responsible for deciding whether a neuron should be activated or not. It makes the decision by calculating the weighted sum and further adding bias with it. The basic purpose of the activation function is to introduce non-linearity into the output of a neuron.',\n",
       " '21) How many types of activation function are available?',\n",
       " '\\nBinary Step\\nSigmoid\\nTanh\\nReLU\\nLeaky ReLU\\nSoftmax\\nSwish\\n',\n",
       " '22) What is a binary step function?',\n",
       " 'The binary step function is an activation function, which is usually based on a threshold. If the input value is above or below a particular threshold limit, the neuron is activated, then it sends the same signal to the next layer. This function does not allow multi-value outputs.',\n",
       " '23) What is the sigmoid function?',\n",
       " 'The sigmoid activation function is also called the logistic function. It is traditionally a trendy activation function for neural networks. The input data to the function is transformed into a value between 0.0 and 1.0. Input values that are much larger than 1.0 are transformed to the value 1.0. Similarly, values that are much smaller than 0.0 are transformed into 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. It was the default activation used on neural networks, in the early 1990s.',\n",
       " '24) What is Tanh function?',\n",
       " 'The hyperbolic tangent function, also known as tanh for short, is a similar shaped nonlinear activation function. It provides output values between -1.0 and 1.0. Later in the 1990s and through the 2000s, this function was preferred over the sigmoid activation function as models. It was easier to train and often had better predictive performance.',\n",
       " '25) What is ReLU function?',\n",
       " 'A node or unit which implements the activation function is referred to as a rectified linear activation unit or ReLU for short. Generally, networks that use the rectifier function for the hidden layers are referred to as rectified networks.',\n",
       " 'Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution.',\n",
       " '26) What is the use of leaky ReLU function?',\n",
       " 'The Leaky ReLU (LReLU or LReL) manages the function to allow small negative values when the input is less than zero.',\n",
       " '27) What is the softmax function?',\n",
       " \"The softmax function is used to calculate the probability distribution of the event over 'n' different events. One of the main advantages of using softmax is the output probabilities range. The range will be between 0 to 1, and the sum of all the probabilities will be equal to one. When the softmax function is used for multi-classification model, it returns the probabilities of each class, and the target class will have a high probability.\",\n",
       " '28) What is a Swish function?',\n",
       " 'Swish is a new, self-gated activation function. Researchers at Google discovered the Swish function. According to their paper, it performs better than ReLU with a similar level of computational efficiency. ',\n",
       " '29) What is the most used activation function?',\n",
       " 'Relu function is the most used activation function. It helps us to solve vanishing gradient problems.',\n",
       " '30) Can Relu function be used in output layer?',\n",
       " 'No, Relu function has to be used in hidden layers.',\n",
       " '31) In which layer softmax activation function used?',\n",
       " 'Softmax activation function has to be used in the output layer.',\n",
       " '32) What do you understand by Autoencoder?',\n",
       " 'Autoencoder is an artificial neural network. It can learn representation for a set of data without any supervision. The network automatically learns by copying its input to the output; typically,internet representation consists of smaller dimensions than the input vector. As a result, they can learn efficient ways of representing the data. Autoencoder consists of two parts; an encoder tries to fit the inputs to the internal representation, and a decoder converts the internal state to the outputs.',\n",
       " '33) What do you mean by Dropout?',\n",
       " \"Dropout is a cheap regulation technique used for reducing overfitting in neural networks. We randomly drop out a set of nodes at each training step. As a result, we create a different model for each training case, and all of these models share weights. It's a form of model averaging.\",\n",
       " '34) What do you understand by Tensors?',\n",
       " 'Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, which allows us to represent the data having higher dimensions. In general, we deal with high dimensional data sets where dimensions refer to different features present in the data set.',\n",
       " '35) What do you understand by Boltzmann Machine?',\n",
       " 'A Boltzmann machine (also known as stochastic Hopfield network with hidden units) is a type of recurrent neural network. In a Boltzmann machine, nodes make binary decisions with some bias. Boltzmann machines can be strung together to create more sophisticated systems such as deep belief networks. Boltzmann Machines can be used to optimize the solution to a problem. ',\n",
       " 'Some important points about Boltzmann Machine-',\n",
       " '\\nIt uses a recurrent structure.\\nIt consists of stochastic neurons, which include one of the two possible states, either 1 or 0.\\nThe neurons present in this are either in an adaptive state (free state) or clamped state (frozen state).\\nIf we apply simulated annealing or discrete Hopfield network, then it would become a Boltzmann Machine.\\n',\n",
       " '36) What is Model Capacity?',\n",
       " 'The capacity of a deep learning neural network controls the scope of the types of mapping functions that it can learn. Model capacity can approximate any given function. When there is a higher model capacity, it means that the larger amount of information can be stored in the network.',\n",
       " '37) What is the cost function?',\n",
       " \"A cost function describes us how well the neural network is performing with respect to its given training sample and the expected output. It may depend on variables such as weights and biases.It provides the performance of a neural network as a whole. In deep learning, our priority is to minimize the cost function. That's why we prefer to use the concept of gradient descent.\",\n",
       " '38) Explain gradient descent?',\n",
       " \"An optimization algorithm that is used to minimize some function by repeatedly moving in the direction of steepest descent as specified by the negative of the gradient is known as gradient descent. It's an iteration algorithm, in every iteration algorithm, we compute the gradient of a cost function, concerning each parameter and update the parameter of the function via the following formula:\",\n",
       " 'Where,',\n",
       " 'Θ - is the parameter vector, ',\n",
       " 'α - learning rate, ',\n",
       " ' J(Θ) - is a cost function',\n",
       " 'In machine learning, it is used to update the parameters of our model. Parameters represent the coefficients in linear regression and weights in neural networks.',\n",
       " '39) Explain the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch?',\n",
       " '\\nStochastic Gradient Descent\\nStochastic gradient descent is used to calculate the gradient and update the parameters by using only a single training example.\\nBatch Gradient Descent\\nBatch gradient descent is used to calculate the gradients for the whole dataset and perform just one update at each iteration.\\nMini-batch Gradient Descent\\nMini-batch gradient descent is a variation of stochastic gradient descent. Instead of a single training example, mini-batch of samples is used. Mini-batch gradient descent is one of the most popular optimization algorithms.\\n',\n",
       " '40) What are the main benefits of Mini-batch Gradient Descent?',\n",
       " '\\nIt is computationally efficient compared to stochastic gradient descent.\\nIt improves generalization by finding flat minima.\\nIt improves convergence by using mini-batches. We can approximate the gradient of the entire training set, which might help to avoid local minima.\\n',\n",
       " '41) What is matrix element-wise multiplication? Explain with an example.',\n",
       " 'Element-wise matrix multiplication is used to take two matrices of the same dimensions. It further produces another combined matrix with the elements that are a product of corresponding elements of matrix a and b.',\n",
       " '42) What do you understand by a convolutional neural network?',\n",
       " 'A convolutional neural network, often called CNN, is a feedforward neural network. It uses convolution in at least one of its layers. The convolutional layer contains a set of filter (kernels). This filter is sliding across the entire input image, computing the dot product between the weights of the filter and the input image. As a result of training, the network automatically learns filters that can detect specific features.',\n",
       " '43) Explain the different layers of CNN.',\n",
       " 'There are four layered concepts that we should understand in CNN (Convolutional Neural Network):',\n",
       " '\\nConvolution\\nThis layer comprises of a set of independent filters. All these filters are initialized randomly. These filters then become our parameters which will be learned by the network subsequently.\\nReLU\\nThe ReLu layer is used with the convolutional layer.\\nPooling\\nIt reduces the spatial size of the representation to lower the number of parameters and computation in the network. This layer operates on each feature map independently.\\nFull Collectedness\\nNeurons in a completely connected layer have complete connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can be easily computed with a matrix multiplication followed by a bias offset.\\n',\n",
       " '44) What is an RNN?',\n",
       " \"RNN stands for Recurrent Neural Networks. These are the artificial neural networks which are designed to recognize patterns in sequences of data such as handwriting, text, the spoken word, genomes, and numerical time series data. RNN use backpropagation algorithm for training because of their internal memory. RNN can remember important things about the input they received, which enables them to be very precise in predicting what's coming next.\",\n",
       " '45) What are the issues faced while training in Recurrent Networks?',\n",
       " 'Recurrent Neural Network uses backpropagation algorithm for training, but it is applied on every timestamp. It is usually known as Back-propagation Through Time (BTT).',\n",
       " 'There are two significant issues with Back-propagation, such as:',\n",
       " '\\nVanishing Gradient\\nWhen we perform Back-propagation, the gradients tend to get smaller and smaller because we keep on moving backward in the Network. As a result, the neurons in the earlier layer learn very slowly if we compare it with the neurons in the later layers.Earlier layers are more valuable because they are responsible for learning and detecting simple patterns. They are the building blocks of the network.\\nIf they provide improper or inaccurate results, then how can we expect the next layers and complete network to perform nicely and provide accurate results. The training procedure tales long, and the prediction accuracy of the model decreases.\\nExploding Gradient\\nExploding gradients are the main problem when large error gradients accumulate. They provide result in very large updates to neural network model weights during training.\\nGradient Descent process works best when updates are small and controlled. When the magnitudes of the gradient accumulate, an unstable network is likely to occur. It can cause poor prediction of results or even a model that reports nothing useful.\\n',\n",
       " '46) Explain the importance of LSTM.',\n",
       " 'LSTM stands for Long short-term memory. It is an artificial RNN (Recurrent Neural Network) architecture, which is used in the field of deep learning. LSTM has feedback connections which makes it a \"general purpose computer.\" It can process not only a single data point but also entire sequences of data.',\n",
       " 'They are a special kind of RNN which are capable of learning long-term dependencies.',\n",
       " '47) What are the different layers of Autoencoders? Explain briefly.',\n",
       " 'An autoencoder contains three layers:',\n",
       " '\\nEncoder\\nThe encoder is used to compress the input into a latent space representation. It encodes the input images as a compressed representation in a reduced dimension. The compressed images are the distorted version of the original image.\\nCode\\nThe code layer is used to represent the compressed input which is fed to the decoder.\\nDecoder\\nThe decoder layer decodes the encoded image back to its original dimension. The decoded image is a reduced reconstruction of the original image. It is automatically reconstructed from the latent space representation.\\n',\n",
       " '48) What do you understand by Deep Autoencoders?',\n",
       " 'Deep Autoencoder is the extension of the simple Autoencoder. The first layer present in DeepAutoencoder is responsible for first-order functions in the raw input. The second layer is responsible for second-order functions corresponding to patterns in the appearance of first-order functions. Deeper layers which are available in the Deep Autoencoder tend to learn even high-order features.',\n",
       " 'A deep autoencoder is the combination of two, symmetrical deep-belief networks:',\n",
       " '\\nFirst four or five shallow layers represent the encoding half.\\nThe other combination of four or five layers makes up the decoding half.\\n',\n",
       " '49) What are the three steps to developing the necessary assumption structure in Deep learning?',\n",
       " 'The procedure of developing an assumption structure involves three specific actions. ',\n",
       " '\\nThe first step contains algorithm development. This particular process is lengthy.\\nThe second step contains algorithm analyzing, which represents the in-process methodology. \\nThe third step is about implementing the general algorithm in the final procedure. The entire framework is interlinked and required for throughout the process.\\n',\n",
       " '50) What do you understand by Perceptron? Also, explain its type.',\n",
       " 'A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features. It is an algorithm for supervised learning of binary classifiers. This algorithm is used to enable neurons to learn and processes elements in the training set one at a time.',\n",
       " 'There are two types of perceptrons:',\n",
       " '\\nSingle-Layer Perceptron\\nSingle layer perceptrons can learn only linearly separable patterns.\\nMultilayer Perceptrons\\nMultilayer perceptrons or feedforward neural networks with two or more layers have the higher processing power.\\n',\n",
       " '\\nJava Interview Questions\\nSQL Interview Questions\\nPython Interview Questions\\nJavaScript Interview Questions\\nAngular Interview Questions\\nSelenium Interview Questions\\nSpring Boot Interview Questions\\nHR Interview Questions\\nC Programming Interview Questions\\nC++ Interview Questions\\nData Structure Interview Questions\\nDBMS Interview Questions\\nHTML Interview Questions\\nIAS Interview Questions\\nManual Testing Interview Questions\\nOOPs Interview Questions\\n.Net Interview Questions\\nC# Interview Questions\\nReactJS Interview Questions\\nNetworking Interview Questions\\nPHP Interview Questions\\nCSS Interview Questions\\nNode.js Interview Questions\\nSpring Interview Questions\\nHibernate Interview Questions\\nAWS Interview Questions\\nAccounting Interview Questions\\n',\n",
       " 'Splunk',\n",
       " 'SPSS',\n",
       " 'Swagger',\n",
       " 'Transact-SQL',\n",
       " 'Tumblr',\n",
       " 'ReactJS',\n",
       " 'Regex',\n",
       " 'Reinforcement Learning',\n",
       " 'R Programming',\n",
       " 'RxJS',\n",
       " 'React Native',\n",
       " 'Python Design Patterns',\n",
       " 'Python Pillow',\n",
       " 'Python Turtle',\n",
       " 'Keras',\n",
       " 'Aptitude',\n",
       " 'Reasoning',\n",
       " 'Verbal Ability',\n",
       " 'Interview Questions',\n",
       " 'Company Questions',\n",
       " 'Artificial Intelligence',\n",
       " 'AWS',\n",
       " 'Selenium',\n",
       " 'Cloud Computing',\n",
       " 'Hadoop',\n",
       " 'ReactJS',\n",
       " 'Data Science',\n",
       " 'Angular 7',\n",
       " 'Blockchain',\n",
       " 'Git',\n",
       " 'Machine Learning',\n",
       " 'DevOps',\n",
       " 'DBMS',\n",
       " 'Data Structures',\n",
       " 'DAA',\n",
       " 'Operating System',\n",
       " 'Computer Network',\n",
       " 'Compiler Design',\n",
       " 'Computer Organization',\n",
       " 'Discrete Mathematics',\n",
       " 'Ethical Hacking',\n",
       " 'Computer Graphics',\n",
       " 'Software Engineering',\n",
       " 'Web Technology',\n",
       " 'Cyber Security',\n",
       " 'Automata',\n",
       " 'C Programming',\n",
       " 'C++',\n",
       " 'Java',\n",
       " '.Net',\n",
       " 'Python',\n",
       " 'Programs',\n",
       " 'Control System',\n",
       " 'Data Mining',\n",
       " 'Data Warehouse',\n",
       " 'JavaTpoint offers too many high quality services. Mail us on [email\\xa0protected], to get more information about given services. ',\n",
       " ' Website DesigningWebsite DevelopmentJava DevelopmentPHP DevelopmentWordPressGraphic DesigningLogoDigital MarketingOn Page and Off Page SEOPPCContent DevelopmentCorporate TrainingClassroom and Online TrainingData Entry',\n",
       " '',\n",
       " 'JavaTpoint offers college campus training on Core Java, Advance Java, .Net, Android, Hadoop, PHP, Web Technology and Python. Please mail your requirement at [email\\xa0protected] Duration: 1 week to 2 week',\n",
       " 'Learn Tutorials',\n",
       " 'Our Websites',\n",
       " 'Our Services',\n",
       " 'Website Development',\n",
       " 'Android Development',\n",
       " 'Website Designing',\n",
       " 'Digital Marketing',\n",
       " 'Summer Training',\n",
       " 'Industrial Training',\n",
       " 'College Campus Training',\n",
       " 'Contact',\n",
       " 'Address: G-13, 2nd Floor, Sec-3',\n",
       " 'Noida, UP, 201301, India',\n",
       " 'Contact No: 0120-4256464, 9990449935',\n",
       " '© Copyright 2011-2021 www.javatpoint.com. All rights reserved. Developed by JavaTpoint.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst5 = []\n",
    "url = \"https://www.javatpoint.com/deep-learning-interview-questions\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h3','p','ul'])\n",
    "for answer in answers:\n",
    "    lst5.append(answer.text)\n",
    "lst5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1) What is deep learning?',\n",
       " 'Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.',\n",
       " '2) What are the main differences between AI, Machine Learning, and Deep Learning?',\n",
       " '\\nAI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior.\\nMachine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.\\n',\n",
       " '\\nDeep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making.\\n',\n",
       " '3) Differentiate supervised and unsupervised deep learning procedures.',\n",
       " '\\nSupervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing.\\nUnsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data.\\n',\n",
       " '4) What are the applications of deep learning?',\n",
       " 'There are various applications of deep learning:',\n",
       " '\\nComputer vision\\nNatural language processing and pattern recognition\\nImage recognition and processing\\nMachine translation\\nSentiment analysis\\nQuestion Answering system\\nObject Classification and Detection\\nAutomatic Handwriting Generation\\nAutomatic Text Generation.\\n',\n",
       " '5) Do you think that deep network is better than a shallow one?',\n",
       " 'Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input.',\n",
       " '6) What do you mean by \"overfitting\"?',\n",
       " 'Overfitting is the most common issue which occurs in deep learning. It usually occurs when a deep learning algorithm apprehends the sound of specific data. It also appears when the particular algorithm is well suitable for the data and shows up when the algorithm or model represents high variance and low bias.',\n",
       " '7) What is Backpropagation?',\n",
       " 'Backpropagation is a training algorithm which is used for multilayer neural networks. It transfers the error information from the end of the network to all the weights inside the network. It allows the efficient computation of the gradient.',\n",
       " 'Backpropagation can be divided into the following steps:',\n",
       " '\\nIt can forward propagation of training data through the network to generate output.\\nIt uses target value and output value to compute error derivative concerning output activations.\\nIt can backpropagate to compute the derivative of the error concerning output activations in the previous layer and continue for all hidden layers.\\nIt uses the previously calculated derivatives for output and all hidden layers to calculate the error derivative concerning weights.\\nIt updates the weights.\\n',\n",
       " '8) What is the function of the Fourier Transform in Deep Learning?',\n",
       " 'Fourier transform package is highly efficient for analyzing, maintaining, and managing a large databases. The software is created with a high-quality feature known as the special portrayal. One can effectively utilize it to generate real-time array data, which is extremely helpful for processing all categories of signals.',\n",
       " '9) Describe the theory of autonomous form of deep learning in a few words.',\n",
       " 'There are several forms and categories available for the particular subject, but the autonomous pattern represents independent or unspecified mathematical bases which are free from any specific categorizer or formula.',\n",
       " \"10) What is the use of Deep learning in today's age, and how is it adding data scientists?\",\n",
       " 'Deep learning has brought significant changes or revolution in the field of machine learning and data science. The concept of a complex neural network (CNN) is the main center of attention for data scientists. It is widely taken because of its advantages in performing next-level machine learning operations. The advantages of deep learning also include the process of clarifying and simplifying issues based on an algorithm due to its utmost flexible and adaptable nature. It is one of the rare procedures which allow the movement of data in independent pathways. Most of the data scientists are viewing this particular medium as an advanced additive and extended way to the existing process of machine learning and utilizing the same for solving complex day to day issues.',\n",
       " '11) What are the deep learning frameworks or tools?',\n",
       " 'Deep learning frameworks or tools are:',\n",
       " 'Tensorflow, Keras, Chainer, Pytorch, Theano & Ecosystem, Caffe2, CNTK, DyNetGensim, DSSTNE, Gluon, Paddle, Mxnet, BigDL',\n",
       " '12) What are the disadvantages of deep learning?',\n",
       " 'There are some disadvantages of deep learning, which are:',\n",
       " '\\nDeep learning model takes longer time to execute the model. In some cases, it even takes several days to execute a single model depends on complexity.\\nThe deep learning model is not good for small data sets, and it fails here.\\n',\n",
       " '13) What is the meaning of term weight initialization in neural networks?',\n",
       " 'In neural networking, weight initialization is one of the essential factors. A bad weight initialization prevents a network from learning. On the other side, a good weight initialization helps in giving a quicker convergence and a better overall error. Biases can be initialized to zero. The standard rule for setting the weights is to be close to zero without being too small.',\n",
       " '14) Explain Data Normalization.',\n",
       " 'Data normalization is an essential preprocessing step, which is used to rescale values to fit in a specific range. It assures better convergence during backpropagation. In general, data normalization boils down to subtracting the mean of each data point and dividing by its standard deviation.',\n",
       " '15) Why is zero initialization not a good weight initialization process?',\n",
       " 'If the set of weights in the network is put to a zero, then all the neurons at each layer will start producing the same output and the same gradients during backpropagation.',\n",
       " 'As a result, the network cannot learn at all because there is no source of asymmetry between neurons. That is the reason why we need to add randomness to the weight initialization process.',\n",
       " '16) What are the prerequisites for starting in Deep Learning?',\n",
       " 'There are some basic requirements for starting in Deep Learning, which are:',\n",
       " '\\nMachine Learning\\nMathematics\\nPython Programming\\n',\n",
       " '17) What are the supervised learning algorithms in Deep learning?',\n",
       " '\\nArtificial neural network\\nConvolution neural network\\nRecurrent neural network\\n',\n",
       " '18) What are the unsupervised learning algorithms in Deep learning?',\n",
       " '\\nSelf Organizing Maps\\nDeep belief networks (Boltzmann Machine)\\nAuto Encoders\\n',\n",
       " '19) How many layers in the neural network?',\n",
       " '\\nInput Layer\\nThe input layer contains input neurons which send information to the hidden layer.\\nHidden Layer\\nThe hidden layer is used to send data to the output layer.\\nOutput Layer\\nThe data is made available at the output layer.\\n',\n",
       " '20) What is the use of the Activation function?',\n",
       " 'The activation function is used to introduce nonlinearity into the neural network so that it can learn more complex function. Without the Activation function, the neural network would be only able to learn function, which is a linear combination of its input data.',\n",
       " 'Activation function translates the inputs into outputs. The activation function is responsible for deciding whether a neuron should be activated or not. It makes the decision by calculating the weighted sum and further adding bias with it. The basic purpose of the activation function is to introduce non-linearity into the output of a neuron.',\n",
       " '21) How many types of activation function are available?',\n",
       " '\\nBinary Step\\nSigmoid\\nTanh\\nReLU\\nLeaky ReLU\\nSoftmax\\nSwish\\n',\n",
       " '22) What is a binary step function?',\n",
       " 'The binary step function is an activation function, which is usually based on a threshold. If the input value is above or below a particular threshold limit, the neuron is activated, then it sends the same signal to the next layer. This function does not allow multi-value outputs.',\n",
       " '23) What is the sigmoid function?',\n",
       " 'The sigmoid activation function is also called the logistic function. It is traditionally a trendy activation function for neural networks. The input data to the function is transformed into a value between 0.0 and 1.0. Input values that are much larger than 1.0 are transformed to the value 1.0. Similarly, values that are much smaller than 0.0 are transformed into 0.0. The shape of the function for all possible inputs is an S-shape from zero up through 0.5 to 1.0. It was the default activation used on neural networks, in the early 1990s.',\n",
       " '24) What is Tanh function?',\n",
       " 'The hyperbolic tangent function, also known as tanh for short, is a similar shaped nonlinear activation function. It provides output values between -1.0 and 1.0. Later in the 1990s and through the 2000s, this function was preferred over the sigmoid activation function as models. It was easier to train and often had better predictive performance.',\n",
       " '25) What is ReLU function?',\n",
       " 'A node or unit which implements the activation function is referred to as a rectified linear activation unit or ReLU for short. Generally, networks that use the rectifier function for the hidden layers are referred to as rectified networks.',\n",
       " 'Adoption of ReLU may easily be considered one of the few milestones in the deep learning revolution.',\n",
       " '26) What is the use of leaky ReLU function?',\n",
       " 'The Leaky ReLU (LReLU or LReL) manages the function to allow small negative values when the input is less than zero.',\n",
       " '27) What is the softmax function?',\n",
       " \"The softmax function is used to calculate the probability distribution of the event over 'n' different events. One of the main advantages of using softmax is the output probabilities range. The range will be between 0 to 1, and the sum of all the probabilities will be equal to one. When the softmax function is used for multi-classification model, it returns the probabilities of each class, and the target class will have a high probability.\",\n",
       " '28) What is a Swish function?',\n",
       " 'Swish is a new, self-gated activation function. Researchers at Google discovered the Swish function. According to their paper, it performs better than ReLU with a similar level of computational efficiency. ',\n",
       " '29) What is the most used activation function?',\n",
       " 'Relu function is the most used activation function. It helps us to solve vanishing gradient problems.',\n",
       " '30) Can Relu function be used in output layer?',\n",
       " 'No, Relu function has to be used in hidden layers.',\n",
       " '31) In which layer softmax activation function used?',\n",
       " 'Softmax activation function has to be used in the output layer.',\n",
       " '32) What do you understand by Autoencoder?',\n",
       " 'Autoencoder is an artificial neural network. It can learn representation for a set of data without any supervision. The network automatically learns by copying its input to the output; typically,internet representation consists of smaller dimensions than the input vector. As a result, they can learn efficient ways of representing the data. Autoencoder consists of two parts; an encoder tries to fit the inputs to the internal representation, and a decoder converts the internal state to the outputs.',\n",
       " '33) What do you mean by Dropout?',\n",
       " \"Dropout is a cheap regulation technique used for reducing overfitting in neural networks. We randomly drop out a set of nodes at each training step. As a result, we create a different model for each training case, and all of these models share weights. It's a form of model averaging.\",\n",
       " '34) What do you understand by Tensors?',\n",
       " 'Tensors are nothing but a de facto for representing the data in deep learning. They are just multidimensional arrays, which allows us to represent the data having higher dimensions. In general, we deal with high dimensional data sets where dimensions refer to different features present in the data set.',\n",
       " '35) What do you understand by Boltzmann Machine?',\n",
       " 'A Boltzmann machine (also known as stochastic Hopfield network with hidden units) is a type of recurrent neural network. In a Boltzmann machine, nodes make binary decisions with some bias. Boltzmann machines can be strung together to create more sophisticated systems such as deep belief networks. Boltzmann Machines can be used to optimize the solution to a problem. ',\n",
       " 'Some important points about Boltzmann Machine-',\n",
       " '\\nIt uses a recurrent structure.\\nIt consists of stochastic neurons, which include one of the two possible states, either 1 or 0.\\nThe neurons present in this are either in an adaptive state (free state) or clamped state (frozen state).\\nIf we apply simulated annealing or discrete Hopfield network, then it would become a Boltzmann Machine.\\n',\n",
       " '36) What is Model Capacity?',\n",
       " 'The capacity of a deep learning neural network controls the scope of the types of mapping functions that it can learn. Model capacity can approximate any given function. When there is a higher model capacity, it means that the larger amount of information can be stored in the network.',\n",
       " '37) What is the cost function?',\n",
       " \"A cost function describes us how well the neural network is performing with respect to its given training sample and the expected output. It may depend on variables such as weights and biases.It provides the performance of a neural network as a whole. In deep learning, our priority is to minimize the cost function. That's why we prefer to use the concept of gradient descent.\",\n",
       " '38) Explain gradient descent?',\n",
       " \"An optimization algorithm that is used to minimize some function by repeatedly moving in the direction of steepest descent as specified by the negative of the gradient is known as gradient descent. It's an iteration algorithm, in every iteration algorithm, we compute the gradient of a cost function, concerning each parameter and update the parameter of the function via the following formula:\",\n",
       " 'Where,',\n",
       " 'Θ - is the parameter vector, ',\n",
       " 'α - learning rate, ',\n",
       " ' J(Θ) - is a cost function',\n",
       " 'In machine learning, it is used to update the parameters of our model. Parameters represent the coefficients in linear regression and weights in neural networks.',\n",
       " '39) Explain the following variant of Gradient Descent: Stochastic, Batch, and Mini-batch?',\n",
       " '\\nStochastic Gradient Descent\\nStochastic gradient descent is used to calculate the gradient and update the parameters by using only a single training example.\\nBatch Gradient Descent\\nBatch gradient descent is used to calculate the gradients for the whole dataset and perform just one update at each iteration.\\nMini-batch Gradient Descent\\nMini-batch gradient descent is a variation of stochastic gradient descent. Instead of a single training example, mini-batch of samples is used. Mini-batch gradient descent is one of the most popular optimization algorithms.\\n',\n",
       " '40) What are the main benefits of Mini-batch Gradient Descent?',\n",
       " '\\nIt is computationally efficient compared to stochastic gradient descent.\\nIt improves generalization by finding flat minima.\\nIt improves convergence by using mini-batches. We can approximate the gradient of the entire training set, which might help to avoid local minima.\\n',\n",
       " '41) What is matrix element-wise multiplication? Explain with an example.',\n",
       " 'Element-wise matrix multiplication is used to take two matrices of the same dimensions. It further produces another combined matrix with the elements that are a product of corresponding elements of matrix a and b.',\n",
       " '42) What do you understand by a convolutional neural network?',\n",
       " 'A convolutional neural network, often called CNN, is a feedforward neural network. It uses convolution in at least one of its layers. The convolutional layer contains a set of filter (kernels). This filter is sliding across the entire input image, computing the dot product between the weights of the filter and the input image. As a result of training, the network automatically learns filters that can detect specific features.',\n",
       " '43) Explain the different layers of CNN.',\n",
       " 'There are four layered concepts that we should understand in CNN (Convolutional Neural Network):',\n",
       " '\\nConvolution\\nThis layer comprises of a set of independent filters. All these filters are initialized randomly. These filters then become our parameters which will be learned by the network subsequently.\\nReLU\\nThe ReLu layer is used with the convolutional layer.\\nPooling\\nIt reduces the spatial size of the representation to lower the number of parameters and computation in the network. This layer operates on each feature map independently.\\nFull Collectedness\\nNeurons in a completely connected layer have complete connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can be easily computed with a matrix multiplication followed by a bias offset.\\n',\n",
       " '44) What is an RNN?',\n",
       " \"RNN stands for Recurrent Neural Networks. These are the artificial neural networks which are designed to recognize patterns in sequences of data such as handwriting, text, the spoken word, genomes, and numerical time series data. RNN use backpropagation algorithm for training because of their internal memory. RNN can remember important things about the input they received, which enables them to be very precise in predicting what's coming next.\",\n",
       " '45) What are the issues faced while training in Recurrent Networks?',\n",
       " 'Recurrent Neural Network uses backpropagation algorithm for training, but it is applied on every timestamp. It is usually known as Back-propagation Through Time (BTT).',\n",
       " 'There are two significant issues with Back-propagation, such as:',\n",
       " '\\nVanishing Gradient\\nWhen we perform Back-propagation, the gradients tend to get smaller and smaller because we keep on moving backward in the Network. As a result, the neurons in the earlier layer learn very slowly if we compare it with the neurons in the later layers.Earlier layers are more valuable because they are responsible for learning and detecting simple patterns. They are the building blocks of the network.\\nIf they provide improper or inaccurate results, then how can we expect the next layers and complete network to perform nicely and provide accurate results. The training procedure tales long, and the prediction accuracy of the model decreases.\\nExploding Gradient\\nExploding gradients are the main problem when large error gradients accumulate. They provide result in very large updates to neural network model weights during training.\\nGradient Descent process works best when updates are small and controlled. When the magnitudes of the gradient accumulate, an unstable network is likely to occur. It can cause poor prediction of results or even a model that reports nothing useful.\\n',\n",
       " '46) Explain the importance of LSTM.',\n",
       " 'LSTM stands for Long short-term memory. It is an artificial RNN (Recurrent Neural Network) architecture, which is used in the field of deep learning. LSTM has feedback connections which makes it a \"general purpose computer.\" It can process not only a single data point but also entire sequences of data.',\n",
       " 'They are a special kind of RNN which are capable of learning long-term dependencies.',\n",
       " '47) What are the different layers of Autoencoders? Explain briefly.',\n",
       " 'An autoencoder contains three layers:',\n",
       " '\\nEncoder\\nThe encoder is used to compress the input into a latent space representation. It encodes the input images as a compressed representation in a reduced dimension. The compressed images are the distorted version of the original image.\\nCode\\nThe code layer is used to represent the compressed input which is fed to the decoder.\\nDecoder\\nThe decoder layer decodes the encoded image back to its original dimension. The decoded image is a reduced reconstruction of the original image. It is automatically reconstructed from the latent space representation.\\n',\n",
       " '48) What do you understand by Deep Autoencoders?',\n",
       " 'Deep Autoencoder is the extension of the simple Autoencoder. The first layer present in DeepAutoencoder is responsible for first-order functions in the raw input. The second layer is responsible for second-order functions corresponding to patterns in the appearance of first-order functions. Deeper layers which are available in the Deep Autoencoder tend to learn even high-order features.',\n",
       " 'A deep autoencoder is the combination of two, symmetrical deep-belief networks:',\n",
       " '\\nFirst four or five shallow layers represent the encoding half.\\nThe other combination of four or five layers makes up the decoding half.\\n',\n",
       " '49) What are the three steps to developing the necessary assumption structure in Deep learning?',\n",
       " 'The procedure of developing an assumption structure involves three specific actions. ',\n",
       " '\\nThe first step contains algorithm development. This particular process is lengthy.\\nThe second step contains algorithm analyzing, which represents the in-process methodology. \\nThe third step is about implementing the general algorithm in the final procedure. The entire framework is interlinked and required for throughout the process.\\n',\n",
       " '50) What do you understand by Perceptron? Also, explain its type.',\n",
       " 'A perceptron is a neural network unit (an artificial neuron) that does certain computations to detect features. It is an algorithm for supervised learning of binary classifiers. This algorithm is used to enable neurons to learn and processes elements in the training set one at a time.',\n",
       " 'There are two types of perceptrons:',\n",
       " '\\nSingle-Layer Perceptron\\nSingle layer perceptrons can learn only linearly separable patterns.\\nMultilayer Perceptrons\\nMultilayer perceptrons or feedforward neural networks with two or more layers have the higher processing power.\\n']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new5 = lst5[2:129]\n",
    "lst_new5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is deep learning?</td>\n",
       "      <td>Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the main differences between AI, Machine Learning, and Deep Learning?</td>\n",
       "      <td>AI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior. Machine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.  Deep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Differentiate supervised and unsupervised deep learning procedures.</td>\n",
       "      <td>Supervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing. Unsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the applications of deep learning?</td>\n",
       "      <td>There are various applications of deep learning: Computer vision Natural language processing and pattern recognition Image recognition and processing Machine translation Sentiment analysis Question Answering system Object Classification and Detection Automatic Handwriting Generation Automatic Text Generation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do you think that deep network is better than a shallow one?</td>\n",
       "      <td>Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          Questions  \\\n",
       "0                                                            What is deep learning?   \n",
       "1    What are the main differences between AI, Machine Learning, and Deep Learning?   \n",
       "2               Differentiate supervised and unsupervised deep learning procedures.   \n",
       "3                                       What are the applications of deep learning?   \n",
       "4                      Do you think that deep network is better than a shallow one?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Answer  \n",
       "0                                                     Deep learning is a part of machine learning with an algorithm inspired by the structure and function of the brain, which is called an artificial neural network. In the mid-1960s, Alexey Grigorevich Ivakhnenko published the first general, while working on deep learning network. Deep learning is suited over a range of fields such as computer vision, speech recognition, natural language processing, etc.  \n",
       "1                                            AI stands for Artificial Intelligence. It is a technique which enables machines to mimic human behavior. Machine Learning is a subset of AI which uses statistical methods to enable machines to improve with experiences.  Deep learning is a part of Machine learning, which makes the computation of multi-layer neural networks feasible. It takes advantage of neural networks to simulate human-like decision making.   \n",
       "2   Supervised learning is a system in which both input and desired output data are provided. Input and output data are labeled to provide a learning basis for future data processing. Unsupervised procedure does not need labeling information explicitly, and the operations can be carried out without the same. The common unsupervised learning method is cluster analysis. It is used for exploratory data analysis to find hidden patterns or grouping in data.   \n",
       "3                                                                                                                                                 There are various applications of deep learning: Computer vision Natural language processing and pattern recognition Image recognition and processing Machine translation Sentiment analysis Question Answering system Object Classification and Detection Automatic Handwriting Generation Automatic Text Generation.   \n",
       "4                                                                                                        Both shallow and deep networks are good enough and capable of approximating any function. But for the same level of accuracy, deeper networks can be much more efficient in terms of computation and number of parameters. Deeper networks can create deep representations. At every layer, the network learns a new, more abstract representation of the input.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\)[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new5:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"^\\d+\\)\",\" \",ques[i])\n",
    "    \n",
    "df5=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df5[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Lesson 17 of 17By Shivam Arora',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " 'Table of Contents',\n",
       " 'Deep Learning is one of the fastest-growing fields of information technology. It is a set of techniques that permits machines to predict outputs from a layered set of inputs. Deep Learning is being embraced by companies all over the world, and anyone with software and data skills can find numerous job opportunities in this field. A career in data science can be the most satisfying job you ever had.\\xa0',\n",
       " 'And if you want to start a career in deep learning, you will come across various deep learning interviews. Worried? You shouldn’t! Here are some of the most frequently asked deep learning interview questions and answers that might help you crack your next deep learning interview. These deep learning interview questions include a variety of questions - based on expertise level, scenarios and more.',\n",
       " 'Deep Learning Interview Questions and Answers ',\n",
       " 'Check out some of the frequently asked deep learning interview questions below:',\n",
       " '1. What is Deep Learning?',\n",
       " 'If you are going for a deep learning interview, you definitely know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example.\\xa0Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog).',\n",
       " '',\n",
       " '2. What is a Neural Network?',\n",
       " 'Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.',\n",
       " '',\n",
       " 'The most common Neural Networks consist of three network layers:',\n",
       " 'Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.',\n",
       " 'Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course',\n",
       " '3. What Is a Multi-layer Perceptron(MLP)?',\n",
       " 'As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same\\xa0structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.',\n",
       " 'Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called “backpropagation.” In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately).',\n",
       " '4. What Is Data Normalization, and Why Do We Need It?',\n",
       " 'The process of standardizing and reforming data is called “Data Normalization.” It’s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence.',\n",
       " '5. What is the Boltzmann Machine?',\n",
       " 'One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected.',\n",
       " '6. What Is the Role of Activation Functions in a Neural Network?',\n",
       " 'At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions.',\n",
       " '',\n",
       " '7. What Is the Cost Function?',\n",
       " 'Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s performance is. It’s used to compute the error of the output layer during backpropagation. We push that error backward through the neural network and use that during the different training functions.',\n",
       " '',\n",
       " '8. What Is Gradient Descent?',\n",
       " 'Gradient Descent is an optimal algorithm to minimize the cost function or to minimize an error. The aim is to find the local-global minima of a function. This determines the direction the model should take to reduce the error.',\n",
       " '',\n",
       " '9. What Do You Understand by Backpropagation?',\n",
       " 'This is one of the most frequently asked deep learning interview questions. Backpropagation is a technique to improve the performance of the network. It backpropagates the error and updates the weights to reduce the error.',\n",
       " '',\n",
       " '10. What Is the Difference Between a Feedforward Neural Network and Recurrent Neural Network?',\n",
       " 'In this deep learning interview question, the interviewee expects you to give a detailed answer.',\n",
       " 'A Feedforward Neural Network signals travel in one direction from input to output. There are no feedback loops; the network considers only the current input. It cannot memorize previous inputs (e.g., CNN).',\n",
       " 'Deep Learning Course (with TensorFlow & Keras)Master the Deep Learning Concepts and ModelsView Course',\n",
       " 'A Recurrent Neural Network’s signals travel in both directions, creating a looped network. It considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory.',\n",
       " '',\n",
       " '11. What Are the Applications of a Recurrent Neural Network (RNN)?',\n",
       " 'The RNN can be used for sentiment analysis, text mining, and image captioning. Recurrent Neural Networks can also address time series problems such as predicting the prices of stocks in a month or quarter.',\n",
       " '12. What Are the Softmax and ReLU Functions?',\n",
       " 'Softmax is an activation function that generates the output between zero and one. It divides each output, such that the total sum of the outputs is equal to one. Softmax is often used for output layers.',\n",
       " '',\n",
       " 'ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an output of X if X is positive and zeros otherwise. ReLU is often used for hidden layers.',\n",
       " '',\n",
       " '13. What Are Hyperparameters?',\n",
       " 'This is another frequently asked deep learning interview question. With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.).',\n",
       " '',\n",
       " '14. What Will Happen If the Learning Rate Is Set Too Low or Too High?',\n",
       " 'When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point.',\n",
       " 'If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train).',\n",
       " '',\n",
       " '15. What Is Dropout and Batch Normalization?',\n",
       " 'Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping 20 percent of the nodes). It doubles the number of iterations needed to converge the network.',\n",
       " '',\n",
       " 'Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.',\n",
       " '16. What Is the Difference Between Batch Gradient Descent and Stochastic Gradient Descent?',\n",
       " 'Batch Gradient Descent',\n",
       " 'Stochastic Gradient Descent',\n",
       " 'The batch gradient computes the gradient using the entire dataset.',\n",
       " 'It takes time to converge because the volume of data is huge, and weights update slowly.',\n",
       " 'The stochastic gradient computes the gradient using a single sample.',\n",
       " 'It converges much faster than the batch gradient because it updates weight more frequently.',\n",
       " '17. What is Overfitting and Underfitting, and How to Combat Them?',\n",
       " \"Overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. It is more likely to occur with nonlinear models that have more flexibility when learning a target function. An example would be if a model is looking at cars and trucks, but only recognizes trucks that have a specific box shape. It might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. The model performs well on training data, but not in the real world.\",\n",
       " 'Underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. This usually happens when there is less and incorrect data to train a model. Underfitting has both poor performance and accuracy.',\n",
       " 'To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model.',\n",
       " '18. How Are Weights Initialized in a Network?',\n",
       " 'There are two methods here: we can either initialize the weights to zero or assign them randomly.',\n",
       " 'Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless.',\n",
       " 'Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method.',\n",
       " 'Free Deep Learning for Beginners CourseMaster the Basics of Deep LearningEnroll Now',\n",
       " '19. What Are the Different Layers on CNN?',\n",
       " 'There are four layers in CNN:',\n",
       " '20. What is Pooling on CNN, and How Does It Work?',\n",
       " 'Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix.',\n",
       " '',\n",
       " '21. How Does an LSTM Network Work?',\n",
       " 'Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. There are three steps in an LSTM network:',\n",
       " '',\n",
       " '22. What Are Vanishing and Exploding Gradients?',\n",
       " 'While training an RNN, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a “Vanishing Gradient.” When the slope tends to grow exponentially instead of decaying, it’s referred to as an “Exploding Gradient.” Gradient problems lead to long training times, poor performance, and low accuracy.',\n",
       " '',\n",
       " '23. What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning?',\n",
       " '24. Why is Tensorflow the Most Preferred Library in Deep Learning?',\n",
       " 'Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and GPU computing devices.',\n",
       " '25. What Do You Mean by Tensor in Tensorflow?',\n",
       " 'This is another most frequently asked deep learning interview question. A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with different dimensions and ranks fed as input to the neural network are called “Tensors.”',\n",
       " '',\n",
       " '26. What Are the Programming Elements in Tensorflow?',\n",
       " 'Constants - Constants are parameters whose value does not change. To define a constant we use \\xa0tf.constant() command. For example:',\n",
       " 'a = tf.constant(2.0,tf.float32)',\n",
       " 'b = tf.constant(3.0)',\n",
       " 'Print(a, b)',\n",
       " 'Variables - Variables allow us to add new trainable parameters to graph. To define a variable, we use the tf.Variable() command and initialize them before running the graph in a session. An example:',\n",
       " 'W = tf.Variable([.3].dtype=tf.float32)',\n",
       " 'b = tf.Variable([-.3].dtype=tf.float32)',\n",
       " 'Placeholders - these allow us to feed data to a tensorflow model from outside a model. It permits a value to be assigned later. To define a placeholder, we use the tf.placeholder() command. An example:',\n",
       " 'a = tf.placeholder (tf.float32)',\n",
       " 'b = a*2',\n",
       " 'with tf.Session() as sess:',\n",
       " 'result = sess.run(b,feed_dict={a:3.0})',\n",
       " 'print result',\n",
       " 'Sessions - a session is run to evaluate the nodes. This is called the “Tensorflow runtime.” For example:',\n",
       " 'a = tf.constant(2.0)',\n",
       " 'b = tf.constant(4.0)',\n",
       " 'c = a+b',\n",
       " '# Launch Session',\n",
       " 'Sess = tf.Session()',\n",
       " '# Evaluate the tensor c',\n",
       " 'print(sess.run(c))',\n",
       " 'FREE Machine Learning CourseLearn In-demand Machine Learning Skills and ToolsStart Learning',\n",
       " '27. Explain a Computational Graph.',\n",
       " 'Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a “DataFlow Graph.”',\n",
       " '28. Explain Generative Adversarial Network.',\n",
       " 'Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.',\n",
       " 'The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner’s check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic.',\n",
       " 'The forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.',\n",
       " '',\n",
       " 'Let us understand this example with the help of an image shown above.',\n",
       " 'There is a noise vector coming into the forger who is generating fake wine.',\n",
       " 'Here the forger acts as a Generator.',\n",
       " 'The shop owner acts as a Discriminator.',\n",
       " 'The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake.',\n",
       " 'So, there are two primary components of Generative Adversarial Network (GAN) named:',\n",
       " 'The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images.',\n",
       " '29. What Is an Auto-encoder?',\n",
       " '',\n",
       " \"This Neural Network has three layers in which the input neurons are equal to the output neurons. The network's target outside is the same as the input. It uses dimensionality reduction to restructure the input. It works by compressing the image input to a latent space representation then reconstructing the output from this representation.\",\n",
       " '30. What Is Bagging and Boosting?',\n",
       " 'Bagging and Boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call.',\n",
       " '',\n",
       " 'With Bagging, we take a dataset and split it into training data and test data. Then we randomly select data to place into the bags and train the model separately.',\n",
       " '',\n",
       " 'With Boosting, the emphasis is on selecting data points which give wrong output to improve the accuracy.',\n",
       " 'With this we come to an end of the top 30 deep learning interview questions and answers article, that will help you land the perfect job that you always desired. Do you feel unprepared regarding the concepts covered in these deep learning interview questions? Then Simplilearn is here to help you upskill yourself. We offer Deep Learning with TensorFlow Certification courses that will assist you in gaining expertise in all the concepts of Deep Learning, and also add a shining star to your resume. Start learning now!',\n",
       " '\\n',\n",
       " 'Find our Deep Learning with Keras and TensorFlow Online Classroom training classes in top cities:',\n",
       " 'About the Author',\n",
       " 'Shivam Arora is a Senior Product Manager at Simplilearn. Passionate about driving product growth, Shivam has managed key AI and IOT based products across different business functions. He has 6+ years of product experience with a Masters in Marketing and Business Analytics.',\n",
       " 'Recommended Programs',\n",
       " 'Deep Learning with Keras and TensorFlow ',\n",
       " 'Artificial Intelligence Engineer ',\n",
       " '*Lifetime access to high-quality, self-paced e-learning content.',\n",
       " 'Recommended Resources',\n",
       " 'What Is Keras? The Best Introductory Guide to Keras',\n",
       " 'Deep Learning Interview Guide',\n",
       " 'Keras vs Tensorflow vs Pytorch: Understanding the Most Popular Deep Learning Frameworks',\n",
       " 'The Best Introduction to Deep Learning - A Step by Step Guide',\n",
       " 'What Is TensorFlow 2.0? The Best Guide to Understand TensorFlow',\n",
       " \"Introduction to Machine Learning: A Beginner's Guide\",\n",
       " '© 2009 -2021- Simplilearn Solutions',\n",
       " 'Follow us!',\n",
       " 'Company',\n",
       " 'Work with us',\n",
       " 'Discover',\n",
       " 'For Businesses',\n",
       " 'Learn On the Go!',\n",
       " 'Trending Post Graduate Programs',\n",
       " 'Trending Master Programs',\n",
       " 'Trending Courses',\n",
       " 'Trending Resources']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst6 = []\n",
    "url = \"https://www.simplilearn.com/tutorials/deep-learning-tutorial/deep-learning-interview-questions\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h2','p'])\n",
    "for answer in answers:\n",
    "    lst6.append(answer.get_text())\n",
    "lst6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is Deep Learning?',\n",
       " 'If you are going for a deep learning interview, you definitely know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example.\\xa0Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog).',\n",
       " '',\n",
       " '2. What is a Neural Network?',\n",
       " 'Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.',\n",
       " '',\n",
       " 'The most common Neural Networks consist of three network layers:',\n",
       " 'Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.',\n",
       " 'Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course',\n",
       " '3. What Is a Multi-layer Perceptron(MLP)?',\n",
       " 'As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same\\xa0structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.',\n",
       " 'Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called “backpropagation.” In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately).',\n",
       " '4. What Is Data Normalization, and Why Do We Need It?',\n",
       " 'The process of standardizing and reforming data is called “Data Normalization.” It’s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence.',\n",
       " '5. What is the Boltzmann Machine?',\n",
       " 'One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected.',\n",
       " '6. What Is the Role of Activation Functions in a Neural Network?',\n",
       " 'At the most basic level, an activation function decides whether a neuron should be fired or not. It accepts the weighted sum of the inputs and bias as input to any activation function. Step function, Sigmoid, ReLU, Tanh, and Softmax are examples of activation functions.',\n",
       " '',\n",
       " '7. What Is the Cost Function?',\n",
       " 'Also referred to as “loss” or “error,” cost function is a measure to evaluate how good your model’s performance is. It’s used to compute the error of the output layer during backpropagation. We push that error backward through the neural network and use that during the different training functions.',\n",
       " '',\n",
       " '8. What Is Gradient Descent?',\n",
       " 'Gradient Descent is an optimal algorithm to minimize the cost function or to minimize an error. The aim is to find the local-global minima of a function. This determines the direction the model should take to reduce the error.',\n",
       " '',\n",
       " '9. What Do You Understand by Backpropagation?',\n",
       " 'This is one of the most frequently asked deep learning interview questions. Backpropagation is a technique to improve the performance of the network. It backpropagates the error and updates the weights to reduce the error.',\n",
       " '',\n",
       " '10. What Is the Difference Between a Feedforward Neural Network and Recurrent Neural Network?',\n",
       " 'In this deep learning interview question, the interviewee expects you to give a detailed answer.',\n",
       " 'A Feedforward Neural Network signals travel in one direction from input to output. There are no feedback loops; the network considers only the current input. It cannot memorize previous inputs (e.g., CNN).',\n",
       " 'Deep Learning Course (with TensorFlow & Keras)Master the Deep Learning Concepts and ModelsView Course',\n",
       " 'A Recurrent Neural Network’s signals travel in both directions, creating a looped network. It considers the current input with the previously received inputs for generating the output of a layer and can memorize past data due to its internal memory.',\n",
       " '',\n",
       " '11. What Are the Applications of a Recurrent Neural Network (RNN)?',\n",
       " 'The RNN can be used for sentiment analysis, text mining, and image captioning. Recurrent Neural Networks can also address time series problems such as predicting the prices of stocks in a month or quarter.',\n",
       " '12. What Are the Softmax and ReLU Functions?',\n",
       " 'Softmax is an activation function that generates the output between zero and one. It divides each output, such that the total sum of the outputs is equal to one. Softmax is often used for output layers.',\n",
       " '',\n",
       " 'ReLU (or Rectified Linear Unit) is the most widely used activation function. It gives an output of X if X is positive and zeros otherwise. ReLU is often used for hidden layers.',\n",
       " '',\n",
       " '13. What Are Hyperparameters?',\n",
       " 'This is another frequently asked deep learning interview question. With neural networks, you’re usually working with hyperparameters once the data is formatted correctly. A hyperparameter is a parameter whose value is set before the learning process begins. It determines how a network is trained and the structure of the network (such as the number of hidden units, the learning rate, epochs, etc.).',\n",
       " '',\n",
       " '14. What Will Happen If the Learning Rate Is Set Too Low or Too High?',\n",
       " 'When your learning rate is too low, training of the model will progress very slowly as we are making minimal updates to the weights. It will take many updates before reaching the minimum point.',\n",
       " 'If the learning rate is set too high, this causes undesirable divergent behavior to the loss function due to drastic updates in weights. It may fail to converge (model can give a good output) or even diverge (data is too chaotic for the network to train).',\n",
       " '',\n",
       " '15. What Is Dropout and Batch Normalization?',\n",
       " 'Dropout is a technique of dropping out hidden and visible units of a network randomly to prevent overfitting of data (typically dropping 20 percent of the nodes). It doubles the number of iterations needed to converge the network.',\n",
       " '',\n",
       " 'Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.',\n",
       " '16. What Is the Difference Between Batch Gradient Descent and Stochastic Gradient Descent?',\n",
       " 'Batch Gradient Descent',\n",
       " 'Stochastic Gradient Descent',\n",
       " 'The batch gradient computes the gradient using the entire dataset.',\n",
       " 'It takes time to converge because the volume of data is huge, and weights update slowly.',\n",
       " 'The stochastic gradient computes the gradient using a single sample.',\n",
       " 'It converges much faster than the batch gradient because it updates weight more frequently.',\n",
       " '17. What is Overfitting and Underfitting, and How to Combat Them?',\n",
       " \"Overfitting occurs when the model learns the details and noise in the training data to the degree that it adversely impacts the execution of the model on new information. It is more likely to occur with nonlinear models that have more flexibility when learning a target function. An example would be if a model is looking at cars and trucks, but only recognizes trucks that have a specific box shape. It might not be able to notice a flatbed truck because there's only a particular kind of truck it saw in training. The model performs well on training data, but not in the real world.\",\n",
       " 'Underfitting alludes to a model that is neither well-trained on data nor can generalize to new information. This usually happens when there is less and incorrect data to train a model. Underfitting has both poor performance and accuracy.',\n",
       " 'To combat overfitting and underfitting, you can resample the data to estimate the model accuracy (k-fold cross-validation) and by having a validation dataset to evaluate the model.',\n",
       " '18. How Are Weights Initialized in a Network?',\n",
       " 'There are two methods here: we can either initialize the weights to zero or assign them randomly.',\n",
       " 'Initializing all weights to 0: This makes your model similar to a linear model. All the neurons and every layer perform the same operation, giving the same output and making the deep net useless.',\n",
       " 'Initializing all weights randomly: Here, the weights are assigned randomly by initializing them very close to 0. It gives better accuracy to the model since every neuron performs different computations. This is the most commonly used method.',\n",
       " 'Free Deep Learning for Beginners CourseMaster the Basics of Deep LearningEnroll Now',\n",
       " '19. What Are the Different Layers on CNN?',\n",
       " 'There are four layers in CNN:',\n",
       " '20. What is Pooling on CNN, and How Does It Work?',\n",
       " 'Pooling is used to reduce the spatial dimensions of a CNN. It performs down-sampling operations to reduce the dimensionality and creates a pooled feature map by sliding a filter matrix over the input matrix.',\n",
       " '',\n",
       " '21. How Does an LSTM Network Work?',\n",
       " 'Long-Short-Term Memory (LSTM) is a special kind of recurrent neural network capable of learning long-term dependencies, remembering information for long periods as its default behavior. There are three steps in an LSTM network:',\n",
       " '',\n",
       " '22. What Are Vanishing and Exploding Gradients?',\n",
       " 'While training an RNN, your slope can become either too small or too large; this makes the training difficult. When the slope is too small, the problem is known as a “Vanishing Gradient.” When the slope tends to grow exponentially instead of decaying, it’s referred to as an “Exploding Gradient.” Gradient problems lead to long training times, poor performance, and low accuracy.',\n",
       " '',\n",
       " '23. What Is the Difference Between Epoch, Batch, and Iteration in Deep Learning?',\n",
       " '24. Why is Tensorflow the Most Preferred Library in Deep Learning?',\n",
       " 'Tensorflow provides both C++ and Python APIs, making it easier to work on and has a faster compilation time compared to other Deep Learning libraries like Keras and Torch. Tensorflow supports both CPU and GPU computing devices.',\n",
       " '25. What Do You Mean by Tensor in Tensorflow?',\n",
       " 'This is another most frequently asked deep learning interview question. A tensor is a mathematical object represented as arrays of higher dimensions. These arrays of data with different dimensions and ranks fed as input to the neural network are called “Tensors.”',\n",
       " '',\n",
       " '26. What Are the Programming Elements in Tensorflow?',\n",
       " 'Constants - Constants are parameters whose value does not change. To define a constant we use \\xa0tf.constant() command. For example:',\n",
       " 'a = tf.constant(2.0,tf.float32)',\n",
       " 'b = tf.constant(3.0)',\n",
       " 'Print(a, b)',\n",
       " 'Variables - Variables allow us to add new trainable parameters to graph. To define a variable, we use the tf.Variable() command and initialize them before running the graph in a session. An example:',\n",
       " 'W = tf.Variable([.3].dtype=tf.float32)',\n",
       " 'b = tf.Variable([-.3].dtype=tf.float32)',\n",
       " 'Placeholders - these allow us to feed data to a tensorflow model from outside a model. It permits a value to be assigned later. To define a placeholder, we use the tf.placeholder() command. An example:',\n",
       " 'a = tf.placeholder (tf.float32)',\n",
       " 'b = a*2',\n",
       " 'with tf.Session() as sess:',\n",
       " 'result = sess.run(b,feed_dict={a:3.0})',\n",
       " 'print result',\n",
       " 'Sessions - a session is run to evaluate the nodes. This is called the “Tensorflow runtime.” For example:',\n",
       " 'a = tf.constant(2.0)',\n",
       " 'b = tf.constant(4.0)',\n",
       " 'c = a+b',\n",
       " '# Launch Session',\n",
       " 'Sess = tf.Session()',\n",
       " '# Evaluate the tensor c',\n",
       " 'print(sess.run(c))',\n",
       " 'FREE Machine Learning CourseLearn In-demand Machine Learning Skills and ToolsStart Learning',\n",
       " '27. Explain a Computational Graph.',\n",
       " 'Everything in a tensorflow is based on creating a computational graph. It has a network of nodes where each node operates, Nodes represent mathematical operations, and edges represent tensors. Since data flows in the form of a graph, it is also called a “DataFlow Graph.”',\n",
       " '28. Explain Generative Adversarial Network.',\n",
       " 'Suppose there is a wine shop purchasing wine from dealers, which they resell later. But some dealers sell fake wine. In this case, the shop owner should be able to distinguish between fake and authentic wine.',\n",
       " 'The forger will try different techniques to sell fake wine and make sure specific techniques go past the shop owner’s check. The shop owner would probably get some feedback from wine experts that some of the wine is not original. The owner would have to improve how he determines whether a wine is fake or authentic.',\n",
       " 'The forger’s goal is to create wines that are indistinguishable from the authentic ones while the shop owner intends to tell if the wine is real or not accurately.',\n",
       " '',\n",
       " 'Let us understand this example with the help of an image shown above.',\n",
       " 'There is a noise vector coming into the forger who is generating fake wine.',\n",
       " 'Here the forger acts as a Generator.',\n",
       " 'The shop owner acts as a Discriminator.',\n",
       " 'The Discriminator gets two inputs; one is the fake wine, while the other is the real authentic wine. The shop owner has to figure out whether it is real or fake.',\n",
       " 'So, there are two primary components of Generative Adversarial Network (GAN) named:',\n",
       " 'The generator is a CNN that keeps keys producing images and is closer in appearance to the real images while the discriminator tries to determine the difference between real and fake images The ultimate aim is to make the discriminator learn to identify real and fake images.',\n",
       " '29. What Is an Auto-encoder?',\n",
       " '',\n",
       " \"This Neural Network has three layers in which the input neurons are equal to the output neurons. The network's target outside is the same as the input. It uses dimensionality reduction to restructure the input. It works by compressing the image input to a latent space representation then reconstructing the output from this representation.\",\n",
       " '30. What Is Bagging and Boosting?',\n",
       " 'Bagging and Boosting are ensemble techniques to train multiple models using the same learning algorithm and then taking a call.',\n",
       " '',\n",
       " 'With Bagging, we take a dataset and split it into training data and test data. Then we randomly select data to place into the bags and train the model separately.',\n",
       " '',\n",
       " 'With Boosting, the emphasis is on selecting data points which give wrong output to improve the accuracy.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new6 = lst6[42:173]\n",
    "lst_new6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Deep Learning?</td>\n",
       "      <td>If you are going for a deep learning interview, you definitely know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example. Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a Neural Network?</td>\n",
       "      <td>Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.The most common Neural Networks consist of three network layers:Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Is a Multi-layer Perceptron(MLP)?</td>\n",
       "      <td>As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called “backpropagation.” In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What Is Data Normalization, and Why Do We Need It?</td>\n",
       "      <td>The process of standardizing and reforming data is called “Data Normalization.” It’s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Boltzmann Machine?</td>\n",
       "      <td>One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Questions  \\\n",
       "0                                What is Deep Learning?   \n",
       "1                             What is a Neural Network?   \n",
       "2                What Is a Multi-layer Perceptron(MLP)?   \n",
       "3    What Is Data Normalization, and Why Do We Need It?   \n",
       "4                        What is the Boltzmann Machine?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                 If you are going for a deep learning interview, you definitely know what exactly deep learning is. However, with this question the interviewee expects you to give an in-detail answer, with an example. Deep Learning involves taking large volumes of structured or unstructured data and using complex algorithms to train neural networks. It performs complex operations to extract hidden patterns and features (for instance, distinguishing the image of a cat from that of a dog).  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                               Neural Networks replicate the way humans learn, inspired by how the neurons in our brains fire, only much simpler.The most common Neural Networks consist of three network layers:Each sheet contains neurons called “nodes,” performing various operations. Neural Networks are used in deep learning algorithms like CNN, RNN, GAN, etc.Post Graduate Program in AI and Machine LearningIn Partnership with Purdue UniversityExplore Course  \n",
       "2  As in Neural Networks, MLPs have an input layer, a hidden layer, and an output layer. It has the same structure as a single layer perceptron with one or more hidden layers. A single layer perceptron can classify only linear separable classes with binary output (0,1), but MLP can classify nonlinear classes.Except for the input layer, each node in the other layers uses a nonlinear activation function. This means the input layers, the data coming in, and the activation function is based upon all nodes and weights being added together, producing the output. MLP uses a supervised learning method called “backpropagation.” In backpropagation, the neural network calculates the error with the help of cost function. It propagates this error backward from where it came (adjusts the weights to train the model more accurately).  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The process of standardizing and reforming data is called “Data Normalization.” It’s a pre-processing step to eliminate data redundancy. Often, data comes in, and you get the same information in different formats. In these cases, you should rescale values to fit into a particular range, achieving better convergence.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                              One of the most basic Deep Learning models is a Boltzmann Machine, resembling a simplified version of the Multi-Layer Perceptron. This model features a visible input layer and a hidden layer -- just a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. Nodes are connected across layers, but no two nodes of the same layer are connected.  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new6:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df6=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df6[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=df6.drop(df6.index[[22]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df6[5:]\n",
    "# df6.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n \\n\\n',\n",
       " 'Deep Learning is the closest concept that has helped machines become autonomous. Being a top career for the decade, Deep Learning has caused a lot of stir in the market as it has created thousands of jobs. Every company out there makes use of Artificial Intelligence in one way or another. This demand calls for proficient developers in Deep Learning and understanding the top Deep Learning Interview questions and answers can help in acing the interviews.',\n",
       " 'Categories',\n",
       " ' \\nAutomation\\n3\\n\\nTop RPA (Robotic Process Automation) Interview Questions and AnswersTop 50 UiPath Interview Questions and Answers in 2021Top 50 Automation Anywhere Interview Questions and Answers in 2021 \\n\\n\\nBig Data\\n12\\n\\nTop Splunk Interview Questions and AnswersTop Hadoop Interview Questions and AnswersTop Apache Solr Interview Questions And AnswersTop Apache Storm Interview Questions And AnswersTop 40 Apache Spark Interview Questions and Answers in 2021Top Mapreduce Interview Questions And AnswersTop HDFS Interview Questions And AnswersTop Kafka Interview Questions – Most AskedTop Couchbase Interview Questions - Most AskedTop PIG Interview Questions - Most AskedTop Hive Interview Questions – Most AskedTop Sqoop Interview Questions – Most Asked \\n\\n\\nBusiness Intelligence\\n21\\n\\nTop SAS Interview Questions and AnswersTop Obiee Interview Questions And AnswersTop Pentaho Interview Questions And AnswersTop MSBI Interview Questions and AnswersTop QlikView Interview Questions and AnswersTop 65 Tableau Interview Questions and Answers in 2021Top Data Warehousing Interview Questions and AnswersTop Microstrategy Interview Questions And AnswersTop Cognos Interview Questions And AnswersTop Cognos TM1 Interview Questions And AnswersTop 60 Talend Interview Questions with Answers 2021Top 40 DataStage Interview Questions and AnswersTop Informatica Interview Questions and AnswersTop Spotfire Interview Questions And AnswersTop Jaspersoft Interview Questions And AnswersTop ETL Interview Questions and AnswersTop Hyperion Interview Questions And AnswersTop Ireport Interview Questions And AnswersTop Qliksense Interview Questions - Most AskedTop 40 Power BI Interview Questions and Answers in 2021Top 35 Business Analyst Interview Questions and Answers \\n\\n\\nCloud Computing\\n17\\n\\nTop Openstack Interview Questions And AnswersTop SharePoint Interview Questions and AnswersTop Amazon AWS Interview Questions - Most AskedTop 60 DevOps Interview Questions and Answers in 2021Top Cloud Computing Interview Questions – Most AskedTop 53 Blockchain Interview Questions And AnswersTop 50 Microsoft Azure Interview Questions And AnswersTop Chef Interview Questions And AnswersTop Docker Interview Questions and AnswersTop Git Interview Questions And AnswersTop Jenkins Interview Questions and AnswersTop Kubernetes Interview Questions and AnswersTop Puppet Interview Questions And AnswersTop 30 GCP Interview Questions and AnswersAzure Data Factory Interview QuestionsTop 50 AWS DevOps Interview QuestionsTop 30 Azure DevOps Interview Questions and Answers \\n\\n\\nCyber Security\\n2\\n\\nTop Ethical Hacking Interview Questions And AnswersTop 50 Cyber Security Interview Questions and Answers \\n\\n\\nData Science\\n11\\n\\nTop R Interview Questions And AnswersTop 78 Data Science Interview Questions and Answers for 2021Top Mahout Interview Questions And AnswersTop 74 Artificial Intelligence Interview Questions and AnswersTop 40 Machine Learning Interview Questions and AnswersTop 50 Data Analyst Interview Questions and Answers for 2021Top 50 Data Engineer Interview Questions and AnswersTop 30 NLP Interview Questions and AnswersTop 50 Deep Learning and Machine Learning Interview QuestionsTop 50 TensorFlow Interview QuestionsTop 75 Statistics Interview Questions \\n\\n\\nDatabase\\n6\\n\\nTop 50 SQL Interview Questions and Answers in 2021Top 55 Oracle DBA Interview Questions and Answers 2021Top 60 PL/SQL Interview Questions and AnswersTop DB2 Interview Questions and AnswersTop MySQL Interview Questions and AnswersTop SQL Server Interview Questions and Answers \\n\\n\\nDigital Marketing\\n2\\n\\nTop 60 Digital Marketing Interview Questions and Answers in 2021Top SEO Interview Questions and Answers in 2021 \\n\\n\\nMobile Development\\n2\\n\\nTop iOS Interview Questions and AnswersTop Android Interview Questions and Answers \\n\\n\\nNo-SQL\\n5\\n\\nTop MongoDB Interview Questions and AnswersTop HBase Interview Questions And AnswersTop Cassandra Interview Questions and AnswersTop NoSQL Interview Questions And AnswersTop Couchdb Interview Questions And Answers \\n\\n\\nProgramming\\n16\\n\\nTop 100 Python Interview Questions and Answers in 2021Top 100+ Java Interview Questions and AnswersTop 64 PHP Interview QuestionsTop 50 Linux Interview Questions and AnswersTop C & Data Structure Interview Questions And AnswersTop SOA Interview Questions And AnswersTop JBPM Interview Questions and Answers in 2021Top Drools Interview Questions And AnswersTop Junit Interview Questions And AnswersTop Spring Interview Questions and AnswersTop HTML Interview Questions - Most AskedTop Django Interview Questions and AnswersTop 50 Data Structures Interview QuestionsTop 50 Node.js Interview QuestionsTop Java 8 Interview QuestionsTop .NET Interview Questions \\n\\n\\nProject Management\\n5\\n\\nTop Agile Scrum Master Interview Questions and AnswersTop ITIL Interview Questions and AnswersTop Prince2 Interview Questions And AnswersTop Togaf Interview Questions - Most AskedTop Project Management Interview Questions And Answers \\n\\n\\nSalesforce\\n2\\n\\nTop 60 Salesforce Interview Questions and Answers in 2021Top 50 Salesforce Admin Interview Questions and Answers \\n\\n\\nTesting\\n4\\n\\nTop 50 Selenium Interview Questions and Answers in 2021Top Software Testing Interview Questions And AnswersTop ETL Testing Interview Questions and AnswersTop Manual Testing Interview Questions and Answers \\n\\n\\nWebsite Development\\n5\\n\\nTop CSS Interview Questions And AnswersTop Jquery Interview Questions And AnswersTop React Interview QuestionsTop 50 Web Development Interview QuestionsTop 50 Angular Interview Questions and Answers 2021 \\n\\n',\n",
       " 'Automation',\n",
       " 'Top RPA (Robotic Process Automation) Interview Questions and AnswersTop 50 UiPath Interview Questions and Answers in 2021Top 50 Automation Anywhere Interview Questions and Answers in 2021 ',\n",
       " 'Big Data',\n",
       " 'Top Splunk Interview Questions and AnswersTop Hadoop Interview Questions and AnswersTop Apache Solr Interview Questions And AnswersTop Apache Storm Interview Questions And AnswersTop 40 Apache Spark Interview Questions and Answers in 2021Top Mapreduce Interview Questions And AnswersTop HDFS Interview Questions And AnswersTop Kafka Interview Questions – Most AskedTop Couchbase Interview Questions - Most AskedTop PIG Interview Questions - Most AskedTop Hive Interview Questions – Most AskedTop Sqoop Interview Questions – Most Asked ',\n",
       " 'Business Intelligence',\n",
       " 'Top SAS Interview Questions and AnswersTop Obiee Interview Questions And AnswersTop Pentaho Interview Questions And AnswersTop MSBI Interview Questions and AnswersTop QlikView Interview Questions and AnswersTop 65 Tableau Interview Questions and Answers in 2021Top Data Warehousing Interview Questions and AnswersTop Microstrategy Interview Questions And AnswersTop Cognos Interview Questions And AnswersTop Cognos TM1 Interview Questions And AnswersTop 60 Talend Interview Questions with Answers 2021Top 40 DataStage Interview Questions and AnswersTop Informatica Interview Questions and AnswersTop Spotfire Interview Questions And AnswersTop Jaspersoft Interview Questions And AnswersTop ETL Interview Questions and AnswersTop Hyperion Interview Questions And AnswersTop Ireport Interview Questions And AnswersTop Qliksense Interview Questions - Most AskedTop 40 Power BI Interview Questions and Answers in 2021Top 35 Business Analyst Interview Questions and Answers ',\n",
       " 'Cloud Computing',\n",
       " 'Top Openstack Interview Questions And AnswersTop SharePoint Interview Questions and AnswersTop Amazon AWS Interview Questions - Most AskedTop 60 DevOps Interview Questions and Answers in 2021Top Cloud Computing Interview Questions – Most AskedTop 53 Blockchain Interview Questions And AnswersTop 50 Microsoft Azure Interview Questions And AnswersTop Chef Interview Questions And AnswersTop Docker Interview Questions and AnswersTop Git Interview Questions And AnswersTop Jenkins Interview Questions and AnswersTop Kubernetes Interview Questions and AnswersTop Puppet Interview Questions And AnswersTop 30 GCP Interview Questions and AnswersAzure Data Factory Interview QuestionsTop 50 AWS DevOps Interview QuestionsTop 30 Azure DevOps Interview Questions and Answers ',\n",
       " 'Cyber Security',\n",
       " 'Top Ethical Hacking Interview Questions And AnswersTop 50 Cyber Security Interview Questions and Answers ',\n",
       " 'Data Science',\n",
       " 'Top R Interview Questions And AnswersTop 78 Data Science Interview Questions and Answers for 2021Top Mahout Interview Questions And AnswersTop 74 Artificial Intelligence Interview Questions and AnswersTop 40 Machine Learning Interview Questions and AnswersTop 50 Data Analyst Interview Questions and Answers for 2021Top 50 Data Engineer Interview Questions and AnswersTop 30 NLP Interview Questions and AnswersTop 50 Deep Learning and Machine Learning Interview QuestionsTop 50 TensorFlow Interview QuestionsTop 75 Statistics Interview Questions ',\n",
       " 'Database',\n",
       " 'Top 50 SQL Interview Questions and Answers in 2021Top 55 Oracle DBA Interview Questions and Answers 2021Top 60 PL/SQL Interview Questions and AnswersTop DB2 Interview Questions and AnswersTop MySQL Interview Questions and AnswersTop SQL Server Interview Questions and Answers ',\n",
       " 'Digital Marketing',\n",
       " 'Top 60 Digital Marketing Interview Questions and Answers in 2021Top SEO Interview Questions and Answers in 2021 ',\n",
       " 'Mobile Development',\n",
       " 'Top iOS Interview Questions and AnswersTop Android Interview Questions and Answers ',\n",
       " 'No-SQL',\n",
       " 'Top MongoDB Interview Questions and AnswersTop HBase Interview Questions And AnswersTop Cassandra Interview Questions and AnswersTop NoSQL Interview Questions And AnswersTop Couchdb Interview Questions And Answers ',\n",
       " 'Programming',\n",
       " 'Top 100 Python Interview Questions and Answers in 2021Top 100+ Java Interview Questions and AnswersTop 64 PHP Interview QuestionsTop 50 Linux Interview Questions and AnswersTop C & Data Structure Interview Questions And AnswersTop SOA Interview Questions And AnswersTop JBPM Interview Questions and Answers in 2021Top Drools Interview Questions And AnswersTop Junit Interview Questions And AnswersTop Spring Interview Questions and AnswersTop HTML Interview Questions - Most AskedTop Django Interview Questions and AnswersTop 50 Data Structures Interview QuestionsTop 50 Node.js Interview QuestionsTop Java 8 Interview QuestionsTop .NET Interview Questions ',\n",
       " 'Project Management',\n",
       " 'Top Agile Scrum Master Interview Questions and AnswersTop ITIL Interview Questions and AnswersTop Prince2 Interview Questions And AnswersTop Togaf Interview Questions - Most AskedTop Project Management Interview Questions And Answers ',\n",
       " 'Salesforce',\n",
       " 'Top 60 Salesforce Interview Questions and Answers in 2021Top 50 Salesforce Admin Interview Questions and Answers ',\n",
       " 'Testing',\n",
       " 'Top 50 Selenium Interview Questions and Answers in 2021Top Software Testing Interview Questions And AnswersTop ETL Testing Interview Questions and AnswersTop Manual Testing Interview Questions and Answers ',\n",
       " 'Website Development',\n",
       " 'Top CSS Interview Questions And AnswersTop Jquery Interview Questions And AnswersTop React Interview QuestionsTop 50 Web Development Interview QuestionsTop 50 Angular Interview Questions and Answers 2021 ',\n",
       " 'CTA',\n",
       " 'Deep Learning is a well-covered skill to possess in the 21st century. Working with it requires a lot of effort and this is seen in the interviews as well. The questions can sometimes get a bit tough.\\nThis ‘Top Deep Learning Interview Questions’ blog is put together with questions sourced from experts in the field, which have the highest probability of occurrence in interviews. Studying these questions will help you ace your next Deep Learning interview.\\nQ1. What is the difference between Machine Learning and Deep Learning?\\nQ2. What is a perceptron?\\nQ3. How is Deep Learning better than Machine Learning?\\nQ4. What are some of the most used applications of Deep Learning?\\nQ5. What is the meaning of overfitting?\\nQ6. What are activation functions?\\nQ7. Why is Fourier transform used in Deep Learning?\\nQ8. What are the steps involved in training a perceptron in Deep Learning?\\nQ9. What is the use of the loss function?\\nQ10. What are some of the Deep Learning frameworks or tools that you have used?\\nThis Top Deep Learning Interview Questions blog is divided into three parts:\\n1. Basic\\n2. Intermediate\\n3. Advanced\\nCheck out our Deep Learning Interview Questions And Answers on YouTube, designed especially for beginners:\\n Top 50 Deep Learning and Machine Learning Interview Questions Top 50 Deep Learning and Machine Learning Interview Questions\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBasic Interview Questions\\n\\n',\n",
       " 'Deep Learning is a well-covered skill to possess in the 21st century. Working with it requires a lot of effort and this is seen in the interviews as well. The questions can sometimes get a bit tough.',\n",
       " 'This ‘Top Deep Learning Interview Questions’ blog is put together with questions sourced from experts in the field, which have the highest probability of occurrence in interviews. Studying these questions will help you ace your next Deep Learning interview.',\n",
       " 'Q1. What is the difference between Machine Learning and Deep Learning?\\nQ2. What is a perceptron?\\nQ3. How is Deep Learning better than Machine Learning?\\nQ4. What are some of the most used applications of Deep Learning?\\nQ5. What is the meaning of overfitting?\\nQ6. What are activation functions?\\nQ7. Why is Fourier transform used in Deep Learning?\\nQ8. What are the steps involved in training a perceptron in Deep Learning?\\nQ9. What is the use of the loss function?\\nQ10. What are some of the Deep Learning frameworks or tools that you have used?',\n",
       " 'This Top Deep Learning Interview Questions blog is divided into three parts:',\n",
       " '1. Basic',\n",
       " '2. Intermediate',\n",
       " '3. Advanced',\n",
       " 'Check out our Deep Learning Interview Questions And Answers on YouTube, designed especially for beginners:',\n",
       " ' Top 50 Deep Learning and Machine Learning Interview Questions Top 50 Deep Learning and Machine Learning Interview Questions\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '1. What is the difference between Machine Learning and Deep Learning?',\n",
       " 'Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.\\nDeep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.\\n\\n',\n",
       " 'Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.',\n",
       " 'Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.',\n",
       " '2. What is a perceptron?',\n",
       " 'A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.\\nA perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.\\n\\n',\n",
       " 'A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.',\n",
       " 'A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.',\n",
       " '3. How is Deep Learning better than Machine Learning?',\n",
       " 'Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.\\n\\n',\n",
       " 'Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.',\n",
       " '4. What are some of the most used applications of Deep Learning?',\n",
       " 'Deep Learning is used in a variety of fields today. The most used ones are as follows:\\n\\nSentiment Analysis\\nComputer Vision\\nAutomatic Text Generation\\nObject Detection\\nNatural Language Processing\\nImage Recognition\\n\\n\\n',\n",
       " 'Deep Learning is used in a variety of fields today. The most used ones are as follows:',\n",
       " '\\nSentiment Analysis\\nComputer Vision\\nAutomatic Text Generation\\nObject Detection\\nNatural Language Processing\\nImage Recognition\\n',\n",
       " '5. What is the meaning of overfitting?',\n",
       " 'Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.\\nThis makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.\\n\\n',\n",
       " 'Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.',\n",
       " 'This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.',\n",
       " '6. What are activation functions?',\n",
       " 'Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias.\\nUsing an activation function makes the model output to be non-linear. There are many types of activation functions:\\n\\nReLU\\nSoftmax\\nSigmoid\\nLinear\\nTanh\\n\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias.',\n",
       " 'Using an activation function makes the model output to be non-linear. There are many types of activation functions:',\n",
       " '\\nReLU\\nSoftmax\\nSigmoid\\nLinear\\nTanh\\n',\n",
       " 'Get 50% Hike!',\n",
       " 'Master Most in Demand Skills Now !',\n",
       " '7. Why is Fourier transform used in Deep Learning?',\n",
       " 'Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.\\n\\n',\n",
       " 'Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.',\n",
       " '8. What are the steps involved in training a perception in Deep Learning?',\n",
       " 'There are five main steps that determine the learning of a perceptron:\\n\\nInitialize thresholds and weights\\nProvide inputs\\nCalculate outputs\\nUpdate weights in each step\\nRepeat steps 2 to 4\\n\\n\\n',\n",
       " 'There are five main steps that determine the learning of a perceptron:',\n",
       " '9. What is the use of the loss function?',\n",
       " 'The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset.\\nThe loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.\\n\\n',\n",
       " 'The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset.',\n",
       " 'The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.',\n",
       " '10. What are some of the Deep Learning frameworks or tools that you have used?',\n",
       " 'This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools.\\nHowever, some of the top Deep Learning frameworks out there today are:\\n\\nTensorFlow\\nKeras\\nPyTorch\\nCaffe2\\nCNTK\\nMXNet\\nTheano\\n\\n',\n",
       " 'This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools.',\n",
       " 'However, some of the top Deep Learning frameworks out there today are:',\n",
       " '\\nTensorFlow\\nKeras\\nPyTorch\\nCaffe2\\nCNTK\\nMXNet\\nTheano\\n',\n",
       " '11. What is the use of the swish function?',\n",
       " 'The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency.\\n',\n",
       " 'The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency.',\n",
       " '12. What are autoencoders?',\n",
       " 'Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs.\\nAutoencoders, as the name suggests, consist of two entities:\\n\\nEncoder: Used to fit the input into an internal computation state\\nDecoder: Used to convert the computational state back into the output\\n\\n',\n",
       " 'Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs.',\n",
       " 'Autoencoders, as the name suggests, consist of two entities:',\n",
       " '\\nEncoder: Used to fit the input into an internal computation state\\nDecoder: Used to convert the computational state back into the output\\n',\n",
       " '13. What are the steps to be followed to use the gradient descent algorithm?',\n",
       " 'There are five main steps that are used to initialize and use the gradient descent algorithm:\\n\\nInitialize biases and weights for the network\\nSend input data through the network (the input layer)\\nCalculate the difference (the error) between expected and predicted values\\nChange values in neurons to minimize the loss function\\nMultiple iterations to determine the best weights for efficient working\\n\\n',\n",
       " 'There are five main steps that are used to initialize and use the gradient descent algorithm:',\n",
       " '\\nInitialize biases and weights for the network\\nSend input data through the network (the input layer)\\nCalculate the difference (the error) between expected and predicted values\\nChange values in neurons to minimize the loss function\\nMultiple iterations to determine the best weights for efficient working\\n',\n",
       " '14. Differentiate between a single-layer perceptron and a multi-layer perceptron.',\n",
       " '\\n\\n\\n\\nSingle-layer Perceptron\\nMulti-layer Perceptron\\n\\n\\nCannot classify non-linear data points\\nCan classify non-linear data\\n\\n\\nTakes in a limited amount of parameters\\nWithstands a lot of parameters\\n\\n\\nLess efficient with large data\\nHighly efficient with large datasets\\n\\n\\n\\n\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '15. What is data normalization in Deep Learning?',\n",
       " 'Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation.\\n',\n",
       " 'Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation.',\n",
       " '16. What is forward propagation?',\n",
       " 'Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer.\\n',\n",
       " 'Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer.',\n",
       " '17. What is backpropagation?',\n",
       " 'Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers.\\n',\n",
       " 'Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers.',\n",
       " '18. What are hyperparameters in Deep Learning?',\n",
       " 'Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers, and more, present in the neural network.\\n',\n",
       " 'Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers, and more, present in the neural network.',\n",
       " '19. How can hyperparameters be trained in neural networks?',\n",
       " 'Hyperparameters can be trained using four components as shown below:\\n\\nBatch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement.\\nEpochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data.\\nMomentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training.\\nLearning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn.\\n\\nNext up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.\\n\\nIntermediate Interview Questions\\n',\n",
       " 'Hyperparameters can be trained using four components as shown below:',\n",
       " '\\nBatch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement.\\nEpochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data.\\nMomentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training.\\nLearning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn.\\n',\n",
       " 'Next up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.',\n",
       " '20. What is the meaning of dropout in Deep Learning?',\n",
       " 'Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby, causing lower efficiency.\\n',\n",
       " 'Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby, causing lower efficiency.',\n",
       " '21. What are tensors?',\n",
       " 'Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors is easily understood and broadly used.\\n',\n",
       " 'Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors is easily understood and broadly used.',\n",
       " '22. What is the meaning of model capacity in Deep Learning?',\n",
       " 'In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network.\\nWe will check out neural network interview questions alongside as it is also a vital part of Deep Learning.\\n',\n",
       " 'In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network.',\n",
       " 'We will check out neural network interview questions alongside as it is also a vital part of Deep Learning.',\n",
       " '23. What is a Boltzmann machine?',\n",
       " 'A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there.\\n',\n",
       " 'A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there.',\n",
       " '24. What are some of the advantages of using TensorFlow?',\n",
       " 'TensorFlow has numerous advantages, and some of them are as follows:\\n\\nHigh amount of flexibility and platform independence\\nTrains using CPU and GPU\\nSupports auto differentiation and its features\\nHandles threads and asynchronous computation easily\\nOpen-source\\nHas a large community\\n\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'TensorFlow has numerous advantages, and some of them are as follows:',\n",
       " '\\nHigh amount of flexibility and platform independence\\nTrains using CPU and GPU\\nSupports auto differentiation and its features\\nHandles threads and asynchronous computation easily\\nOpen-source\\nHas a large community\\n',\n",
       " 'Courses you may like',\n",
       " '25. What is a computational graph in Deep Learning?',\n",
       " 'A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.\\nIf you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course.\\n',\n",
       " 'A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.',\n",
       " 'If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course.',\n",
       " '26. What is a CNN?',\n",
       " 'CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily.\\nThese Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary.\\n',\n",
       " 'CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily.',\n",
       " 'These Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary.',\n",
       " '27. What are the various layers present in a CNN?',\n",
       " 'There are four main layers that form a convolutional neural network:\\n\\nConvolution: These are layers consisting of entities called filters that are used as parameters to train the network.\\nReLu: It is used as the activation function and is always used with the convolution layer.\\nPooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage.\\nConnectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.\\n\\n',\n",
       " 'There are four main layers that form a convolutional neural network:',\n",
       " '\\nConvolution: These are layers consisting of entities called filters that are used as parameters to train the network.\\nReLu: It is used as the activation function and is always used with the convolution layer.\\nPooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage.\\nConnectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.\\n',\n",
       " '28. What is an RNN in Deep Learning?',\n",
       " 'RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements.\\n',\n",
       " 'RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements.',\n",
       " '29. What is a vanishing gradient when using RNNs?',\n",
       " 'Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network.\\n',\n",
       " 'Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network.',\n",
       " '30. What is exploding gradient descent in Deep Learning?',\n",
       " 'Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training.\\nThe working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model.\\n',\n",
       " 'Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training.',\n",
       " 'The working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model.',\n",
       " '31. What is the use of LSTM?',\n",
       " 'LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity.\\n',\n",
       " 'LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity.',\n",
       " '32. Where are autoencoders used?',\n",
       " 'Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:\\n\\nAdding color to black–white images\\nRemoving noise from images\\nDimensionality reduction\\nFeature removal and variation\\n\\n',\n",
       " 'Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:',\n",
       " '\\nAdding color to black–white images\\nRemoving noise from images\\nDimensionality reduction\\nFeature removal and variation\\n',\n",
       " '33. What are the types of autoencoders?',\n",
       " 'There are four main types of autoencoders:\\n\\nDeep autoencoders\\nConvolutional autoencoders\\nSparse autoencoders\\nContractive autoencoders\\n\\n',\n",
       " 'There are four main types of autoencoders:',\n",
       " '\\nDeep autoencoders\\nConvolutional autoencoders\\nSparse autoencoders\\nContractive autoencoders\\n',\n",
       " '34. What is a Restricted Boltzmann Machine?',\n",
       " 'A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform:\\n\\nDimensionality reduction\\nRegression\\nClassification\\nCollaborative filtering\\nTopic modeling\\n\\nNext up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.\\n\\n\\nAdvanced Interview Questions\\n',\n",
       " 'A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform:',\n",
       " '\\nDimensionality reduction\\nRegression\\nClassification\\nCollaborative filtering\\nTopic modeling\\n',\n",
       " 'Next up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.',\n",
       " '',\n",
       " '35. What are some of the limitations of Deep Learning?',\n",
       " 'There are a few disadvantages of Deep Learning as mentioned below:\\n\\nNetworks in Deep Learning require a huge amount of data to train well.\\nDeep Learning concepts can be complex to implement sometimes.\\nAchieving a high amount of model efficiency is difficult in many cases.\\n\\nThese are some of the vital advanced deep learning interview questions that you have to know about!\\n',\n",
       " 'There are a few disadvantages of Deep Learning as mentioned below:',\n",
       " '\\nNetworks in Deep Learning require a huge amount of data to train well.\\nDeep Learning concepts can be complex to implement sometimes.\\nAchieving a high amount of model efficiency is difficult in many cases.\\n',\n",
       " 'These are some of the vital advanced deep learning interview questions that you have to know about!',\n",
       " '36. What are the variants of gradient descent?',\n",
       " 'There are three variants of gradient descent as shown below:\\n\\nStochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters.\\nBatch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration.\\nMini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent.\\n\\n',\n",
       " 'There are three variants of gradient descent as shown below:',\n",
       " '\\nStochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters.\\nBatch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration.\\nMini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent.\\n',\n",
       " '37. Why is mini-batch gradient descent so popular?',\n",
       " 'Mini-batch gradient descent is popular as:\\n\\nIt is more efficient when compared to stochastic gradient descent.\\nGeneralization is done by finding the flat minima.\\nIt helps avoid the local minima by allowing the approximation of the gradient for the entire dataset.\\n\\n',\n",
       " 'Mini-batch gradient descent is popular as:',\n",
       " '\\nIt is more efficient when compared to stochastic gradient descent.\\nGeneralization is done by finding the flat minima.\\nIt helps avoid the local minima by allowing the approximation of the gradient for the entire dataset.\\n',\n",
       " '38. What are deep autoencoders?',\n",
       " 'Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on.\\nUsually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where:\\n\\nThe first five shallow layers consist of the encoding part\\nThe other layers take care of the decoding part\\n\\nOn the next set of Deep Learning questions, let us look further into the topic.\\n',\n",
       " 'Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on.',\n",
       " 'Usually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where:',\n",
       " '\\nThe first five shallow layers consist of the encoding part\\nThe other layers take care of the decoding part\\n',\n",
       " 'On the next set of Deep Learning questions, let us look further into the topic.',\n",
       " '39. Why is the Leaky ReLU function used in Deep Learning?',\n",
       " 'Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero.\\n',\n",
       " 'Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero.',\n",
       " '40. What are some of the examples of supervised learning algorithms in Deep Learning?',\n",
       " 'There are three main supervised learning algorithms in Deep Learning:\\n\\nArtificial neural networks\\nConvolutional neural networks\\nRecurrent neural networks\\n\\n',\n",
       " 'There are three main supervised learning algorithms in Deep Learning:',\n",
       " '\\nArtificial neural networks\\nConvolutional neural networks\\nRecurrent neural networks\\n',\n",
       " '41. What are some of the examples of unsupervised learning algorithms in Deep Learning?',\n",
       " 'There are three main unsupervised learning algorithms in Deep Learning:\\n\\nAutoencoders\\nBoltzmann machines\\nSelf-organizing maps\\n\\nNext up, let us look at\\xa0 more neural network interview questions that will help you ace the interviews.\\n',\n",
       " 'There are three main unsupervised learning algorithms in Deep Learning:',\n",
       " '\\nAutoencoders\\nBoltzmann machines\\nSelf-organizing maps\\n',\n",
       " 'Next up, let us look at\\xa0 more neural network interview questions that will help you ace the interviews.',\n",
       " '42. Can we initialize the weights of a network to start from zero?',\n",
       " 'Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons.\\n',\n",
       " 'Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons.',\n",
       " '43. What is the meaning of valid padding and same padding in CNN?',\n",
       " '\\nValid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n – f + 1) X (n – f + 1) after convolution.\\nSame padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix.\\n\\n',\n",
       " '\\nValid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n – f + 1) X (n – f + 1) after convolution.\\nSame padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix.\\n',\n",
       " '44. What are some of the applications of transfer learning in Deep Learning?',\n",
       " 'Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks.\\nThe popular examples of transfer learning are in the case of:\\n\\nBERT\\nResNet\\nGPT-2\\nVGG-16\\n\\n',\n",
       " 'Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks.',\n",
       " 'The popular examples of transfer learning are in the case of:',\n",
       " '\\nBERT\\nResNet\\nGPT-2\\nVGG-16\\n',\n",
       " '45. How is the transformer architecture better than RNNs in Deep Learning?',\n",
       " 'With the use of sequential processing, programmers were up against:\\n\\nThe usage of high processing power\\nThe difficulty of parallel execution\\n\\nThis caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.\\n\\n',\n",
       " 'With the use of sequential processing, programmers were up against:',\n",
       " '\\nThe usage of high processing power\\nThe difficulty of parallel execution\\n',\n",
       " 'This caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.',\n",
       " '',\n",
       " '46. What are the steps involved in the working of an LSTM network?',\n",
       " 'There are three main steps involved in the working of an LSTM network:\\n\\nThe network picks up the information that it has to remember and identifies what to forget.\\nCell state values are updated based on Step 1.\\nThe network calculates and analyzes which part of the current state should make it to the output.\\n\\n',\n",
       " 'There are three main steps involved in the working of an LSTM network:',\n",
       " '\\nThe network picks up the information that it has to remember and identifies what to forget.\\nCell state values are updated based on Step 1.\\nThe network calculates and analyzes which part of the current state should make it to the output.\\n',\n",
       " '47. What are the elements in TensorFlow that are programmable?',\n",
       " 'In TensorFlow, users can program three elements:\\n\\nConstants\\nVariables\\nPlaceholders\\n\\n',\n",
       " 'In TensorFlow, users can program three elements:',\n",
       " '\\nConstants\\nVariables\\nPlaceholders\\n',\n",
       " '48. What is the meaning of bagging and boosting in Deep Learning?',\n",
       " 'Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model.\\nBoosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy.\\n',\n",
       " 'Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model.',\n",
       " 'Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy.',\n",
       " '49. What are generative adversarial networks (GANs)?',\n",
       " 'Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output.\\nThe generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator.\\n',\n",
       " 'Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output.',\n",
       " 'The generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator.',\n",
       " '50. Why are generative adversarial networks (GANs) so popular?',\n",
       " 'Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working.\\n\\nCreation of art: GANs are used to create artistic images, sketches, and paintings.\\nImage enhancement: They are used to greatly enhance the resolution of the input images.\\nImage translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily.\\n\\nIf you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Course. With this program, you can become proficient in all of the concepts of Deep Learning and AI and earn a course certificate as well.\\n',\n",
       " 'Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working.',\n",
       " '\\nCreation of art: GANs are used to create artistic images, sketches, and paintings.\\nImage enhancement: They are used to greatly enhance the resolution of the input images.\\nImage translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily.\\n',\n",
       " 'If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Course. With this program, you can become proficient in all of the concepts of Deep Learning and AI and earn a course certificate as well.',\n",
       " 'Course Schedule',\n",
       " 'Leave a Reply Cancel reply',\n",
       " 'Your email address will not be published. Required fields are marked *',\n",
       " 'Comment ',\n",
       " 'Name * ',\n",
       " 'Email * ',\n",
       " ' \\n\\n',\n",
       " '\\n \\n',\n",
       " 'Courses',\n",
       " '\\nHadoop Training\\nMachine Learning Course\\nData Science Course\\nPython Certification\\nDevops Training\\nCEH Certification \\n Business Analyst Certification\\nCyber Security Courses\\n',\n",
       " 'Courses',\n",
       " '\\nAWS Training\\nSalesforce Training\\nSelenium Training\\nArtificial Intelligence Course\\nBig Data Course\\nAzure Administrator Certification\\nCyber Security Certification\\n',\n",
       " 'Tutorials',\n",
       " '\\nPython Tutorial \\nAWS Tutorial \\nDevops Tutorial \\nTableau Tutorial \\nBlockchain Tutorial \\n',\n",
       " 'Interview Questions',\n",
       " '\\nPython Interview Questions\\nAWS Interview Questions \\nData Science Interview Questions \\nDevops Interview Questions \\nSalesforce Interview Questions \\n',\n",
       " 'Download Salary Trends',\n",
       " 'Learn how professionals like you got upto 100% hike!',\n",
       " 'Course Preview',\n",
       " 'Expert-Led No.1',\n",
       " 'Top 50 Deep Learning and Machine Learning Interview Questions']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst7 = []\n",
    "url = \"https://intellipaat.com/blog/interview-question/deep-learning-interview-questions/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h3','p','ul'])\n",
    "for answer in answers:\n",
    "    lst7.append(answer.text)\n",
    "lst7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the difference between Machine Learning and Deep Learning?',\n",
       " 'Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.\\nDeep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.\\n\\n',\n",
       " 'Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.',\n",
       " 'Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.',\n",
       " '2. What is a perceptron?',\n",
       " 'A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.\\nA perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.\\n\\n',\n",
       " 'A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.',\n",
       " 'A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.',\n",
       " '3. How is Deep Learning better than Machine Learning?',\n",
       " 'Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.\\n\\n',\n",
       " 'Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.',\n",
       " '4. What are some of the most used applications of Deep Learning?',\n",
       " 'Deep Learning is used in a variety of fields today. The most used ones are as follows:\\n\\nSentiment Analysis\\nComputer Vision\\nAutomatic Text Generation\\nObject Detection\\nNatural Language Processing\\nImage Recognition\\n\\n\\n',\n",
       " 'Deep Learning is used in a variety of fields today. The most used ones are as follows:',\n",
       " '\\nSentiment Analysis\\nComputer Vision\\nAutomatic Text Generation\\nObject Detection\\nNatural Language Processing\\nImage Recognition\\n',\n",
       " '5. What is the meaning of overfitting?',\n",
       " 'Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.\\nThis makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.\\n\\n',\n",
       " 'Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.',\n",
       " 'This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.',\n",
       " '6. What are activation functions?',\n",
       " 'Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias.\\nUsing an activation function makes the model output to be non-linear. There are many types of activation functions:\\n\\nReLU\\nSoftmax\\nSigmoid\\nLinear\\nTanh\\n\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Activation functions are entities in Deep Learning that are used to translate inputs into a usable output parameter. It is a function that decides if a neuron needs activation or not by calculating the weighted sum on it with the bias.',\n",
       " 'Using an activation function makes the model output to be non-linear. There are many types of activation functions:',\n",
       " '\\nReLU\\nSoftmax\\nSigmoid\\nLinear\\nTanh\\n',\n",
       " 'Get 50% Hike!',\n",
       " 'Master Most in Demand Skills Now !',\n",
       " '7. Why is Fourier transform used in Deep Learning?',\n",
       " 'Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.\\n\\n',\n",
       " 'Fourier transform is an effective package used for analyzing and managing large amounts of data present in a database. It can take in real-time array data and process it quickly. This ensures that high efficiency is maintained and also makes the model more open to processing a variety of signals.',\n",
       " '8. What are the steps involved in training a perception in Deep Learning?',\n",
       " 'There are five main steps that determine the learning of a perceptron:\\n\\nInitialize thresholds and weights\\nProvide inputs\\nCalculate outputs\\nUpdate weights in each step\\nRepeat steps 2 to 4\\n\\n\\n',\n",
       " 'There are five main steps that determine the learning of a perceptron:',\n",
       " '9. What is the use of the loss function?',\n",
       " 'The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset.\\nThe loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.\\n\\n',\n",
       " 'The loss function is used as a measure of accuracy to see if a neural network has learned accurately from the training data or not. This is done by comparing the training dataset to the testing dataset.',\n",
       " 'The loss function is a primary measure of the performance of the neural network. In Deep Learning, a good performing network will have a low loss function at all times when training.',\n",
       " '10. What are some of the Deep Learning frameworks or tools that you have used?',\n",
       " 'This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools.\\nHowever, some of the top Deep Learning frameworks out there today are:\\n\\nTensorFlow\\nKeras\\nPyTorch\\nCaffe2\\nCNTK\\nMXNet\\nTheano\\n\\n',\n",
       " 'This question is quite common in a Deep Learning interview. Make sure to answer based on the experience you have with the tools.',\n",
       " 'However, some of the top Deep Learning frameworks out there today are:',\n",
       " '\\nTensorFlow\\nKeras\\nPyTorch\\nCaffe2\\nCNTK\\nMXNet\\nTheano\\n',\n",
       " '11. What is the use of the swish function?',\n",
       " 'The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency.\\n',\n",
       " 'The swish function is a self-gated activation function developed by Google. It is now a popular activation function used by many as Google claims that it outperforms all of the other activation functions in terms of computational efficiency.',\n",
       " '12. What are autoencoders?',\n",
       " 'Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs.\\nAutoencoders, as the name suggests, consist of two entities:\\n\\nEncoder: Used to fit the input into an internal computation state\\nDecoder: Used to convert the computational state back into the output\\n\\n',\n",
       " 'Autoencoders are artificial neural networks that learn without any supervision. Here, these networks have the ability to automatically learn by mapping the inputs to the corresponding outputs.',\n",
       " 'Autoencoders, as the name suggests, consist of two entities:',\n",
       " '\\nEncoder: Used to fit the input into an internal computation state\\nDecoder: Used to convert the computational state back into the output\\n',\n",
       " '13. What are the steps to be followed to use the gradient descent algorithm?',\n",
       " 'There are five main steps that are used to initialize and use the gradient descent algorithm:\\n\\nInitialize biases and weights for the network\\nSend input data through the network (the input layer)\\nCalculate the difference (the error) between expected and predicted values\\nChange values in neurons to minimize the loss function\\nMultiple iterations to determine the best weights for efficient working\\n\\n',\n",
       " 'There are five main steps that are used to initialize and use the gradient descent algorithm:',\n",
       " '\\nInitialize biases and weights for the network\\nSend input data through the network (the input layer)\\nCalculate the difference (the error) between expected and predicted values\\nChange values in neurons to minimize the loss function\\nMultiple iterations to determine the best weights for efficient working\\n',\n",
       " '14. Differentiate between a single-layer perceptron and a multi-layer perceptron.',\n",
       " '\\n\\n\\n\\nSingle-layer Perceptron\\nMulti-layer Perceptron\\n\\n\\nCannot classify non-linear data points\\nCan classify non-linear data\\n\\n\\nTakes in a limited amount of parameters\\nWithstands a lot of parameters\\n\\n\\nLess efficient with large data\\nHighly efficient with large datasets\\n\\n\\n\\n\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '15. What is data normalization in Deep Learning?',\n",
       " 'Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation.\\n',\n",
       " 'Data normalization is a preprocessing step that is used to refit the data into a specific range. This ensures that the network can learn effectively as it has better convergence when performing backpropagation.',\n",
       " '16. What is forward propagation?',\n",
       " 'Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer.\\n',\n",
       " 'Forward propagation is the scenario where inputs are passed to the hidden layer with weights. In every single hidden layer, the output of the activation function is calculated until the next layer can be processed. It is called forward propagation as the process begins from the input layer and moves toward the final output layer.',\n",
       " '17. What is backpropagation?',\n",
       " 'Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers.\\n',\n",
       " 'Backprobation is used to minimize the cost function by first seeing how the value changes when weights and biases are tweaked in the neural network. This change is easily calculated by understanding the gradient at every hidden layer. It is called backpropagation as the process begins from the output layer, moving backward to the input layers.',\n",
       " '18. What are hyperparameters in Deep Learning?',\n",
       " 'Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers, and more, present in the neural network.\\n',\n",
       " 'Hyperparameters are variables used to determine the structure of a neural network. They are also used to understand parameters, such as the learning rate and the number of hidden layers, and more, present in the neural network.',\n",
       " '19. How can hyperparameters be trained in neural networks?',\n",
       " 'Hyperparameters can be trained using four components as shown below:\\n\\nBatch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement.\\nEpochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data.\\nMomentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training.\\nLearning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn.\\n\\nNext up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.\\n\\nIntermediate Interview Questions\\n',\n",
       " 'Hyperparameters can be trained using four components as shown below:',\n",
       " '\\nBatch size: This is used to denote the size of the input chunk. Batch sizes can be varied and cut into sub-batches based on the requirement.\\nEpochs: An epoch denotes the number of times the training data is visible to the neural network so that it can train. Since the process is iterative, the number of epochs will vary based on the data.\\nMomentum: Momentum is used to understand the next consecutive steps that occur with the current data being executed at hand. It is used to avoid oscillations when training.\\nLearning rate: Learning rate is used as a parameter to denote the time required for the network to update the parameters and learn.\\n',\n",
       " 'Next up on this top Deep Learning interview questions and answers blog, let us take a look at the intermediate questions.',\n",
       " '20. What is the meaning of dropout in Deep Learning?',\n",
       " 'Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby, causing lower efficiency.\\n',\n",
       " 'Dropout is a technique that is used to avoid overfitting a model in Deep Learning. If the dropout value is too low, then it will have minimal effect on learning. If it is too high, then the model can under-learn, thereby, causing lower efficiency.',\n",
       " '21. What are tensors?',\n",
       " 'Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors is easily understood and broadly used.\\n',\n",
       " 'Tensors are multidimensional arrays in Deep Learning that are used to represent data. They represent the data with higher dimensions. Due to the high-level nature of the programming languages, the syntax of tensors is easily understood and broadly used.',\n",
       " '22. What is the meaning of model capacity in Deep Learning?',\n",
       " 'In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network.\\nWe will check out neural network interview questions alongside as it is also a vital part of Deep Learning.\\n',\n",
       " 'In Deep Learning, model capacity refers to the capacity of the model to take in a variety of mapping functions. Higher model capacity means a large amount of information can be stored in the network.',\n",
       " 'We will check out neural network interview questions alongside as it is also a vital part of Deep Learning.',\n",
       " '23. What is a Boltzmann machine?',\n",
       " 'A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there.\\n',\n",
       " 'A Boltzmann machine is a type of recurrent neural network that uses binary decisions, alongside biases, to function. These neural networks can be hooked up together to create deep belief networks, which are very sophisticated and used to solve the most complex problems out there.',\n",
       " '24. What are some of the advantages of using TensorFlow?',\n",
       " 'TensorFlow has numerous advantages, and some of them are as follows:\\n\\nHigh amount of flexibility and platform independence\\nTrains using CPU and GPU\\nSupports auto differentiation and its features\\nHandles threads and asynchronous computation easily\\nOpen-source\\nHas a large community\\n\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'TensorFlow has numerous advantages, and some of them are as follows:',\n",
       " '\\nHigh amount of flexibility and platform independence\\nTrains using CPU and GPU\\nSupports auto differentiation and its features\\nHandles threads and asynchronous computation easily\\nOpen-source\\nHas a large community\\n',\n",
       " 'Courses you may like',\n",
       " '25. What is a computational graph in Deep Learning?',\n",
       " 'A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.\\nIf you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course.\\n',\n",
       " 'A computation graph is a series of operations that are performed to take inputs and arrange them as nodes in a graph structure. It can be considered as a way of implementing mathematical calculations into a graph. This helps in parallel processing and provides high performance in terms of computational capability.',\n",
       " 'If you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Engineer Course.',\n",
       " '26. What is a CNN?',\n",
       " 'CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily.\\nThese Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary.\\n',\n",
       " 'CNNs are convolutional neural networks that are used to perform analysis on images and visuals. These classes of neural networks can input a multi-channel image and work on it easily.',\n",
       " 'These Deep Learning questions must be answered in a concise way. So make sure to understand them and revisit them if necessary.',\n",
       " '27. What are the various layers present in a CNN?',\n",
       " 'There are four main layers that form a convolutional neural network:\\n\\nConvolution: These are layers consisting of entities called filters that are used as parameters to train the network.\\nReLu: It is used as the activation function and is always used with the convolution layer.\\nPooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage.\\nConnectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.\\n\\n',\n",
       " 'There are four main layers that form a convolutional neural network:',\n",
       " '\\nConvolution: These are layers consisting of entities called filters that are used as parameters to train the network.\\nReLu: It is used as the activation function and is always used with the convolution layer.\\nPooling: Pooling is the concept of shrinking the complex data entities that form after convolution and is primarily used to maintain the size of an image after shrinkage.\\nConnectedness: This is used to ensure that all of the layers in the neural network are fully connected and activation can be computed using the bias easily.\\n',\n",
       " '28. What is an RNN in Deep Learning?',\n",
       " 'RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements.\\n',\n",
       " 'RNNs stand for recurrent neural networks, which form to be a popular type of artificial neural network. They are used to process sequences of data, text, genomes, handwriting, and more. RNNs make use of backpropagation for the training requirements.',\n",
       " '29. What is a vanishing gradient when using RNNs?',\n",
       " 'Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network.\\n',\n",
       " 'Vanishing gradient is a scenario that occurs when we use RNNs. Since RNNs make use of backpropagation, gradients at every step of the way will tend to get smaller as the network traverses through backward iterations. This equates to the model learning very slowly, thereby, causing efficiency problems in the network.',\n",
       " '30. What is exploding gradient descent in Deep Learning?',\n",
       " 'Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training.\\nThe working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model.\\n',\n",
       " 'Exploding gradients are an issue causing a scenario that clumps up the gradients. This creates a large number of updates of the weights in the model when training.',\n",
       " 'The working of gradient descent is based on the condition that the updates are small and controlled. Controlling the updates will directly affect the efficiency of the model.',\n",
       " '31. What is the use of LSTM?',\n",
       " 'LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity.\\n',\n",
       " 'LSTM stands for long short-term memory. It is a type of RNN that is used to sequence a string of data. It consists of feedback chains that give it the ability to perform like a general-purpose computational entity.',\n",
       " '32. Where are autoencoders used?',\n",
       " 'Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:\\n\\nAdding color to black–white images\\nRemoving noise from images\\nDimensionality reduction\\nFeature removal and variation\\n\\n',\n",
       " 'Autoencoders have a wide variety of usage in the real world. The following are some of the popular ones:',\n",
       " '\\nAdding color to black–white images\\nRemoving noise from images\\nDimensionality reduction\\nFeature removal and variation\\n',\n",
       " '33. What are the types of autoencoders?',\n",
       " 'There are four main types of autoencoders:\\n\\nDeep autoencoders\\nConvolutional autoencoders\\nSparse autoencoders\\nContractive autoencoders\\n\\n',\n",
       " 'There are four main types of autoencoders:',\n",
       " '\\nDeep autoencoders\\nConvolutional autoencoders\\nSparse autoencoders\\nContractive autoencoders\\n',\n",
       " '34. What is a Restricted Boltzmann Machine?',\n",
       " 'A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform:\\n\\nDimensionality reduction\\nRegression\\nClassification\\nCollaborative filtering\\nTopic modeling\\n\\nNext up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.\\n\\n\\nAdvanced Interview Questions\\n',\n",
       " 'A Restricted Boltzmann Machine, or RBM for short, is an undirected graphical model that is popularly used in Deep Learning today. It is an algorithm that is used to perform:',\n",
       " '\\nDimensionality reduction\\nRegression\\nClassification\\nCollaborative filtering\\nTopic modeling\\n',\n",
       " 'Next up on this top Deep Learning interview questions and answers blog, let us take a look at the advanced questions.',\n",
       " '',\n",
       " '35. What are some of the limitations of Deep Learning?',\n",
       " 'There are a few disadvantages of Deep Learning as mentioned below:\\n\\nNetworks in Deep Learning require a huge amount of data to train well.\\nDeep Learning concepts can be complex to implement sometimes.\\nAchieving a high amount of model efficiency is difficult in many cases.\\n\\nThese are some of the vital advanced deep learning interview questions that you have to know about!\\n',\n",
       " 'There are a few disadvantages of Deep Learning as mentioned below:',\n",
       " '\\nNetworks in Deep Learning require a huge amount of data to train well.\\nDeep Learning concepts can be complex to implement sometimes.\\nAchieving a high amount of model efficiency is difficult in many cases.\\n',\n",
       " 'These are some of the vital advanced deep learning interview questions that you have to know about!',\n",
       " '36. What are the variants of gradient descent?',\n",
       " 'There are three variants of gradient descent as shown below:\\n\\nStochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters.\\nBatch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration.\\nMini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent.\\n\\n',\n",
       " 'There are three variants of gradient descent as shown below:',\n",
       " '\\nStochastic gradient descent: A single training example is used for the calculation of gradient and for updating parameters.\\nBatch gradient descent: Gradient is calculated for the entire dataset, and parameters are updated at every iteration.\\nMini-batch gradient descent: Samples are broken down into smaller-sized batches and then worked on as in the case of stochastic gradient descent.\\n',\n",
       " '37. Why is mini-batch gradient descent so popular?',\n",
       " 'Mini-batch gradient descent is popular as:\\n\\nIt is more efficient when compared to stochastic gradient descent.\\nGeneralization is done by finding the flat minima.\\nIt helps avoid the local minima by allowing the approximation of the gradient for the entire dataset.\\n\\n',\n",
       " 'Mini-batch gradient descent is popular as:',\n",
       " '\\nIt is more efficient when compared to stochastic gradient descent.\\nGeneralization is done by finding the flat minima.\\nIt helps avoid the local minima by allowing the approximation of the gradient for the entire dataset.\\n',\n",
       " '38. What are deep autoencoders?',\n",
       " 'Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on.\\nUsually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where:\\n\\nThe first five shallow layers consist of the encoding part\\nThe other layers take care of the decoding part\\n\\nOn the next set of Deep Learning questions, let us look further into the topic.\\n',\n",
       " 'Deep autoencoders are an extension of the regular autoencoders. Here, the first layer is responsible for the first-order function execution of the input. The second layer will take care of the second-order functions, and it goes on.',\n",
       " 'Usually, a deep autoencoder is a combination of two or more symmetrical deep-belief networks where:',\n",
       " '\\nThe first five shallow layers consist of the encoding part\\nThe other layers take care of the decoding part\\n',\n",
       " 'On the next set of Deep Learning questions, let us look further into the topic.',\n",
       " '39. Why is the Leaky ReLU function used in Deep Learning?',\n",
       " 'Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero.\\n',\n",
       " 'Leaky ReLU, also called LReL, is used to manage a function to allow the passing of small-sized negative values if the input value to the network is less than zero.',\n",
       " '40. What are some of the examples of supervised learning algorithms in Deep Learning?',\n",
       " 'There are three main supervised learning algorithms in Deep Learning:\\n\\nArtificial neural networks\\nConvolutional neural networks\\nRecurrent neural networks\\n\\n',\n",
       " 'There are three main supervised learning algorithms in Deep Learning:',\n",
       " '\\nArtificial neural networks\\nConvolutional neural networks\\nRecurrent neural networks\\n',\n",
       " '41. What are some of the examples of unsupervised learning algorithms in Deep Learning?',\n",
       " 'There are three main unsupervised learning algorithms in Deep Learning:\\n\\nAutoencoders\\nBoltzmann machines\\nSelf-organizing maps\\n\\nNext up, let us look at\\xa0 more neural network interview questions that will help you ace the interviews.\\n',\n",
       " 'There are three main unsupervised learning algorithms in Deep Learning:',\n",
       " '\\nAutoencoders\\nBoltzmann machines\\nSelf-organizing maps\\n',\n",
       " 'Next up, let us look at\\xa0 more neural network interview questions that will help you ace the interviews.',\n",
       " '42. Can we initialize the weights of a network to start from zero?',\n",
       " 'Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons.\\n',\n",
       " 'Yes, it is possible to begin with zero initialization. However, it is not recommended to use because setting up the weights to zero initially will cause all of the neurons to produce the same output and the same gradients when performing backpropagation. This means that the network will not have the ability to learn at all due to the absence of asymmetry between each of the neurons.',\n",
       " '43. What is the meaning of valid padding and same padding in CNN?',\n",
       " '\\nValid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n – f + 1) X (n – f + 1) after convolution.\\nSame padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix.\\n\\n',\n",
       " '\\nValid padding: It is used when there is no requirement for padding. The output matrix will have the dimensions (n – f + 1) X (n – f + 1) after convolution.\\nSame padding: Here, padding elements are added all around the output matrix. It will have the same dimensions as the input matrix.\\n',\n",
       " '44. What are some of the applications of transfer learning in Deep Learning?',\n",
       " 'Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks.\\nThe popular examples of transfer learning are in the case of:\\n\\nBERT\\nResNet\\nGPT-2\\nVGG-16\\n\\n',\n",
       " 'Transfer learning is a scenario where a large model is trained on a dataset with a large amount of data and this model is used on simpler datasets, thereby resulting in extremely efficient and accurate neural networks.',\n",
       " 'The popular examples of transfer learning are in the case of:',\n",
       " '\\nBERT\\nResNet\\nGPT-2\\nVGG-16\\n',\n",
       " '45. How is the transformer architecture better than RNNs in Deep Learning?',\n",
       " 'With the use of sequential processing, programmers were up against:\\n\\nThe usage of high processing power\\nThe difficulty of parallel execution\\n\\nThis caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.\\n\\n',\n",
       " 'With the use of sequential processing, programmers were up against:',\n",
       " '\\nThe usage of high processing power\\nThe difficulty of parallel execution\\n',\n",
       " 'This caused the rise of the transformer architecture. Here, there is a mechanism called attention mechanism, which is used to map all of the dependencies between sentences, thereby making huge progress in the case of NLP models.',\n",
       " '',\n",
       " '46. What are the steps involved in the working of an LSTM network?',\n",
       " 'There are three main steps involved in the working of an LSTM network:\\n\\nThe network picks up the information that it has to remember and identifies what to forget.\\nCell state values are updated based on Step 1.\\nThe network calculates and analyzes which part of the current state should make it to the output.\\n\\n',\n",
       " 'There are three main steps involved in the working of an LSTM network:',\n",
       " '\\nThe network picks up the information that it has to remember and identifies what to forget.\\nCell state values are updated based on Step 1.\\nThe network calculates and analyzes which part of the current state should make it to the output.\\n',\n",
       " '47. What are the elements in TensorFlow that are programmable?',\n",
       " 'In TensorFlow, users can program three elements:\\n\\nConstants\\nVariables\\nPlaceholders\\n\\n',\n",
       " 'In TensorFlow, users can program three elements:',\n",
       " '\\nConstants\\nVariables\\nPlaceholders\\n',\n",
       " '48. What is the meaning of bagging and boosting in Deep Learning?',\n",
       " 'Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model.\\nBoosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy.\\n',\n",
       " 'Bagging is the concept of splitting a dataset and randomly placing it into bags for training the model.',\n",
       " 'Boosting is the scenario where incorrect data points are used to force the model to produce the wrong output. This is used to retrain the model and increase accuracy.',\n",
       " '49. What are generative adversarial networks (GANs)?',\n",
       " 'Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output.\\nThe generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator.\\n',\n",
       " 'Generative adversarial networks are used to achieve generative modeling in Deep Learning. It is an unsupervised task that involves the discovery of patterns in the input data to generate the output.',\n",
       " 'The generator is used to generate new examples, while the discriminator is used to classify the examples generated by the generator.',\n",
       " '50. Why are generative adversarial networks (GANs) so popular?',\n",
       " 'Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working.\\n\\nCreation of art: GANs are used to create artistic images, sketches, and paintings.\\nImage enhancement: They are used to greatly enhance the resolution of the input images.\\nImage translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily.\\n\\nIf you are looking forward to becoming an expert in Deep Learning, make sure to check out Intellipaat’s AI Course. With this program, you can become proficient in all of the concepts of Deep Learning and AI and earn a course certificate as well.\\n',\n",
       " 'Generative adversarial networks are used for a variety of purposes. In the case of working with images, they have a high amount of traction and efficient working.',\n",
       " '\\nCreation of art: GANs are used to create artistic images, sketches, and paintings.\\nImage enhancement: They are used to greatly enhance the resolution of the input images.\\nImage translation: They are also used to change certain aspects, such as day to night and summer to winter, in images easily.\\n']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new7 = lst7[46:244]\n",
    "lst_new7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between Machine Learning and Deep Learning?</td>\n",
       "      <td>Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience. Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.  Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is a perceptron?</td>\n",
       "      <td>A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output. A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.  A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How is Deep Learning better than Machine Learning?</td>\n",
       "      <td>Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.  Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are some of the most used applications of Deep Learning?</td>\n",
       "      <td>Deep Learning is used in a variety of fields today. The most used ones are as follows:  Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition   Deep Learning is used in a variety of fields today. The most used ones are as follows: Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the meaning of overfitting?</td>\n",
       "      <td>Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.  Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Questions  \\\n",
       "0    What is the difference between Machine Learning and Deep Learning?   \n",
       "1                                                 What is a perceptron?   \n",
       "2                    How is Deep Learning better than Machine Learning?   \n",
       "3         What are some of the most used applications of Deep Learning?   \n",
       "4                                   What is the meaning of overfitting?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Answer  \n",
       "0                                                                                                                            Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience. Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.  Machine Learning forms a subset of Artificial Intelligence, where we use statistics and algorithms to train machines with data, thereby, helping them improve with experience.Deep Learning is a part of Machine Learning, which involves mimicking the human brain in terms of structures called neurons, thereby, forming neural networks.  \n",
       "1                                                                A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output. A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.  A perceptron is similar to the actual neuron in the human brain. It receives inputs from various entities and applies functions to these inputs, which transform them to be the output.A perceptron is mainly used to perform binary classification where it sees an input, computes functions based on the weights of the input, and outputs the required transformation.  \n",
       "2                                                                                                                                                             Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.  Machine Learning is powerful in a way that it is sufficient to solve most of the problems. However, Deep Learning gets an upper hand when it comes to working with data that has a large number of dimensions. With data that is large in size, a Deep Learning model can easily work with it as it is built to handle this.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                              Deep Learning is used in a variety of fields today. The most used ones are as follows:  Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition   Deep Learning is used in a variety of fields today. The most used ones are as follows: Sentiment Analysis Computer Vision Automatic Text Generation Object Detection Natural Language Processing Image Recognition   \n",
       "4  Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information. This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.  Overfitting is a very common issue when working with Deep Learning. It is a scenario where the Deep Learning algorithm vigorously hunts through the data to obtain some valid information.This makes the Deep Learning model pick up noise rather than useful data, causing very high variance and low bias. This makes the model less accurate, and this is an undesirable effect that can be prevented.  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new7:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df7=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df7[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign in',\n",
       " 'Tomer Amit',\n",
       " 'Mar 29, 2020·6 min read',\n",
       " 'Below are 25 questions on deep learning which can help you test your knowledge, as well as being a good review resource for interview preparation.',\n",
       " '1. Why is it necessary to introduce non-linearities in a neural network?',\n",
       " 'Solution: otherwise, we would have a composition of linear functions, which is also a linear function, giving a linear model. A linear model has a much smaller number of parameters, and is therefore limited in the complexity it can model.',\n",
       " '2. Describe two ways of dealing with the vanishing gradient problem in a neural network.',\n",
       " 'Solution:',\n",
       " 'Using ReLU activation instead of sigmoid.Using Xavier initialization.',\n",
       " '3. What are some advantages in using a CNN (convolutional neural network) rather than a DNN (dense neural network) in an image classification task?',\n",
       " 'Solution: while both models can capture the relationship between close pixels, CNNs have the following properties:',\n",
       " 'It is translation invariant — the exact location of the pixel is irrelevant for the filter.It is less likely to overfit — the typical number of parameters in a CNN is much smaller than that of a DNN.Gives us a better understanding of the model — we can look at the filters’ weights and visualize what the network “learned”.Hierarchical nature — learns patterns in by describing complex patterns using simpler ones.',\n",
       " '4. Describe two ways to visualize features of a CNN in an image classification task.',\n",
       " 'Solution:',\n",
       " 'Input occlusion — cover a part of the input image and see which part affect the classification the most. For instance, given a trained image classification model, give the images below as input. If, for instance, we see that the 3rd image is classified with 98% probability as a dog, while the 2nd image only with 65% accuracy, it means that the part covered in the 2nd image is more important.',\n",
       " 'Activation Maximization — the idea is to create an artificial input image that maximize the target response (gradient ascent).',\n",
       " '5. Is trying the following learning rates: 0.1,0.2,…,0.5 a good strategy to optimize the learning rate?',\n",
       " 'Solution: No, it is recommended to try a logarithmic scale to optimize the learning rate.',\n",
       " '6. Suppose you have a NN with 3 layers and ReLU activations. What will happen if we initialize all the weights with the same value? what if we only had 1 layer (i.e linear/logistic regression?)',\n",
       " 'Solution: If we initialize all the weights to be the same we would not be able to break the symmetry; i.e, all gradients will be updated the same and the network will not be able to learn. In the 1-layers scenario, however, the cost function is convex (linear/sigmoid) and thus the weights will always converge to the optimal point, regardless of the initial value (convergence may be slower).',\n",
       " '7. Explain the idea behind the Adam optimizer.',\n",
       " 'Solution: Adam, or adaptive momentum, combines two ideas to improve convergence: per-parameter updates which give faster convergence, and momentum which helps to avoid getting stuck in saddle point.',\n",
       " '8. Compare batch, mini-batch and stochastic gradient descent.',\n",
       " 'Solution: batch refers to estimating the data by taking the entire data, mini-batch by sampling a few datapoints, and SGD refers to update the gradient one datapoint at each epoch. The tradeoff here is between how precise the calculation of the gradient is versus what size of batch we can keep in memory. Moreover, taking mini-batch rather than the entire batch has a regularizing effect by adding random noise at each epoch.',\n",
       " '9. What is data augmentation? Give examples.',\n",
       " 'Solution: Data augmentation is a technique to increase the input data by performing manipulations on the original data. For instance in images, one can: rotate the image, reflect (flip) the image, add Gaussian blur.',\n",
       " '10. What is the idea behind GANs?',\n",
       " 'Solution: GANs, or generative adversarial networks, consist of two networks (D,G) where D is the “discriminator” network and G is the “generative” network. The goal is to create data — images, for instance, which are undistinguishable from real images. Suppose we want to create an adversarial example of a cat. The network G will generate images. The network D will classify images according to whether they are a cat or not. The cost function of G will be constructed such that it tries to “fool” D — to classify its output always as cat.',\n",
       " '11. What are the advantages of using Batchnorm?',\n",
       " 'Solution: Batchnorm accelerates the training process. It also (as a byproduct of including some noise) has a regularizing effect.',\n",
       " '12. What is multi-task learning? When should it be used?',\n",
       " 'Solution: Multi-tasking is useful when we have a small amount of data for some task, and we would benefit from training a model on a large dataset of another task. Parameters of the models are shared — either in a “hard” way (i.e the same parameters) or a “soft” way (i.e regularization/penalty to the cost function).',\n",
       " '13. What is end-to-end learning? Give a few of its advantages.',\n",
       " 'Solution: End-to-end learning is usually a model which gets the raw data and outputs directly the desired outcome, with no intermediate tasks or feature engineering. It has several advantages, among which: there is no need to handcraft features, and it generally leads to lower bias.',\n",
       " '14. What happens if we use a ReLU activation and then a sigmoid as the final layer?',\n",
       " 'Solution: Since ReLU always outputs a non-negative result, the network will constantly predict one class for all the inputs!',\n",
       " '15. How to solve the exploding gradient problem?',\n",
       " 'Solution: A simple solution to the exploding gradient problem is gradient clipping — taking the gradient to be ±M when its absolute value is bigger than M, where M is some large number.',\n",
       " '16. Is it necessary to shuffle the training data when using batch gradient descent?',\n",
       " 'Solution: No, because the gradient is calculated at each epoch using the entire training data, so shuffling does not make a difference.',\n",
       " '17. When using mini batch gradient descent, why is it important to shuffle the data?',\n",
       " 'Solution: otherwise, suppose we train a NN classifier and have two classes — A and B, and that all samples of one class come before the other class. Not shuffling the data will make the weights converge to a wrong value.',\n",
       " '18. Describe some hyperparameters for transfer learning.',\n",
       " 'Solution: How many layers to keep, how many layers to add, how many to freeze.',\n",
       " '19. Is dropout used on the test set?',\n",
       " 'Solution: No! only in the train set. Dropout is a regularization technique that is applied in the training process.',\n",
       " '20. Explain why dropout in a neural network acts as a regularizer.',\n",
       " 'Solution: There are several (related) explanations to why dropout works. It can be seen as a form of model averaging — at each step we “turn off” a part of the model and average the models we get. It also adds noise, which naturally has a regularizing effect. It also leads to more sparsity of the weights and essentially prevents co-adaptation of neurons in the network.',\n",
       " '21. Give examples in which a many-to-one RNN architecture is appropriate.',\n",
       " 'Solution: A few examples are: sentiment analysis, gender recognition from speech.',\n",
       " '22. When can’t we use BiLSTM? Explain what assumption has to be made.',\n",
       " 'Solution: in any bi-directional model, we assume that we have access to the next elements of the sequence in a given “time”. This is the case for text data (i.e sentiment analysis, translation etc.), but not the case for time-series data.',\n",
       " '23. True/false: adding L2 regularization to a RNN can help with the vanishing gradient problem.',\n",
       " 'Solution: false! Adding L2 regularization will shrink the weights towards zero, which can actually make the vanishing gradients worse in some cases.',\n",
       " '24. Suppose the training error/cost is high and that the validation cost/error is almost equal to it. What does it mean? What should be done?',\n",
       " 'Solution: this indicates underfitting. One can add more parameters, increase the complexity of the model, or lower the regularization.',\n",
       " '25. Describe how L2 regularization can be explained as a sort of a weight decay.',\n",
       " 'Solution: Suppose our cost function is C(w), and that we add a penalization c|w|2 . When using gradient descent, the iterations will look like',\n",
       " 'w = w -grad(C)(w) — 2cw = (1–2c)w — grad(C)(w)',\n",
       " 'In this equation, the weight is multiplied by a factor < 1.',\n",
       " 'Quantitative researcher, data scientist & machine learning specialist: https://www.linkedin.com/in/tomer-amit',\n",
       " '404 ',\n",
       " '3',\n",
       " '404\\xa0',\n",
       " '404 ',\n",
       " '3',\n",
       " 'Deep LearningMachine LearningInterviewAIData Science',\n",
       " 'Your home for data science. A Medium publication sharing concepts, ideas and codes.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst8 = []\n",
    "url = \"https://towardsdatascience.com/50-deep-learning-interview-questions-part-1-2-8bbc8a00ec61\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['p','ul'])\n",
    "for answer in answers:\n",
    "    lst8.append(answer.text)\n",
    "lst8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Why is it necessary to introduce non-linearities in a neural network?',\n",
       " 'Solution: otherwise, we would have a composition of linear functions, which is also a linear function, giving a linear model. A linear model has a much smaller number of parameters, and is therefore limited in the complexity it can model.',\n",
       " '2. Describe two ways of dealing with the vanishing gradient problem in a neural network.',\n",
       " 'Solution:',\n",
       " 'Using ReLU activation instead of sigmoid.Using Xavier initialization.',\n",
       " '3. What are some advantages in using a CNN (convolutional neural network) rather than a DNN (dense neural network) in an image classification task?',\n",
       " 'Solution: while both models can capture the relationship between close pixels, CNNs have the following properties:',\n",
       " 'It is translation invariant — the exact location of the pixel is irrelevant for the filter.It is less likely to overfit — the typical number of parameters in a CNN is much smaller than that of a DNN.Gives us a better understanding of the model — we can look at the filters’ weights and visualize what the network “learned”.Hierarchical nature — learns patterns in by describing complex patterns using simpler ones.',\n",
       " '4. Describe two ways to visualize features of a CNN in an image classification task.',\n",
       " 'Solution:',\n",
       " 'Input occlusion — cover a part of the input image and see which part affect the classification the most. For instance, given a trained image classification model, give the images below as input. If, for instance, we see that the 3rd image is classified with 98% probability as a dog, while the 2nd image only with 65% accuracy, it means that the part covered in the 2nd image is more important.',\n",
       " 'Activation Maximization — the idea is to create an artificial input image that maximize the target response (gradient ascent).',\n",
       " '5. Is trying the following learning rates: 0.1,0.2,…,0.5 a good strategy to optimize the learning rate?',\n",
       " 'Solution: No, it is recommended to try a logarithmic scale to optimize the learning rate.',\n",
       " '6. Suppose you have a NN with 3 layers and ReLU activations. What will happen if we initialize all the weights with the same value? what if we only had 1 layer (i.e linear/logistic regression?)',\n",
       " 'Solution: If we initialize all the weights to be the same we would not be able to break the symmetry; i.e, all gradients will be updated the same and the network will not be able to learn. In the 1-layers scenario, however, the cost function is convex (linear/sigmoid) and thus the weights will always converge to the optimal point, regardless of the initial value (convergence may be slower).',\n",
       " '7. Explain the idea behind the Adam optimizer.',\n",
       " 'Solution: Adam, or adaptive momentum, combines two ideas to improve convergence: per-parameter updates which give faster convergence, and momentum which helps to avoid getting stuck in saddle point.',\n",
       " '8. Compare batch, mini-batch and stochastic gradient descent.',\n",
       " 'Solution: batch refers to estimating the data by taking the entire data, mini-batch by sampling a few datapoints, and SGD refers to update the gradient one datapoint at each epoch. The tradeoff here is between how precise the calculation of the gradient is versus what size of batch we can keep in memory. Moreover, taking mini-batch rather than the entire batch has a regularizing effect by adding random noise at each epoch.',\n",
       " '9. What is data augmentation? Give examples.',\n",
       " 'Solution: Data augmentation is a technique to increase the input data by performing manipulations on the original data. For instance in images, one can: rotate the image, reflect (flip) the image, add Gaussian blur.',\n",
       " '10. What is the idea behind GANs?',\n",
       " 'Solution: GANs, or generative adversarial networks, consist of two networks (D,G) where D is the “discriminator” network and G is the “generative” network. The goal is to create data — images, for instance, which are undistinguishable from real images. Suppose we want to create an adversarial example of a cat. The network G will generate images. The network D will classify images according to whether they are a cat or not. The cost function of G will be constructed such that it tries to “fool” D — to classify its output always as cat.',\n",
       " '11. What are the advantages of using Batchnorm?',\n",
       " 'Solution: Batchnorm accelerates the training process. It also (as a byproduct of including some noise) has a regularizing effect.',\n",
       " '12. What is multi-task learning? When should it be used?',\n",
       " 'Solution: Multi-tasking is useful when we have a small amount of data for some task, and we would benefit from training a model on a large dataset of another task. Parameters of the models are shared — either in a “hard” way (i.e the same parameters) or a “soft” way (i.e regularization/penalty to the cost function).',\n",
       " '13. What is end-to-end learning? Give a few of its advantages.',\n",
       " 'Solution: End-to-end learning is usually a model which gets the raw data and outputs directly the desired outcome, with no intermediate tasks or feature engineering. It has several advantages, among which: there is no need to handcraft features, and it generally leads to lower bias.',\n",
       " '14. What happens if we use a ReLU activation and then a sigmoid as the final layer?',\n",
       " 'Solution: Since ReLU always outputs a non-negative result, the network will constantly predict one class for all the inputs!',\n",
       " '15. How to solve the exploding gradient problem?',\n",
       " 'Solution: A simple solution to the exploding gradient problem is gradient clipping — taking the gradient to be ±M when its absolute value is bigger than M, where M is some large number.',\n",
       " '16. Is it necessary to shuffle the training data when using batch gradient descent?',\n",
       " 'Solution: No, because the gradient is calculated at each epoch using the entire training data, so shuffling does not make a difference.',\n",
       " '17. When using mini batch gradient descent, why is it important to shuffle the data?',\n",
       " 'Solution: otherwise, suppose we train a NN classifier and have two classes — A and B, and that all samples of one class come before the other class. Not shuffling the data will make the weights converge to a wrong value.',\n",
       " '18. Describe some hyperparameters for transfer learning.',\n",
       " 'Solution: How many layers to keep, how many layers to add, how many to freeze.',\n",
       " '19. Is dropout used on the test set?',\n",
       " 'Solution: No! only in the train set. Dropout is a regularization technique that is applied in the training process.',\n",
       " '20. Explain why dropout in a neural network acts as a regularizer.',\n",
       " 'Solution: There are several (related) explanations to why dropout works. It can be seen as a form of model averaging — at each step we “turn off” a part of the model and average the models we get. It also adds noise, which naturally has a regularizing effect. It also leads to more sparsity of the weights and essentially prevents co-adaptation of neurons in the network.',\n",
       " '21. Give examples in which a many-to-one RNN architecture is appropriate.',\n",
       " 'Solution: A few examples are: sentiment analysis, gender recognition from speech.',\n",
       " '22. When can’t we use BiLSTM? Explain what assumption has to be made.',\n",
       " 'Solution: in any bi-directional model, we assume that we have access to the next elements of the sequence in a given “time”. This is the case for text data (i.e sentiment analysis, translation etc.), but not the case for time-series data.',\n",
       " '23. True/false: adding L2 regularization to a RNN can help with the vanishing gradient problem.',\n",
       " 'Solution: false! Adding L2 regularization will shrink the weights towards zero, which can actually make the vanishing gradients worse in some cases.',\n",
       " '24. Suppose the training error/cost is high and that the validation cost/error is almost equal to it. What does it mean? What should be done?',\n",
       " 'Solution: this indicates underfitting. One can add more parameters, increase the complexity of the model, or lower the regularization.',\n",
       " '25. Describe how L2 regularization can be explained as a sort of a weight decay.',\n",
       " 'Solution: Suppose our cost function is C(w), and that we add a penalization c|w|2 . When using gradient descent, the iterations will look like',\n",
       " 'w = w -grad(C)(w) — 2cw = (1–2c)w — grad(C)(w)',\n",
       " 'In this equation, the weight is multiplied by a factor < 1.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new8 = lst8[4:60]\n",
    "lst_new8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is it necessary to introduce non-linearities in a neural network?</td>\n",
       "      <td>Solution: otherwise, we would have a composition of linear functions, which is also a linear function, giving a linear model. A linear model has a much smaller number of parameters, and is therefore limited in the complexity it can model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Describe two ways of dealing with the vanishing gradient problem in a neural network.</td>\n",
       "      <td>Solution:Using ReLU activation instead of sigmoid.Using Xavier initialization.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are some advantages in using a CNN (convolutional neural network) rather than a DNN (dense neural network) in an image classification task?</td>\n",
       "      <td>Solution: while both models can capture the relationship between close pixels, CNNs have the following properties:It is translation invariant — the exact location of the pixel is irrelevant for the filter.It is less likely to overfit — the typical number of parameters in a CNN is much smaller than that of a DNN.Gives us a better understanding of the model — we can look at the filters’ weights and visualize what the network “learned”.Hierarchical nature — learns patterns in by describing complex patterns using simpler ones.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe two ways to visualize features of a CNN in an image classification task.</td>\n",
       "      <td>Solution:Input occlusion — cover a part of the input image and see which part affect the classification the most. For instance, given a trained image classification model, give the images below as input. If, for instance, we see that the 3rd image is classified with 98% probability as a dog, while the 2nd image only with 65% accuracy, it means that the part covered in the 2nd image is more important.Activation Maximization — the idea is to create an artificial input image that maximize the target response (gradient ascent).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is trying the following learning rates: 1, 2,…, 5 a good strategy to optimize the learning rate?</td>\n",
       "      <td>Solution: No, it is recommended to try a logarithmic scale to optimize the learning rate.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                            Questions  \\\n",
       "0                                                                               Why is it necessary to introduce non-linearities in a neural network?   \n",
       "1                                                               Describe two ways of dealing with the vanishing gradient problem in a neural network.   \n",
       "2    What are some advantages in using a CNN (convolutional neural network) rather than a DNN (dense neural network) in an image classification task?   \n",
       "3                                                                   Describe two ways to visualize features of a CNN in an image classification task.   \n",
       "4                                                    Is trying the following learning rates: 1, 2,…, 5 a good strategy to optimize the learning rate?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Answer  \n",
       "0                                                                                                                                                                                                                                                                                                     Solution: otherwise, we would have a composition of linear functions, which is also a linear function, giving a linear model. A linear model has a much smaller number of parameters, and is therefore limited in the complexity it can model.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Solution:Using ReLU activation instead of sigmoid.Using Xavier initialization.  \n",
       "2   Solution: while both models can capture the relationship between close pixels, CNNs have the following properties:It is translation invariant — the exact location of the pixel is irrelevant for the filter.It is less likely to overfit — the typical number of parameters in a CNN is much smaller than that of a DNN.Gives us a better understanding of the model — we can look at the filters’ weights and visualize what the network “learned”.Hierarchical nature — learns patterns in by describing complex patterns using simpler ones.  \n",
       "3  Solution:Input occlusion — cover a part of the input image and see which part affect the classification the most. For instance, given a trained image classification model, give the images below as input. If, for instance, we see that the 3rd image is classified with 98% probability as a dog, while the 2nd image only with 65% accuracy, it means that the part covered in the 2nd image is more important.Activation Maximization — the idea is to create an artificial input image that maximize the target response (gradient ascent).  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                          Solution: No, it is recommended to try a logarithmic scale to optimize the learning rate.  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new8:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df8=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df8[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data has now become the raw material for businesses and vast amounts of structured and unstructured information are being increasingly used to create a new form of economic value for businesses of every size. In this sense, Data Science is precisely the field of study where collective processes, theories, and technologies are combined that allow the review, analysis, and extraction of valuable knowledge and information from hard data. Now with the market growing leaps and bounds, there is a significant dearth of skilled data scientists, who can help businesses sift through an overabundance of data and come up with meaningful insights.',\n",
       " 'So if you are planning to move on the path to becoming a data scientist, you need to prepare well and create a fabulous impression on your prospective employers with your knowledge. This write-up brings you some important data science interview questions and answers to help you crack your data science interview.',\n",
       " 'To learn more about data science, read our blog on –\\xa0What is data science?',\n",
       " 'The article is segmented on different data science topics–',\n",
       " '\\nBasic Data Science Interview Questions\\nStatistical Interview Questions\\nProgramming Language Interview Questions\\nMachine Learning Interview Questions\\n',\n",
       " 'Q1. What is the difference between data science and big data?',\n",
       " 'Ans. The common differences between data science and big data are –',\n",
       " 'Big Data',\n",
       " 'Data Science',\n",
       " 'This question is among the basic data science interview questions and you must prepare for such questions.',\n",
       " 'You may also be interested in exploring:\\xa0',\n",
       " 'Q2. How do you check for data quality?',\n",
       " 'Ans. Some of the definitions used to check for data quality are:',\n",
       " '\\nCompleteness\\nConsistency\\nUniqueness\\nIntegrity\\nConformity\\nAccuracy\\xa0\\n',\n",
       " 'Q3. Suppose you are given survey data, and it has some missing data, how would you deal with missing values \\u200b\\u200bfrom that survey?',\n",
       " 'Ans. This is among the important data science interview questions. There are two main techniques for dealing with missing values –\\xa0',\n",
       " 'Debugging Techniques – It is a Data Cleaning process consisting of evaluating the quality of the information collected, increasing its quality, in order to avoid lax analysis. The most popular debugging techniques are –\\xa0',\n",
       " 'Searching the list of values: It is about searching the data matrix for values \\u200b\\u200bthat are outside the response range. These values \\u200b\\u200bcan be considered as missing, or the correct value can be estimated from other variables',\n",
       " 'Filtering questions: It is about comparing the number of responses of a filter category and another filtered category. If any anomaly is observed that cannot be solved, it will be considered as a lost value.',\n",
       " 'Checking for Logical Consistencies: The answers that may be considered contradictory to each other are checked.',\n",
       " 'Counting the Level of representativeness: A count is made of the number of responses obtained in each variable. If the number of unanswered questions is very high, it is possible to assume equality between the answers and the non-answers or to make an imputation of the non-answer.',\n",
       " '\\nImputation Technique\\n',\n",
       " 'This technique consists of replacing the missing values \\u200b\\u200bwith valid values \\u200b\\u200bor answers by estimating them. There are three types of imputation:',\n",
       " '\\nRandom imputation\\nHot Deck imputation\\xa0\\nImputation of the mean of subclasses\\n',\n",
       " 'Q4. How would you deal with missing random values \\u200b\\u200bfrom a data set?',\n",
       " 'Ans. There are two forms of randomly missing values:',\n",
       " 'MCAR or Missing completely at random. Such errors happen when the missing values are randomly distributed across all observations.\\xa0',\n",
       " 'We can confirm this error by partitioning the data into two parts –',\n",
       " 'After we have partitioned the data, we conduct a t-test of mean difference to check if there is any difference in the sample between the two data sets.',\n",
       " 'In case the data are MCAR, we may choose a pair-wise or a list-wise deletion of missing value cases.\\xa0\\xa0\\xa0',\n",
       " 'MAR or Missing at random. It is a common occurrence. Here, the missing values are not randomly distributed across observations but are distributed within one or more sub-samples. We cannot predict the probability from the variables in the model. Data imputation is mainly performed to replace them.',\n",
       " 'Q5. What is Hadoop, and why should I care?',\n",
       " 'Ans. Hadoop is an open-source processing framework that manages data processing and storage for big data applications running on pooled systems.',\n",
       " 'Apache Hadoop is a collection of open-source utility software that makes it easy to use a network of multiple computers to solve problems involving large amounts of data and computation. It provides a software framework for distributed storage and big data processing using the MapReduce programming model.',\n",
       " 'Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packets of code to nodes to process the data in parallel. This allows the data set to be processed faster and more efficiently than if conventional supercomputing architecture were used.',\n",
       " 'Q6. What is ‘fsck’?',\n",
       " 'Ans. ‘fsck ‘ abbreviation for ‘ file system check.’ It is a type of command that searches for possible errors in the file. fsck generates a summary report, which lists the file system’s overall health and sends it to the Hadoop distributed file system.',\n",
       " 'This is among the important data science interview questions and you must prepare for the related terminologies as well.',\n",
       " 'Q7. Which is better – good data or good models?',\n",
       " 'Ans. This might be one of the frequently asked data science interview questions.',\n",
       " 'The answer to this question is very subjective and depends on the specific case. Big companies prefer good data; it is the foundation of any successful business. On the other hand, good models couldn’t be created without good data.',\n",
       " 'Based on your personal preference, you will probably choose no right or wrong answer (unless the company requires one specifically).',\n",
       " 'Q8. What are Recommender Systems?',\n",
       " 'Ans. Recommender systems are a subclass of information filtering systems, used to predict how users would rate or score particular objects (movies, music, merchandise, etc.). Recommender systems filter large volumes of information based on the data provided by a user and other factors, and they take care of the user’s preference and interest.',\n",
       " 'Recommender systems utilize algorithms that optimize the analysis of the data to build the recommendations. They ensure a high level of efficiency as they can associate elements of our consumption profiles such as purchase history, content selection, and even our hours of activity, to make accurate recommendations.',\n",
       " 'To know more about the job profile and responsibilities of a Data Scientist, refer to this article on What is Data Scientist?',\n",
       " 'Q9.\\xa0What are the different types of Recommender Systems?',\n",
       " 'Ans. There are three main types of Recommender systems.',\n",
       " 'Collaborative filtering – Collaborative filtering is a method of making automatic predictions by using the recommendations of other people. There are two types of collaborative filtering techniques –',\n",
       " '\\nUser-User collaborative filtering\\nItem-Item collaborative filtering\\n',\n",
       " 'Content-Based Filtering– Content-based filtering is based on the description of an item and a user’s choices. As the name suggests, it uses content (keywords) to describe the items, and the user profile is built to state the type of item this user likes.',\n",
       " '\\xa0',\n",
       " '',\n",
       " 'Image – Collaborative filtering & Content-based filtering',\n",
       " 'Hybrid Recommendation Systems – Hybrid Recommendation engines are a combination of diverse rating and sorting algorithms. A hybrid recommendation engine can recommend a wide range of products to consumers as per their history and preferences with precision.',\n",
       " 'Q10.\\xa0Differentiate between wide and long data formats.',\n",
       " 'Ans. In a wide format, categorical data are always grouped.',\n",
       " 'The long data format is in which there are a number of instances with many variables and subject variables.',\n",
       " 'Q11. What are Interpolation and Extrapolation?',\n",
       " 'Ans. Interpolation – This is the method to guess data points between data sets. It is a prediction between the given data points.',\n",
       " 'Extrapolation – This is the method to guess data point beyond data sets. It is a prediction beyond given data points.',\n",
       " 'Also Read>>Skills That Employers Look For In a Data Scientist',\n",
       " 'Q12. How much data is enough to get a valid outcome?',\n",
       " 'Ans. All the businesses are different and measured in different ways. Thus, you never have enough data and there will be no right answer. The amount of data required depends on the methods you use to have an excellent chance of obtaining vital results.',\n",
       " 'Q13. What is the difference between ‘expected value’ and ‘average value’?',\n",
       " 'Ans. When it comes to functionality, there is no difference between the two. However, they are used in different situations.',\n",
       " 'An expected value usually reflects random variables, while the average value reflects the population sample.',\n",
       " 'Q14. What happens if two users access the same HDFS file at the same time?',\n",
       " 'Ans. This is a bit of a tricky question. The answer itself is not complicated, but it is easy to confuse by the similarity of programs’ reactions.',\n",
       " 'When the first user is accessing the file, the second user’s inputs will be rejected because HDFS NameNode supports exclusive write.',\n",
       " 'Q15. What is power analysis?',\n",
       " 'Ans. Power analysis allows the determination of the sample size required to detect an effect of a given size with a given degree of confidence.',\n",
       " 'Q16.\\xa0Is it better to have too many false negatives or too many false positives?',\n",
       " 'Ans. This is among the popularly asked data science interview questions and will depend on how you show your viewpoint. Give examples',\n",
       " 'These are some of the popular data science interview questions. Always be prepared to answer all types of data science interview questions— technical skills, interpersonal, leadership, or methodologies. If you are someone who has recently started your career in Data Science, you can always get certified to improve your skills and boost your career opportunities.',\n",
       " 'Q17. What is the importance of statistics in data science?',\n",
       " 'Ans. Statistics help data scientists to get a better idea of a customer’s expectations. Using statistical methods, data Scientists can acquire knowledge about consumer interest, behavior, engagement, retention, etc. It also helps to build robust data models to validate certain inferences and predictions.',\n",
       " 'Q18. What are the different statistical techniques used in data science?',\n",
       " 'Ans. There are many statistical techniques used in data science, including –',\n",
       " 'The arithmetic mean – It is a measure of the average of a set of data',\n",
       " 'Graphic display – Includes charts and graphs to visually display, analyze, clarify, and interpret numerical data through histograms, pie charts, bars, etc.',\n",
       " 'Correlation – Establishes and measures relationships between different variables',\n",
       " 'Regression – Allows identifying if the evolution of one variable affects others',\n",
       " 'Time series – It predicts future values \\u200b\\u200bby analyzing sequences of past values',\n",
       " 'Data mining and other Big Data techniques to process large volumes of data',\n",
       " 'Sentiment analysis – It determines the attitude of specific agents or people towards an issue, often using data from social networks',\n",
       " 'Semantic analysis – It helps to extract knowledge from large amounts of texts',\n",
       " 'A / B testing – To determine which of two variables works best with randomized experiments',\n",
       " 'Machine learning using automatic learning algorithms to ensure excellent performance in the presence of big data',\n",
       " 'Check Out Our Data Science Courses',\n",
       " 'Q19. What is an RDBMS? Name some examples for RDBMS?',\n",
       " 'Ans.\\xa0 This is among the most frequently asked data science interview questions.',\n",
       " 'A relational database management system (RDBMS) is a database management system that is based on a relational model.',\n",
       " 'Some examples of RDBMS are MS SQL Server, IBM DB2, Oracle, MySQL, and Microsoft Access.',\n",
       " 'Interviewers often ask such data science interview questions and you must prepare for such abbreviations.',\n",
       " 'Q20. What are a Z test, Chi-Square test, F test, and T-test?',\n",
       " 'Ans. Z test is applied for large samples. Z test = (Estimated Mean – Real Mean)/ (square root real variance / n).',\n",
       " 'Chi-Square test is a statistical method assessing the goodness of fit between a set of observed values and those expected theoretically.',\n",
       " 'F-test is used to compare 2 populations’ variances. F = explained variance/unexplained variance.',\n",
       " 'T-test is applied for small samples. T-test = (Estimated Mean – Real Mean)/ (square root Estimated variance / n).',\n",
       " 'Q21. What does P-value signify about the statistical data?',\n",
       " 'Ans. The p-value is the probability for a given statistical model that, when the null hypothesis is true, the statistical summary would be the same as or more extreme than the actual observed results.',\n",
       " 'When,',\n",
       " 'P-value>0.05, it denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected.',\n",
       " 'P-value <= 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected.',\n",
       " 'P-value=0.05is the marginal value indicating it is possible to go either way',\n",
       " 'Q22. Differentiate between univariate, bivariate, and multivariate analysis.',\n",
       " 'Ans. Univariate analysis is the simplest form of statistical analysis where only one variable is involved.',\n",
       " 'Bivariate analysis is where two variables are analyzed and in multivariate analysis, multiple variables are examined.',\n",
       " 'Q23. What is association analysis? Where is it used?',\n",
       " 'Ans. Association analysis is the task of uncovering relationships among data. It is used to understand how the data items are associated with each other.',\n",
       " 'Also Read –\\xa0 Top 6 Industries Hiring Data Scientists in 2021',\n",
       " 'Q24. What is the difference between squared error and absolute error?',\n",
       " 'Ans. Squared error measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated.',\n",
       " 'Absolute error is the difference between the measured or inferred value of a quantity and its actual value.',\n",
       " 'Q25. What is an API? What are APIs used for?',\n",
       " 'Ans. API stands for Application Program Interface and is a set of routines, protocols, and tools for building software applications.',\n",
       " 'With API, it is easier to develop software applications.',\n",
       " 'Q26. What is Collaborative filtering?',\n",
       " 'Ans. Collaborative filtering is a method of making automatic predictions by using the recommendations of other people.',\n",
       " 'Q27. Why do data scientists use combinatorics or discrete probability?',\n",
       " 'Ans. It is used because it is useful in studying any predictive model.',\n",
       " 'Also Read>>How are Data Scientist and Data Analyst different?',\n",
       " 'Q28. What do you understand by Recall and Precision?',\n",
       " 'Ans. Precision is the fraction of retrieved instances that are relevant, while Recall is the fraction of relevant instances that are retrieved.',\n",
       " 'Become Machine Learning Expert Now>>',\n",
       " 'Q29. What is market basket analysis?',\n",
       " 'Ans. Market Basket Analysis is a modeling technique based upon the theory that if you buy a certain group of items, you are more (or less) likely to buy another group of items.',\n",
       " 'Q30. What is the central limit theorem?',\n",
       " 'Ans. The central limit theorem states that the distribution of an average will tend to be Normal as the sample size increases, regardless of the distribution from which the average is taken except when the moments of the parent distribution do not exist.',\n",
       " 'Q31. Explain the difference between type I and type II errors.',\n",
       " 'Ans. Type I error is the rejection of a true null hypothesis or false-positive finding, while Type II error is the non-rejection of a false null hypothesis or false-negative finding.',\n",
       " 'Q32. What is Linear Regression?',\n",
       " 'Ans. It is one of the most commonly asked networking interview questions.',\n",
       " 'Linear regression is the most popular type of predictive analysis. It is used to model the relationship between a scalar response and explanatory variables.',\n",
       " 'Q33. What are the limitations of a Linear Model/Regression?',\n",
       " 'Ans.',\n",
       " '\\nLinear models are limited to linear relationships, such as dependent and independent variables\\nLinear regression looks at a relationship between the mean of the dependent variable and the independent variables, and not the extremes of the dependent variable\\nLinear regression is sensitive to univariate or multivariate outliers\\nLinear regression tend to assume that the data are independent\\n',\n",
       " 'Q34. What is the goal of A/B Testing?',\n",
       " 'Ans. A/B testing is a comparative study, where two or more variants of a page are presented before random users and their feedback is statistically analyzed to check which variation performs better.',\n",
       " 'Q35. What is the main difference between overfitting and underfitting?',\n",
       " 'Ans. Overfitting – In overfitting, a statistical model describes any random error or noise, and occurs when a model is super complex. An overfit model has poor predictive performance as it overreacts to minor fluctuations in training data.\\nUnderfitting – In underfitting, a statistical model is unable to capture the underlying data trend. This type of model also shows poor predictive performance.',\n",
       " 'Q36. What is a Gaussian distribution and how it is used in data science?',\n",
       " 'Ans. Gaussian distribution or commonly known as bell curve is a common probability distribution curve. Mention the way it can be used in data science in a detailed manner.',\n",
       " 'Q37. Explain the purpose of group functions in SQL. Cite certain examples of group functions.',\n",
       " 'Ans. Group functions provide summary statistics of a data set. Some examples of group functions are –\\na) COUNT\\nb) MAX\\nc) MIN\\nd) AVG\\ne) SUM\\nf) DISTINCT',\n",
       " 'Q38. What is Root Cause Analysis?',\n",
       " 'Ans. Root Cause is defined as a fundamental failure of a process. To analyze such issues, a systematic approach has been devised that is known as Root Cause Analysis (RCA). This method addresses a problem or an accident and gets to its “root cause”.',\n",
       " 'Q39. What is the difference between a Validation Set and a Test Set? ',\n",
       " 'Ans. The validation set is used to minimize overfitting. This is used in parameter selection, which means that it helps to\\nverify any accuracy improvement over the training data set. Test Set is used to test and evaluate the performance of a trained Machine Learning model.',\n",
       " 'Q40. What is the Confusion Matrix?',\n",
       " 'Ans. The confusion matrix is \\u200b\\u200ba very useful tool to assess how good a classification model based on machine learning is. It is also known as an error matrix and can be presented as a summary table to evaluate the performance of a classification model. The number of correct and incorrect predictions are summarized with the count values \\u200b\\u200band broken down by each class.',\n",
       " 'The confusion matrix serves to show explicitly when one class is confused with another, which allows us to work separately with different types of errors.',\n",
       " '',\n",
       " 'Positive (P): The observation is positive (for example, it is a dog)',\n",
       " 'Negative (N): The observation is not positive (for example, it is not a dog)',\n",
       " 'True Positive (TP): Result in which the model correctly predicts the positive class',\n",
       " 'True Negative (TN): Result where the model correctly predicts the negative class',\n",
       " 'False Positive (FP): Also called a type 1 error, a result where the model incorrectly predicts the positive class when it is actually negative',\n",
       " 'False Negative (FN): Also called a type 2 error, a result in which the model incorrectly predicts the negative class when it is actually positive',\n",
       " 'Q41. What is the p-value?',\n",
       " 'Ans. A p-value helps to determine the strength of results in a hypothesis test. It is a number between 0 and 1 and Its value determines the strength of the results.',\n",
       " 'Q42. What is the difference between Causation and Correlation?',\n",
       " 'Ans. Causation denotes any causal relationship between two events and represents its cause and effects.\\nCorrelation determines the relationship between two or more variables.\\nCausation necessarily denotes the presence of correlation, but correlation doesn’t necessarily denote causation.',\n",
       " 'Q43. What is cross-validation?',\n",
       " 'Ans. Cross-validation is a technique to assess the performance of a model on a new independent dataset. One example of cross-validation could be – splitting the data into two groups – training and testing data, where you use the testing data to test the model and training data to build the model.',\n",
       " 'Q44. What do you mean by logistic regression?',\n",
       " 'Ans. Also known as the logit model, logistic regression is a technique to predict the binary result from a linear amalgamation of predictor variables.',\n",
       " 'Q45. What is ‘cluster sampling’?',\n",
       " 'Ans. Cluster sampling is a probability sampling technique where the researcher divides the population into separate groups, called clusters. Then a simple cluster sample is selected from the population. The researcher conducts his analysis of data from the sample pools.',\n",
       " 'Q46. What happens if two users access the same HDFS file at the same time?',\n",
       " 'Ans. This is a bit of a tricky question. The answer itself is not complicated, but it is easy to confuse by the similarity of programs’ reactions.',\n",
       " 'When the first user is accessing the file, the second user’s inputs will be rejected because HDFS NameNode supports exclusive write.',\n",
       " 'Q47. What are the Resampling methods?',\n",
       " 'Ans. Resampling methods are used to estimate the precision of the sample statistics, exchanging labels on data points, and validating models.',\n",
       " 'Q48. What is selection bias, and how can you avoid it?',\n",
       " 'Ans. Selection bias is an experimental error that occurs when the participant pool, or the subsequent data, is not representative of the target population.',\n",
       " 'Selection biases cannot be overcome with statistical analysis of existing data alone, though Heckman correction may be used in special cases.',\n",
       " 'Q49. What is the binomial distribution?',\n",
       " 'Ans. A binomial distribution is a discrete probability distribution that describes the number of successes when conducting independent experiments on a random variable.',\n",
       " 'Formula –',\n",
       " 'Where:',\n",
       " 'n = Number of experiments',\n",
       " 'x = Number of successes',\n",
       " 'p = Probability of success',\n",
       " 'q = Probability of failure (1-p)',\n",
       " 'Q50. What is covariance in statistics?',\n",
       " 'Ans. Covariance is a measure of the joint variability of two random variables. The covariance between two variables x and y can be calculated as follows:',\n",
       " '',\n",
       " 'Where:',\n",
       " '\\nXi – the values of the X-variable\\nYj – the values of the Y-variable\\nX̄ – the mean (average) of the X-variable\\nȲ – the mean (average) of the Y-variable\\nn – the number of data points\\n',\n",
       " 'Q51. What is Root Cause Analysis?',\n",
       " 'Ans. Root Cause Analysis (RCA) is the process of uncovering the root causes of problems to identify appropriate solutions. The RCA assumes that it is much more useful to systematically prevent and resolve underlying issues than just treating symptoms ad hoc and putting out fires.',\n",
       " 'Q52.\\xa0What is Correlation Analysis?',\n",
       " 'Ans. Correlation Analysis is a statistical method to evaluate the strength of the relationship between two quantitative variables. It consists of autocorrelation coefficients, estimated and calculated to make a different spatial relationship. It is used to correlate data based on distance.',\n",
       " 'Q53. What is imputation? List the different types of imputation techniques.',\n",
       " 'Ans. Imputation is the process that allows you to replace missing data with other values. Types of imputation techniques include –',\n",
       " 'Single Imputation: Single imputation denotes that the missing value is replaced by a value.',\n",
       " 'Hot-deck: The missing value is imputed from a similar register, which is chosen at random, based on a punched card.',\n",
       " 'Cold deck Imputation: Select donor data from other sets.',\n",
       " 'Mean Imputation: Substitute the stored value for the mean of that variable in other cases.',\n",
       " 'Mean Imputation: Its purpose is to replace the missing value with predicted values \\u200b\\u200bof a variable that is based on others.',\n",
       " 'Stochastic Regression: equal to the regression, but adds the mean regression variance to the regression imputation.',\n",
       " 'Multiple Imputation: It is a general approach to the problem of missing data, available in commonly used statistical packages. Unlike single imputation, Multiple Imputation estimates the values \\u200b\\u200bmultiple times.',\n",
       " 'Q54. What is the difference between a bar graph and a histogram?',\n",
       " 'Ans. Bar charts and histograms can be used to compare the sizes of the different groups. A bar chart is made up of bars plotted on a chart. A histogram is a graph that represents a frequency distribution; the heights of the bars represent observed frequencies.\\xa0',\n",
       " 'In other words, a histogram is a graphical display of data using bars of different heights. Generally, there is no space between adjacent bars.',\n",
       " 'Bar Charts',\n",
       " '\\nThe columns are placed on a label that represents a categorical variable.\\nThe height of the column indicates the size of the group defined by the categories\\n',\n",
       " 'Histogram',\n",
       " '\\nThe columns are placed on a label that represents a quantitative variable.\\nThe column label can be a single value or a range of values.\\n',\n",
       " 'In bar charts, each column represents a group defined by a categorical variable; and with histograms, each column represents a group defined by a quantitative variable.',\n",
       " 'Q55. Name some of the prominent resampling methods in data science.',\n",
       " 'Ans. The Bootstrap, Permutation Tests, Cross-validation, and Jackknife.',\n",
       " 'Q56. What is an Eigenvalue and Eigenvector?',\n",
       " 'Ans. Eigenvectors are used for understanding linear transformations.',\n",
       " 'Eigenvalue can be referred to as the strength of the transformation in the direction of the eigenvector or the factor by which the compression occurs.',\n",
       " 'Q57. Which technique is used to predict categorical responses?',\n",
       " 'Ans. Classification techniques are used to predict categorical responses.',\n",
       " 'Q58. What is the importance of Sampling?',\n",
       " 'Ans. Sampling is a crucial statistical technique to analyze large volumes of datasets. This involves taking out some samples that represent the entire data population. It is imperative to choose samples that are the true representatives of the whole data set. There are two types of sampling methods – Probability Sampling and Non Probability Sampling.',\n",
       " 'Q59.\\xa0Is it possible to stack two series horizontally? If yes then how will you do it?',\n",
       " 'Ans. Yes, it is possible to stack two series horizontally. We can use concat() function and setting axis = 1.',\n",
       " 'df = pd.concat([s1, s2], axis=1)',\n",
       " 'Q60.\\xa0Tell me the method to convert date-strings to timeseries in a series.',\n",
       " 'Ans.',\n",
       " 'Input:',\n",
       " 'We will use the to_datetime() function',\n",
       " '\\xa0',\n",
       " 'Explore – Python Online Courses & Certifications',\n",
       " 'Q61. Write a program in Python that takes input as the weight of the coins and produces output as the money value of the coins.',\n",
       " 'Ans. Here is an example of the code. You can change the values.',\n",
       " '',\n",
       " 'Q62. Why does Python score high over other programming languages?',\n",
       " 'Ans. This is among the very commonly asked data science interview questions. Python has a wealth of data science libraries; it is incredibly fast and easy to read and learn. The Python suite specializing in deep learning and other machine learning libraries includes popular tools such as sci-kit-learn, Keras, and TensorFlow, which allow data scientists to develop sophisticated data models directly integrated into a production system.',\n",
       " 'To discover data revelations, you will need to use Pandas, the data analysis library for Python. It can handle large amounts of data without the lag of Excel. You can do numerical modeling analysis with Numpy, do scientific computation and calculation with SciPy, and access many powerful machine learning algorithms with the Sci-Kit-learn code library. With the Python API and the iPython Notebook that comes with Anaconda, you will have robust options to visualize your data.',\n",
       " 'Q63. What are the data types used in Python?',\n",
       " 'Ans. Python has the following built-in data types:',\n",
       " '\\nNumber (float, integer)\\nString\\nTuple\\nList\\nSet\\nDictionary\\n',\n",
       " 'Numbers, strings, and tuples are immutable data types, which means that they cannot be modified at run time. Lists, sets, and dictionaries are mutable, which means they can be modified at run time.',\n",
       " 'Q64. What is a Python dictionary?',\n",
       " 'Ans. A dictionary is one of the built-in data types in Python. Defines a messy mapping of unique keys to values. Dictionaries are indexed by keys, and the values \\u200b\\u200bcan be any valid Python data type (even a user-defined class). It should be noted that dictionaries are mutable, which means that they can be modified. A dictionary is created with braces and is indexed using bracket notation.',\n",
       " 'Such common data science interview questions are often asked by the interviewers.',\n",
       " 'Q65. What libraries do data scientists use to plot data in Python?',\n",
       " 'Ans. Matplotlib is the main library used to plot data in Python. However, graphics created with this library need a lot of tweaking to make them look bright and professional. For that reason, many data scientists prefer Seaborn, which allows you to create attractive and meaningful charts with just one line of code.',\n",
       " 'Q66. Explain the difference between lists and tuples.',\n",
       " 'Ans. Both lists and tuples are made up of elements, which are values \\u200b\\u200bof any Python data type. However, these data types have a number of differences:',\n",
       " 'Lists are mutable, while tuples are immutable.',\n",
       " 'Lists are created in brackets (for example, my_list = [a, b, c]), while tuples are in parentheses (for example, my_tuple = (a, b, c)).',\n",
       " 'Lists are slower than tuples.',\n",
       " 'Q67. What are lambda functions?',\n",
       " 'Ans. Lambda functions are anonymous functions in Python. They are very useful when you need to define a function that is very short and consists of a single expression. So instead of formally defining the little function with a specific name, body, and return statement, you can write everything in a short line of code using a lambda function.',\n",
       " 'Q68.\\xa0What is PyTorch?',\n",
       " 'Ans. PyTorch is a Python-based scientific computing package designed to perform numerical calculations using the programming of tensors. It also allows its execution on GPU to speed up calculations. PyTorch is used to replace NumPy and process calculations on GPUs and for research and development in the field of machine learning, mainly focused on the development of neural networks.',\n",
       " 'PyTorch is designed to seamlessly integrate with Python and its popular libraries like NumPy and is easier to learn than other Deep Learning frameworks.\\xa0 PyTorch has a simple Python interface, provides a simple but powerful API, and provides the ability to run models in a production environment, making it a popular deep learning framework.',\n",
       " 'Q69. What are the alternatives to PyTorch?',\n",
       " 'Ans. Some of the best-known alternatives to PyTorch are –',\n",
       " 'Tensorflow – Google Brain Team developed Tensorflow, which is a free software designed for numerical computation using graphs.',\n",
       " 'Caffe – Caffe is a machine learning framework designed with the aim of being used in computer vision or image classification. Caffe is popular for its library of training models that do not require any extra implementation.',\n",
       " 'Microsoft CNTK – Microsoft CNTK is the free software framework developed by Microsoft. It is very popular in the area of \\u200b\\u200bspeech recognition although it can also be used for other fields such as text and images.',\n",
       " 'Theano – Theano is another python library. It helps to define, optimize and evaluate mathematical expressions that involve calculations with multidimensional arrays.',\n",
       " 'Keras – Keras is a high-level API for developing neural networks written in Python. It uses other libraries internally such as Tensorflow, CNTK, and Theano. It was developed to facilitate and speed up the development and experimentation with neural networks.',\n",
       " 'You may consider such data science interview questions to be basic, but such questions are the favorite of interviewers as interviewees often leave behind such data science interview questions while preparing.',\n",
       " 'Q70. What packages are used for data mining in Python and R?',\n",
       " 'Ans. There are various packages in Python and R:',\n",
       " 'Python – Orange, Pandas, NLTK, Matplotlib, and Scikit-learn are some of them.',\n",
       " 'R – Arules, tm, Forecast, and GGPlot are some of the packages.',\n",
       " 'Q71. Which would you prefer – R or Python?',\n",
       " 'Ans.\\xa0 One of the most important data science interview questions.',\n",
       " 'Both R and Python have their own pros and cons. R is mainly used when the data analysis task requires standalone computing or analysis on individual servers. Python, when your data analysis tasks need to be integrated with web apps or if statistics code needs to be incorporated into a production database.',\n",
       " 'Read More – What is Python?',\n",
       " 'Q72. Which package is used to do data import in R and Python? How do you do data import in SAS?',\n",
       " 'Ans. In R, RODBC is used for RDBMS data and data.table for fast-import.',\n",
       " 'In SAS, data and sas7bdat are used to import data.',\n",
       " 'In Python, Pandas package and the commands read_csv, read_sql are used for reading data.',\n",
       " 'Must Read – What is Machine Learning?',\n",
       " 'Q73. What are the various types of classification algorithms?',\n",
       " 'Ans. There are 7 types of classification algorithms, including –\\na) Linear Classifiers: Logistic Regression, Naive Bayes Classifier\\nb) Nearest Neighbor\\nc) Support Vector Machines\\nd) Decision Trees\\ne) Boosted Trees\\nf) Random Forest\\ng) Neural Networks',\n",
       " 'Q74. What is Gradient Descent?',\n",
       " 'Ans. Gradient Descent is a popular algorithm used for training Machine Learning models and find the values of parameters of a function (f), which helps to minimize a cost function.',\n",
       " 'Q75. What is Regularization and what kind of problems does regularization solve?',\n",
       " 'Ans. Regularization is a technique used in an attempt to solve the overfitting problem in statistical models.',\n",
       " 'It helps to solve the overfitting problem in machine learning.',\n",
       " 'Q76. What is a Boltzmann Machine?',\n",
       " 'Ans. Boltzmann Machines have a simple learning algorithm that helps to discover interesting features in training data. These machines represent complex regularities and are used to optimize the weights and the quantity for the problems.',\n",
       " 'This is one of the important data science interview questions that you must prepare for your interview. ',\n",
       " 'Q77. What is hypothesis testing?',\n",
       " 'Ans. Hypothesis testing is an important aspect of any testing procedure in Machine Learning or Data Science to analyze various factors that may have any impact on the outcome of the experiment.',\n",
       " 'Q78. \\xa0What is Pattern Recognition?',\n",
       " 'Ans. Pattern recognition is the process of data classification that includes pattern recognition and identification of data regularities. This methodology involves the extensive use of machine learning algorithms.',\n",
       " 'Q79. \\xa0Where can you use Pattern Recognition?',\n",
       " 'Ans. Pattern Recognition has multiple usabilities, across-',\n",
       " '\\nBio-Informatics\\nComputer Vision\\nData Mining\\nInformal Retrieval\\nStatistics\\nSpeech Recognition\\n',\n",
       " 'Q80. What is an Autoencoder?',\n",
       " 'Ans. These are feedforward learning networks where the input is the same as the output. Autoencoders reduce the number of dimensions in the data to encode it while ensuring minimal error and then reconstruct the output from this representation.',\n",
       " 'Also Explore – Deep Learning Online Courses & Certifications',\n",
       " 'Q81. What is the bias-variance trade-off?',\n",
       " 'Ans. Bias – Bias is the difference between the average prediction of a model and the correct value we are trying to predict.',\n",
       " 'Variance – Variance is the variability of model prediction for a given data point or a value that tells us the spread of our data.',\n",
       " 'Models with high variance focus on training data and such models perform very well on training data. On the other hand, a model with high bias doesn’t focus on training data and oversimplifies the model, leading to increased training and test data error.',\n",
       " '',\n",
       " 'Fig – Optimal balance – Bias vs. Variance (Source – towardsdatascience.com)',\n",
       " 'Q82. When do you need to update the algorithm in Data science?',\n",
       " 'Ans. You need to update an algorithm in the following situation:',\n",
       " '\\nYou want your data model to evolve as data streams using infrastructure\\nThe underlying data source is changing\\nIf it is non-stationarity\\n',\n",
       " 'Q83. Why should you perform dimensionality reduction before fitting an SVM?',\n",
       " 'Ans. These SVMs tend to perform better in reduced space. If the number of features is large as compared to the number of observations, then we should perform dimensionality reduction before fitting an SVM.',\n",
       " 'Q84. Name the different kernels of SVM.',\n",
       " 'Ans. There are nine types of kernels in SVM.',\n",
       " '\\nPolynomial kernel\\nGaussian kernel\\nGaussian radial basis function (RBF)\\nLaplace RBF kernel\\nHyperbolic tangent kernel\\nSigmoid kernel\\nBessel function of the first kind Kernel\\nANOVA radial basis kernel\\nLinear splines kernel in one-dimension\\n',\n",
       " 'Q85. What is the Hierarchical Clustering Algorithm?',\n",
       " 'Ans. Hierarchical grouping algorithm combines and divides the groups that already exist, in this way they create a hierarchical structure that presents the order in which the groups are split or merged.',\n",
       " 'Q86. What is ‘Power Analysis’?',\n",
       " 'Ans. Power Analysis is a type of analysis used to determine what kind of effect a unit will have based simply on its size. Power Analysis can be used to estimate the minimum sample size required for an experiment and is directly related to hypothesis testing. The primary purpose underlying power analysis is to help the investigator determine the smallest sample size that is adequate to detect the effect of a certain test at the desired level of significance.',\n",
       " 'Q87. Have you contributed to any open source project?',\n",
       " 'Ans. This question seeks a continuous learning mindset. It also tells the interviewer that a candidate is curious and how well they work as a team. Good data scientists are collaborative people, sharing new ideas, knowledge, and information with each other to keep up with rapidly changing data science.',\n",
       " 'You must say specifically which projects you have worked on and what was their objective. A good answer would also include what you have learned from participating in open source projects.',\n",
       " 'Q88. How to deal with unbalanced data?',\n",
       " 'Ans. Machine learning algorithms don’t work well with imbalanced data. We can handle this data in a number of ways –\\xa0',\n",
       " '\\nUsing appropriate evaluation metrics for model generated using imbalanced data\\nResampling the training set through undersampling and oversampling\\nProperly applying cross-validation while using the over-sampling method to address imbalance problems\\nUsing more data, primarily by ensembling different resampled datasets\\nResampling with different ratios, where the best ratio majorly depends on data and models used\\nClustering the abundant class\\nDesigning your own models and be creative in using different techniques and approaches to get the best outcome\\n',\n",
       " 'Q89. How will you recover the information from a given data set? What are the most common issues in the process of information retrieval?',\n",
       " 'Ans. The recovery process is carried out through queries to the database where the structured information is stored, using a suitable interrogation language. It is necessary to take into account the key elements that allow the search to be carried out, determining a greater degree of relevance and precision, such as indexes, keywords, thesauri, and the phenomena that can occur in the process such as noise and documentary silence.',\n",
       " 'One of the most common problems that arise when searching for information is whether what we retrieve is “a lot or a little”, that is, depending on the type of search, a multitude of documents or simply a very small number can be retrieved. This phenomenon is called Silence or Documentary Noise.',\n",
       " 'Documentary silence – These are the documents stored in the database but are unrecovered, because the search strategy has been too specific or because the keywords used are not adequate to define the search.',\n",
       " 'Documentary noise – These are the document recovered by the system but are irrelevant. This usually happens when the search strategy has been defined as too generic.',\n",
       " 'Q90. What is Big Data?',\n",
       " 'Ans. Big Data is a set of massive data, a collection of huge in size and exponentially growing data, that cannot be managed, stored, and processed by traditional data management tools.',\n",
       " 'To learn more about Big Data, read our blog – What is Big Data?',\n",
       " 'Q91. What are some of the important tools used in Big Data analytics?',\n",
       " 'Ans. The important Big Data analytics tools are –\\n• NodeXL\\n• KNIME\\n• Tableau\\n• Solver\\n• OpenRefine\\n• Rattle GUI\\n• Qlikview',\n",
       " 'Q92.\\xa0What is a decision tree method?',\n",
       " 'Ans. The Decision Tree method is an analytical method that facilitates better decisions making through a schematic representation of the available alternatives. These decisions trees are very helpful when there are risks, costs, benefits, and multiple options involved. The name is derived from the appearance of the model similar to a tree and its use is widespread in the field of decision making under uncertainty (Decision Theory).',\n",
       " 'Q93.\\xa0What is the importance of the decision tree method?',\n",
       " 'Ans. The decision tree method mitigates the risks of unforeseen consequences and allows you to include smaller details that will lead you to create a step-by-step plan. Once you choose your path, you only need to follow it. Broadly speaking, this is a perfect technique for –',\n",
       " '\\nAnalyzing problems from different perspectives\\nEvaluating all possible solutions\\nEstimating the business costs of each decision\\nMaking reasoned decisions with real and existing information about any company\\nAnalyzing alternatives and probabilities that result in the success of a business\\n',\n",
       " 'Q94.\\xa0How to create a good decision tree?',\n",
       " 'Ans. Following steps are involved in developing a good decision tree –',\n",
       " '\\nIdentify the variables of a central problem\\nList all the factors causing the identified problem or risk\\nPrioritize and limit each decision criterion\\nFind and list the factors from highest to lowest importance\\nEstablish some clear variables to get some factors that include strengths and weaknesses\\nGenerate assumptions in an objective way, taking out their ramifications\\nSelect the most relevant alternatives for your business\\nImplement the alternatives consistent with your possible problems and risks\\nEvaluate the effectiveness of the decision\\n',\n",
       " 'Q95.\\xa0What is Natural Language Processing?',\n",
       " 'Ans. Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret, and manipulate human language. It focuses on the processing of human communications, dividing them into parts, and identifying the most relevant elements of the message. With the Comprehension and Generation of Natural Language, it ensures that machines can understand, interpret and manipulate human language.',\n",
       " 'Q96. Why is natural language processing important?',\n",
       " 'Ans. NLP helps computers communicate with humans in their language and scales other language-related tasks. It contributes towards structuring a highly unstructured data source.',\n",
       " 'Q97.\\xa0What is the usage of natural language processing?',\n",
       " 'Ans. There are several usages of NLP, including –',\n",
       " 'Content categorization – Generate a linguistics-based summary of the document, including search and indexing, content alerts, and duplication detection.',\n",
       " 'Discovery and modeling of themes – Accurately capture meaning and themes in text collections, and apply advanced analytics to text, such as optimization and forecasting.',\n",
       " 'Contextual extraction – Automatically extract structured information from text-based sources.',\n",
       " 'Sentiment analysis – Identification of mood or subjective opinions in large amounts of text, including sentiment mining and average opinions.',\n",
       " 'Speech-to-text and text-to-speech conversion – Transformation of voice commands into written text and vice versa.',\n",
       " 'Document summarization – Automatic generation of synopses of large bodies of text.',\n",
       " 'Machine-based translation – Automatic translation of text or speech from one language to another.',\n",
       " 'Q98. What is Ensemble Learning?',\n",
       " 'Ans.\\xa0This is among the most commonly asked data science interview questions. ',\n",
       " 'Ensemble methods is a machine learning method that contributes to combining base models to create one efficient predictive model. It boosts the overall development of the process. Ensemble learning includes two common techniques',\n",
       " 'Bagging – Bagging includes two machine-learning models, Bootstrapping and Aggregation into a single ensemble model. Here the data set is split for parallel processing of models for accuracy.',\n",
       " 'Boosting – Boosting is a sequential technique where one model is passed to another model with an aim to reduce error and create an efficient model.',\n",
       " 'Q99. What is the main difference between supervised and unsupervised machine learning?',\n",
       " 'Ans. Supervised learning includes training labeled data for a range of tasks such as data classification, while unsupervised learning does not require explicitly labeling data.',\n",
       " 'Q100. What is DBSCAN Clustering?',\n",
       " 'Ans.: \\xa0DBSCAN or density-based spatial clustering is an unsupervised approach that splits vectors into different groups basis minimum distance and a number of points in that range. There are two significant parameters in DBSCAN clustering.',\n",
       " 'Epsilon – Minimum radius or distance between the two data points',\n",
       " 'Min – Sample Points – Minimum sample number within a range to identify as one cluster.',\n",
       " 'Q101. What is data visualization?',\n",
       " 'Ans. Data visualization is the process of presenting datasets and other information through visual mediums like charts, graphs, and others. It enables the user to detect patterns, trends, and correlations that might otherwise go unnoticed in traditional reports, tables, or spreadsheets.',\n",
       " 'Q102. What is Deep Learning?',\n",
       " 'Ans. It is among the most frequently asked data science interview questions. Deep Learning is an artificial intelligence function used in decision-making. Deep Learning imitates the human brain’s functioning to process the data and create the patterns used in decision-making. Deep learning is a key technology behind automated driving, automated machine translation, automated game playing, object classification in photographs, and automated handwriting generation, among others.',\n",
       " 'Read More – What is Deep Learning?',\n",
       " 'Q103. Name different Deep Learning Frameworks.',\n",
       " 'Ans.\\na) Caffe\\nb) Chainer\\nc) Pytorch\\nd) TensorFlow\\ne) Microsoft Cognitive Toolkit\\nf) Keras',\n",
       " 'Also Explore – Machine Learning Online Courses & Certifications',\n",
       " 'We hope these data science interview questions would help you crack your next interview. Always go well-prepared and be ready to share your experience of working on different projects. All the best!',\n",
       " 'On a lighter note –',\n",
       " '',\n",
       " 'Ans. Data science is an interdisciplinary field involving scientific methods, processes, and systems to extract knowledge or a better understanding of data in various forms, whether structured or unstructured.',\n",
       " 'Ans. A data scientist is a professional who develops highly complex data analysis processes, through the design and development of algorithms that allow finding relevant findings in the information, interpreting results, and obtaining relevant conclusions, thus providing very valuable knowledge for the making strategic decisions of any company.',\n",
       " 'Ans. To become a data scientist, you should – Pursue an internship with any Data Science firm; Take up any online Data Science course, and courses that teach Statistics, Probability, and Linear Algebra; Learn about the basics of Natural Language Processing, Information Extraction, Computer Vision, Bioinformatics, and Speech Processing, etc.; Explore Optimization, Information Theory, and Decision Theory; Obtain any professional certification; Try managing databases, analyzing data, or designing the databases',\n",
       " 'Ans. Technical skills required to become a data scientist include –',\n",
       " '\\nKnowledge of Python Coding, Hadoop, Hive, BigQuery, AWS, Spark, SQL Database/Coding, Apache Spark, among others\\nKnowledge of Statistical methods and packages\\nWorking experience with Machine Learning, Multivariable Calculus & Linear Algebra\\nAbility to write codes and manage big data chunks\\nHands-on experience with real-time data and cloud computing\\nAbility to use automated tools and open-source software\\nKnowledge of Data warehousing and business intelligence platforms\\n',\n",
       " 'Ans. As per Ambitionbox, the average salary for a Data Scientist in India is Rs. 10.5 LPA.',\n",
       " 'Ans. The common job responsibilities of a data scientist are – Enhancing data collection procedures for building analytic systems; Processing, cleaning, and verifying the integrity of data; Creating automated anomaly detection systems and track their performance; Digging data from primary and secondary sources; Performing data analysis and interpret results using standard statistical methodologies; Ensuring clear data visualizations for management; Designing, creating and maintaining relevant and useful databases and data systems; Creating data dashboards, graphs, and visualizations; etc.',\n",
       " 'Ans. To work as a data scientist, you must have an undergraduate or a postgraduate degree in a relevant discipline, such as Computer science, Business information systems, Economics, Information Management, Mathematics, or Statistics.',\n",
       " 'Ans. Some of the popular data scientist roles are – Data Scientist – R/Statistical Modelling; Software Engineer – Python/R/Machine Learning; Data Analyst; Lead Python Developer; AI and Machine Learning Expert; Data Science Engineer; Manager – Machine Learning; Senior Data Scientist, Data Scientist – Machine Learning/AI; Senior Data Manager; Senior Manager – Data Scientist; Process Manager – Data Science; Applied Scientist; Principle Data and Applied Scientist, etc.',\n",
       " 'Ans. Some of the most popular recruiters for data scientists are – BFSI, Public Health, Telecommunications; Energy; Automotive; Media & Entertainment; Retail, etc.',\n",
       " 'Ans. Some of the best data science courses are –',\n",
       " '\\nhttps://learning.naukri.com/executive-data-science-specialization-course-courl402\\nhttps://learning.naukri.com/the-data-scientists-toolbox-course-courl468\\nhttps://learning.naukri.com/launching-machine-learning-delivering-operational-success-with-gold-standard-ml-leadership-course-courl509;\\nhttps://learning.naukri.com/simplilearn-data-scientist-masters-program-course-sl02https://learning.naukri.com/data-science-machine-learning-course-edxl236;\\nhttps://learning.naukri.com/python-for-data-science-and-machine-learning-bootcamp-course-udeml455\\n',\n",
       " 'Ans. Data Science is the fastest-growing job on LinkedIn and is speculated to create 11.5 million jobs by 2026. Employment opportunities are available across different industries and are among the highest paying jobs across the globe, making it a very lucrative career option.',\n",
       " 'If you have recently completed a professional course/certification, click here to submit a review.',\n",
       " 'Related Posts',\n",
       " 'HBase vs Cassandra: Which One is Better?',\n",
       " 'Most Popular Programming Languages for Data Science',\n",
       " 'Deep Learning Interview Questions and Answers for 2021',\n",
       " 'Data Analysis and Presentation Skills: the PwC Approach Specialization',\n",
       " 'XLRI Executive Program in Data Science using Python, R & Excel',\n",
       " 'Post Graduate Program in Data Science',\n",
       " 'IIT Mandi & Wiley - Post Graduate Certification in Applied AI and ML',\n",
       " \"Simplilearn Data Scientist Master's Program\",\n",
       " 'Data Science Basics',\n",
       " 'Machine Learning']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General Questions\n",
    "lst9 = []\n",
    "url = \"https://www.naukri.com/learning/articles/data-science-interview-questions-answers/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h3','p','ul'])\n",
    "for answer in answers:\n",
    "    lst9.append(answer.text)\n",
    "lst9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q1. What is the difference between data science and big data?',\n",
       " 'Ans. The common differences between data science and big data are –',\n",
       " 'Big Data',\n",
       " 'Data Science',\n",
       " 'This question is among the basic data science interview questions and you must prepare for such questions.',\n",
       " 'You may also be interested in exploring:\\xa0',\n",
       " 'Q2. How do you check for data quality?',\n",
       " 'Ans. Some of the definitions used to check for data quality are:',\n",
       " '\\nCompleteness\\nConsistency\\nUniqueness\\nIntegrity\\nConformity\\nAccuracy\\xa0\\n',\n",
       " 'Q3. Suppose you are given survey data, and it has some missing data, how would you deal with missing values \\u200b\\u200bfrom that survey?',\n",
       " 'Ans. This is among the important data science interview questions. There are two main techniques for dealing with missing values –\\xa0',\n",
       " 'Debugging Techniques – It is a Data Cleaning process consisting of evaluating the quality of the information collected, increasing its quality, in order to avoid lax analysis. The most popular debugging techniques are –\\xa0',\n",
       " 'Searching the list of values: It is about searching the data matrix for values \\u200b\\u200bthat are outside the response range. These values \\u200b\\u200bcan be considered as missing, or the correct value can be estimated from other variables',\n",
       " 'Filtering questions: It is about comparing the number of responses of a filter category and another filtered category. If any anomaly is observed that cannot be solved, it will be considered as a lost value.',\n",
       " 'Checking for Logical Consistencies: The answers that may be considered contradictory to each other are checked.',\n",
       " 'Counting the Level of representativeness: A count is made of the number of responses obtained in each variable. If the number of unanswered questions is very high, it is possible to assume equality between the answers and the non-answers or to make an imputation of the non-answer.',\n",
       " '\\nImputation Technique\\n',\n",
       " 'This technique consists of replacing the missing values \\u200b\\u200bwith valid values \\u200b\\u200bor answers by estimating them. There are three types of imputation:',\n",
       " '\\nRandom imputation\\nHot Deck imputation\\xa0\\nImputation of the mean of subclasses\\n',\n",
       " 'Q4. How would you deal with missing random values \\u200b\\u200bfrom a data set?',\n",
       " 'Ans. There are two forms of randomly missing values:',\n",
       " 'MCAR or Missing completely at random. Such errors happen when the missing values are randomly distributed across all observations.\\xa0',\n",
       " 'We can confirm this error by partitioning the data into two parts –',\n",
       " 'After we have partitioned the data, we conduct a t-test of mean difference to check if there is any difference in the sample between the two data sets.',\n",
       " 'In case the data are MCAR, we may choose a pair-wise or a list-wise deletion of missing value cases.\\xa0\\xa0\\xa0',\n",
       " 'MAR or Missing at random. It is a common occurrence. Here, the missing values are not randomly distributed across observations but are distributed within one or more sub-samples. We cannot predict the probability from the variables in the model. Data imputation is mainly performed to replace them.',\n",
       " 'Q5. What is Hadoop, and why should I care?',\n",
       " 'Ans. Hadoop is an open-source processing framework that manages data processing and storage for big data applications running on pooled systems.',\n",
       " 'Apache Hadoop is a collection of open-source utility software that makes it easy to use a network of multiple computers to solve problems involving large amounts of data and computation. It provides a software framework for distributed storage and big data processing using the MapReduce programming model.',\n",
       " 'Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packets of code to nodes to process the data in parallel. This allows the data set to be processed faster and more efficiently than if conventional supercomputing architecture were used.',\n",
       " 'Q6. What is ‘fsck’?',\n",
       " 'Ans. ‘fsck ‘ abbreviation for ‘ file system check.’ It is a type of command that searches for possible errors in the file. fsck generates a summary report, which lists the file system’s overall health and sends it to the Hadoop distributed file system.',\n",
       " 'This is among the important data science interview questions and you must prepare for the related terminologies as well.',\n",
       " 'Q7. Which is better – good data or good models?',\n",
       " 'Ans. This might be one of the frequently asked data science interview questions.',\n",
       " 'The answer to this question is very subjective and depends on the specific case. Big companies prefer good data; it is the foundation of any successful business. On the other hand, good models couldn’t be created without good data.',\n",
       " 'Based on your personal preference, you will probably choose no right or wrong answer (unless the company requires one specifically).',\n",
       " 'Q8. What are Recommender Systems?',\n",
       " 'Ans. Recommender systems are a subclass of information filtering systems, used to predict how users would rate or score particular objects (movies, music, merchandise, etc.). Recommender systems filter large volumes of information based on the data provided by a user and other factors, and they take care of the user’s preference and interest.',\n",
       " 'Recommender systems utilize algorithms that optimize the analysis of the data to build the recommendations. They ensure a high level of efficiency as they can associate elements of our consumption profiles such as purchase history, content selection, and even our hours of activity, to make accurate recommendations.',\n",
       " 'To know more about the job profile and responsibilities of a Data Scientist, refer to this article on What is Data Scientist?',\n",
       " 'Q9.\\xa0What are the different types of Recommender Systems?',\n",
       " 'Ans. There are three main types of Recommender systems.',\n",
       " 'Collaborative filtering – Collaborative filtering is a method of making automatic predictions by using the recommendations of other people. There are two types of collaborative filtering techniques –',\n",
       " '\\nUser-User collaborative filtering\\nItem-Item collaborative filtering\\n',\n",
       " 'Content-Based Filtering– Content-based filtering is based on the description of an item and a user’s choices. As the name suggests, it uses content (keywords) to describe the items, and the user profile is built to state the type of item this user likes.',\n",
       " '\\xa0',\n",
       " '',\n",
       " 'Image – Collaborative filtering & Content-based filtering',\n",
       " 'Hybrid Recommendation Systems – Hybrid Recommendation engines are a combination of diverse rating and sorting algorithms. A hybrid recommendation engine can recommend a wide range of products to consumers as per their history and preferences with precision.',\n",
       " 'Q10.\\xa0Differentiate between wide and long data formats.',\n",
       " 'Ans. In a wide format, categorical data are always grouped.',\n",
       " 'The long data format is in which there are a number of instances with many variables and subject variables.',\n",
       " 'Q11. What are Interpolation and Extrapolation?',\n",
       " 'Ans. Interpolation – This is the method to guess data points between data sets. It is a prediction between the given data points.',\n",
       " 'Extrapolation – This is the method to guess data point beyond data sets. It is a prediction beyond given data points.',\n",
       " 'Also Read>>Skills That Employers Look For In a Data Scientist',\n",
       " 'Q12. How much data is enough to get a valid outcome?',\n",
       " 'Ans. All the businesses are different and measured in different ways. Thus, you never have enough data and there will be no right answer. The amount of data required depends on the methods you use to have an excellent chance of obtaining vital results.',\n",
       " 'Q13. What is the difference between ‘expected value’ and ‘average value’?',\n",
       " 'Ans. When it comes to functionality, there is no difference between the two. However, they are used in different situations.',\n",
       " 'An expected value usually reflects random variables, while the average value reflects the population sample.',\n",
       " 'Q14. What happens if two users access the same HDFS file at the same time?',\n",
       " 'Ans. This is a bit of a tricky question. The answer itself is not complicated, but it is easy to confuse by the similarity of programs’ reactions.',\n",
       " 'When the first user is accessing the file, the second user’s inputs will be rejected because HDFS NameNode supports exclusive write.',\n",
       " 'Q15. What is power analysis?',\n",
       " 'Ans. Power analysis allows the determination of the sample size required to detect an effect of a given size with a given degree of confidence.',\n",
       " 'Q16.\\xa0Is it better to have too many false negatives or too many false positives?',\n",
       " 'Ans. This is among the popularly asked data science interview questions and will depend on how you show your viewpoint. Give examples',\n",
       " 'These are some of the popular data science interview questions. Always be prepared to answer all types of data science interview questions— technical skills, interpersonal, leadership, or methodologies. If you are someone who has recently started your career in Data Science, you can always get certified to improve your skills and boost your career opportunities.',\n",
       " 'Q17. What is the importance of statistics in data science?',\n",
       " 'Ans. Statistics help data scientists to get a better idea of a customer’s expectations. Using statistical methods, data Scientists can acquire knowledge about consumer interest, behavior, engagement, retention, etc. It also helps to build robust data models to validate certain inferences and predictions.',\n",
       " 'Q18. What are the different statistical techniques used in data science?',\n",
       " 'Ans. There are many statistical techniques used in data science, including –',\n",
       " 'The arithmetic mean – It is a measure of the average of a set of data',\n",
       " 'Graphic display – Includes charts and graphs to visually display, analyze, clarify, and interpret numerical data through histograms, pie charts, bars, etc.',\n",
       " 'Correlation – Establishes and measures relationships between different variables',\n",
       " 'Regression – Allows identifying if the evolution of one variable affects others',\n",
       " 'Time series – It predicts future values \\u200b\\u200bby analyzing sequences of past values',\n",
       " 'Data mining and other Big Data techniques to process large volumes of data',\n",
       " 'Sentiment analysis – It determines the attitude of specific agents or people towards an issue, often using data from social networks',\n",
       " 'Semantic analysis – It helps to extract knowledge from large amounts of texts',\n",
       " 'A / B testing – To determine which of two variables works best with randomized experiments',\n",
       " 'Machine learning using automatic learning algorithms to ensure excellent performance in the presence of big data',\n",
       " 'Check Out Our Data Science Courses',\n",
       " 'Q19. What is an RDBMS? Name some examples for RDBMS?',\n",
       " 'Ans.\\xa0 This is among the most frequently asked data science interview questions.',\n",
       " 'A relational database management system (RDBMS) is a database management system that is based on a relational model.',\n",
       " 'Some examples of RDBMS are MS SQL Server, IBM DB2, Oracle, MySQL, and Microsoft Access.',\n",
       " 'Interviewers often ask such data science interview questions and you must prepare for such abbreviations.',\n",
       " 'Q20. What are a Z test, Chi-Square test, F test, and T-test?',\n",
       " 'Ans. Z test is applied for large samples. Z test = (Estimated Mean – Real Mean)/ (square root real variance / n).',\n",
       " 'Chi-Square test is a statistical method assessing the goodness of fit between a set of observed values and those expected theoretically.',\n",
       " 'F-test is used to compare 2 populations’ variances. F = explained variance/unexplained variance.',\n",
       " 'T-test is applied for small samples. T-test = (Estimated Mean – Real Mean)/ (square root Estimated variance / n).',\n",
       " 'Q21. What does P-value signify about the statistical data?',\n",
       " 'Ans. The p-value is the probability for a given statistical model that, when the null hypothesis is true, the statistical summary would be the same as or more extreme than the actual observed results.',\n",
       " 'When,',\n",
       " 'P-value>0.05, it denotes weak evidence against the null hypothesis which means the null hypothesis cannot be rejected.',\n",
       " 'P-value <= 0.05 denotes strong evidence against the null hypothesis which means the null hypothesis can be rejected.',\n",
       " 'P-value=0.05is the marginal value indicating it is possible to go either way',\n",
       " 'Q22. Differentiate between univariate, bivariate, and multivariate analysis.',\n",
       " 'Ans. Univariate analysis is the simplest form of statistical analysis where only one variable is involved.',\n",
       " 'Bivariate analysis is where two variables are analyzed and in multivariate analysis, multiple variables are examined.',\n",
       " 'Q23. What is association analysis? Where is it used?',\n",
       " 'Ans. Association analysis is the task of uncovering relationships among data. It is used to understand how the data items are associated with each other.',\n",
       " 'Also Read –\\xa0 Top 6 Industries Hiring Data Scientists in 2021',\n",
       " 'Q24. What is the difference between squared error and absolute error?',\n",
       " 'Ans. Squared error measures the average of the squares of the errors or deviations—that is, the difference between the estimator and what is estimated.',\n",
       " 'Absolute error is the difference between the measured or inferred value of a quantity and its actual value.',\n",
       " 'Q25. What is an API? What are APIs used for?',\n",
       " 'Ans. API stands for Application Program Interface and is a set of routines, protocols, and tools for building software applications.',\n",
       " 'With API, it is easier to develop software applications.',\n",
       " 'Q26. What is Collaborative filtering?',\n",
       " 'Ans. Collaborative filtering is a method of making automatic predictions by using the recommendations of other people.',\n",
       " 'Q27. Why do data scientists use combinatorics or discrete probability?',\n",
       " 'Ans. It is used because it is useful in studying any predictive model.',\n",
       " 'Also Read>>How are Data Scientist and Data Analyst different?',\n",
       " 'Q28. What do you understand by Recall and Precision?',\n",
       " 'Ans. Precision is the fraction of retrieved instances that are relevant, while Recall is the fraction of relevant instances that are retrieved.',\n",
       " 'Become Machine Learning Expert Now>>',\n",
       " 'Q29. What is market basket analysis?',\n",
       " 'Ans. Market Basket Analysis is a modeling technique based upon the theory that if you buy a certain group of items, you are more (or less) likely to buy another group of items.',\n",
       " 'Q30. What is the central limit theorem?',\n",
       " 'Ans. The central limit theorem states that the distribution of an average will tend to be Normal as the sample size increases, regardless of the distribution from which the average is taken except when the moments of the parent distribution do not exist.',\n",
       " 'Q31. Explain the difference between type I and type II errors.',\n",
       " 'Ans. Type I error is the rejection of a true null hypothesis or false-positive finding, while Type II error is the non-rejection of a false null hypothesis or false-negative finding.',\n",
       " 'Q32. What is Linear Regression?',\n",
       " 'Ans. It is one of the most commonly asked networking interview questions.',\n",
       " 'Linear regression is the most popular type of predictive analysis. It is used to model the relationship between a scalar response and explanatory variables.',\n",
       " 'Q33. What are the limitations of a Linear Model/Regression?',\n",
       " 'Ans.',\n",
       " '\\nLinear models are limited to linear relationships, such as dependent and independent variables\\nLinear regression looks at a relationship between the mean of the dependent variable and the independent variables, and not the extremes of the dependent variable\\nLinear regression is sensitive to univariate or multivariate outliers\\nLinear regression tend to assume that the data are independent\\n',\n",
       " 'Q34. What is the goal of A/B Testing?',\n",
       " 'Ans. A/B testing is a comparative study, where two or more variants of a page are presented before random users and their feedback is statistically analyzed to check which variation performs better.',\n",
       " 'Q35. What is the main difference between overfitting and underfitting?',\n",
       " 'Ans. Overfitting – In overfitting, a statistical model describes any random error or noise, and occurs when a model is super complex. An overfit model has poor predictive performance as it overreacts to minor fluctuations in training data.\\nUnderfitting – In underfitting, a statistical model is unable to capture the underlying data trend. This type of model also shows poor predictive performance.',\n",
       " 'Q36. What is a Gaussian distribution and how it is used in data science?',\n",
       " 'Ans. Gaussian distribution or commonly known as bell curve is a common probability distribution curve. Mention the way it can be used in data science in a detailed manner.',\n",
       " 'Q37. Explain the purpose of group functions in SQL. Cite certain examples of group functions.',\n",
       " 'Ans. Group functions provide summary statistics of a data set. Some examples of group functions are –\\na) COUNT\\nb) MAX\\nc) MIN\\nd) AVG\\ne) SUM\\nf) DISTINCT',\n",
       " 'Q38. What is Root Cause Analysis?',\n",
       " 'Ans. Root Cause is defined as a fundamental failure of a process. To analyze such issues, a systematic approach has been devised that is known as Root Cause Analysis (RCA). This method addresses a problem or an accident and gets to its “root cause”.',\n",
       " 'Q39. What is the difference between a Validation Set and a Test Set? ',\n",
       " 'Ans. The validation set is used to minimize overfitting. This is used in parameter selection, which means that it helps to\\nverify any accuracy improvement over the training data set. Test Set is used to test and evaluate the performance of a trained Machine Learning model.',\n",
       " 'Q40. What is the Confusion Matrix?',\n",
       " 'Ans. The confusion matrix is \\u200b\\u200ba very useful tool to assess how good a classification model based on machine learning is. It is also known as an error matrix and can be presented as a summary table to evaluate the performance of a classification model. The number of correct and incorrect predictions are summarized with the count values \\u200b\\u200band broken down by each class.',\n",
       " 'The confusion matrix serves to show explicitly when one class is confused with another, which allows us to work separately with different types of errors.',\n",
       " '',\n",
       " 'Positive (P): The observation is positive (for example, it is a dog)',\n",
       " 'Negative (N): The observation is not positive (for example, it is not a dog)',\n",
       " 'True Positive (TP): Result in which the model correctly predicts the positive class',\n",
       " 'True Negative (TN): Result where the model correctly predicts the negative class',\n",
       " 'False Positive (FP): Also called a type 1 error, a result where the model incorrectly predicts the positive class when it is actually negative',\n",
       " 'False Negative (FN): Also called a type 2 error, a result in which the model incorrectly predicts the negative class when it is actually positive',\n",
       " 'Q41. What is the p-value?',\n",
       " 'Ans. A p-value helps to determine the strength of results in a hypothesis test. It is a number between 0 and 1 and Its value determines the strength of the results.',\n",
       " 'Q42. What is the difference between Causation and Correlation?',\n",
       " 'Ans. Causation denotes any causal relationship between two events and represents its cause and effects.\\nCorrelation determines the relationship between two or more variables.\\nCausation necessarily denotes the presence of correlation, but correlation doesn’t necessarily denote causation.',\n",
       " 'Q43. What is cross-validation?',\n",
       " 'Ans. Cross-validation is a technique to assess the performance of a model on a new independent dataset. One example of cross-validation could be – splitting the data into two groups – training and testing data, where you use the testing data to test the model and training data to build the model.',\n",
       " 'Q44. What do you mean by logistic regression?',\n",
       " 'Ans. Also known as the logit model, logistic regression is a technique to predict the binary result from a linear amalgamation of predictor variables.',\n",
       " 'Q45. What is ‘cluster sampling’?',\n",
       " 'Ans. Cluster sampling is a probability sampling technique where the researcher divides the population into separate groups, called clusters. Then a simple cluster sample is selected from the population. The researcher conducts his analysis of data from the sample pools.',\n",
       " 'Q46. What happens if two users access the same HDFS file at the same time?',\n",
       " 'Ans. This is a bit of a tricky question. The answer itself is not complicated, but it is easy to confuse by the similarity of programs’ reactions.',\n",
       " 'When the first user is accessing the file, the second user’s inputs will be rejected because HDFS NameNode supports exclusive write.',\n",
       " 'Q47. What are the Resampling methods?',\n",
       " 'Ans. Resampling methods are used to estimate the precision of the sample statistics, exchanging labels on data points, and validating models.',\n",
       " 'Q48. What is selection bias, and how can you avoid it?',\n",
       " 'Ans. Selection bias is an experimental error that occurs when the participant pool, or the subsequent data, is not representative of the target population.',\n",
       " 'Selection biases cannot be overcome with statistical analysis of existing data alone, though Heckman correction may be used in special cases.',\n",
       " 'Q49. What is the binomial distribution?',\n",
       " 'Ans. A binomial distribution is a discrete probability distribution that describes the number of successes when conducting independent experiments on a random variable.',\n",
       " 'Formula –',\n",
       " 'Where:',\n",
       " 'n = Number of experiments',\n",
       " 'x = Number of successes',\n",
       " 'p = Probability of success',\n",
       " 'q = Probability of failure (1-p)',\n",
       " 'Q50. What is covariance in statistics?',\n",
       " 'Ans. Covariance is a measure of the joint variability of two random variables. The covariance between two variables x and y can be calculated as follows:',\n",
       " '',\n",
       " 'Where:',\n",
       " '\\nXi – the values of the X-variable\\nYj – the values of the Y-variable\\nX̄ – the mean (average) of the X-variable\\nȲ – the mean (average) of the Y-variable\\nn – the number of data points\\n',\n",
       " 'Q51. What is Root Cause Analysis?',\n",
       " 'Ans. Root Cause Analysis (RCA) is the process of uncovering the root causes of problems to identify appropriate solutions. The RCA assumes that it is much more useful to systematically prevent and resolve underlying issues than just treating symptoms ad hoc and putting out fires.',\n",
       " 'Q52.\\xa0What is Correlation Analysis?',\n",
       " 'Ans. Correlation Analysis is a statistical method to evaluate the strength of the relationship between two quantitative variables. It consists of autocorrelation coefficients, estimated and calculated to make a different spatial relationship. It is used to correlate data based on distance.',\n",
       " 'Q53. What is imputation? List the different types of imputation techniques.',\n",
       " 'Ans. Imputation is the process that allows you to replace missing data with other values. Types of imputation techniques include –',\n",
       " 'Single Imputation: Single imputation denotes that the missing value is replaced by a value.',\n",
       " 'Hot-deck: The missing value is imputed from a similar register, which is chosen at random, based on a punched card.',\n",
       " 'Cold deck Imputation: Select donor data from other sets.',\n",
       " 'Mean Imputation: Substitute the stored value for the mean of that variable in other cases.',\n",
       " 'Mean Imputation: Its purpose is to replace the missing value with predicted values \\u200b\\u200bof a variable that is based on others.',\n",
       " 'Stochastic Regression: equal to the regression, but adds the mean regression variance to the regression imputation.',\n",
       " 'Multiple Imputation: It is a general approach to the problem of missing data, available in commonly used statistical packages. Unlike single imputation, Multiple Imputation estimates the values \\u200b\\u200bmultiple times.',\n",
       " 'Q54. What is the difference between a bar graph and a histogram?',\n",
       " 'Ans. Bar charts and histograms can be used to compare the sizes of the different groups. A bar chart is made up of bars plotted on a chart. A histogram is a graph that represents a frequency distribution; the heights of the bars represent observed frequencies.\\xa0',\n",
       " 'In other words, a histogram is a graphical display of data using bars of different heights. Generally, there is no space between adjacent bars.',\n",
       " 'Bar Charts',\n",
       " '\\nThe columns are placed on a label that represents a categorical variable.\\nThe height of the column indicates the size of the group defined by the categories\\n',\n",
       " 'Histogram',\n",
       " '\\nThe columns are placed on a label that represents a quantitative variable.\\nThe column label can be a single value or a range of values.\\n',\n",
       " 'In bar charts, each column represents a group defined by a categorical variable; and with histograms, each column represents a group defined by a quantitative variable.',\n",
       " 'Q55. Name some of the prominent resampling methods in data science.',\n",
       " 'Ans. The Bootstrap, Permutation Tests, Cross-validation, and Jackknife.',\n",
       " 'Q56. What is an Eigenvalue and Eigenvector?',\n",
       " 'Ans. Eigenvectors are used for understanding linear transformations.',\n",
       " 'Eigenvalue can be referred to as the strength of the transformation in the direction of the eigenvector or the factor by which the compression occurs.',\n",
       " 'Q57. Which technique is used to predict categorical responses?',\n",
       " 'Ans. Classification techniques are used to predict categorical responses.',\n",
       " 'Q58. What is the importance of Sampling?',\n",
       " 'Ans. Sampling is a crucial statistical technique to analyze large volumes of datasets. This involves taking out some samples that represent the entire data population. It is imperative to choose samples that are the true representatives of the whole data set. There are two types of sampling methods – Probability Sampling and Non Probability Sampling.',\n",
       " 'Q59.\\xa0Is it possible to stack two series horizontally? If yes then how will you do it?',\n",
       " 'Ans. Yes, it is possible to stack two series horizontally. We can use concat() function and setting axis = 1.',\n",
       " 'df = pd.concat([s1, s2], axis=1)',\n",
       " 'Q60.\\xa0Tell me the method to convert date-strings to timeseries in a series.',\n",
       " 'Ans.',\n",
       " 'Input:',\n",
       " 'We will use the to_datetime() function',\n",
       " '\\xa0',\n",
       " 'Explore – Python Online Courses & Certifications',\n",
       " 'Q61. Write a program in Python that takes input as the weight of the coins and produces output as the money value of the coins.',\n",
       " 'Ans. Here is an example of the code. You can change the values.',\n",
       " '',\n",
       " 'Q62. Why does Python score high over other programming languages?',\n",
       " 'Ans. This is among the very commonly asked data science interview questions. Python has a wealth of data science libraries; it is incredibly fast and easy to read and learn. The Python suite specializing in deep learning and other machine learning libraries includes popular tools such as sci-kit-learn, Keras, and TensorFlow, which allow data scientists to develop sophisticated data models directly integrated into a production system.',\n",
       " 'To discover data revelations, you will need to use Pandas, the data analysis library for Python. It can handle large amounts of data without the lag of Excel. You can do numerical modeling analysis with Numpy, do scientific computation and calculation with SciPy, and access many powerful machine learning algorithms with the Sci-Kit-learn code library. With the Python API and the iPython Notebook that comes with Anaconda, you will have robust options to visualize your data.',\n",
       " 'Q63. What are the data types used in Python?',\n",
       " 'Ans. Python has the following built-in data types:',\n",
       " '\\nNumber (float, integer)\\nString\\nTuple\\nList\\nSet\\nDictionary\\n',\n",
       " 'Numbers, strings, and tuples are immutable data types, which means that they cannot be modified at run time. Lists, sets, and dictionaries are mutable, which means they can be modified at run time.',\n",
       " 'Q64. What is a Python dictionary?',\n",
       " 'Ans. A dictionary is one of the built-in data types in Python. Defines a messy mapping of unique keys to values. Dictionaries are indexed by keys, and the values \\u200b\\u200bcan be any valid Python data type (even a user-defined class). It should be noted that dictionaries are mutable, which means that they can be modified. A dictionary is created with braces and is indexed using bracket notation.',\n",
       " 'Such common data science interview questions are often asked by the interviewers.',\n",
       " 'Q65. What libraries do data scientists use to plot data in Python?',\n",
       " 'Ans. Matplotlib is the main library used to plot data in Python. However, graphics created with this library need a lot of tweaking to make them look bright and professional. For that reason, many data scientists prefer Seaborn, which allows you to create attractive and meaningful charts with just one line of code.',\n",
       " 'Q66. Explain the difference between lists and tuples.',\n",
       " 'Ans. Both lists and tuples are made up of elements, which are values \\u200b\\u200bof any Python data type. However, these data types have a number of differences:',\n",
       " 'Lists are mutable, while tuples are immutable.',\n",
       " 'Lists are created in brackets (for example, my_list = [a, b, c]), while tuples are in parentheses (for example, my_tuple = (a, b, c)).',\n",
       " 'Lists are slower than tuples.',\n",
       " 'Q67. What are lambda functions?',\n",
       " 'Ans. Lambda functions are anonymous functions in Python. They are very useful when you need to define a function that is very short and consists of a single expression. So instead of formally defining the little function with a specific name, body, and return statement, you can write everything in a short line of code using a lambda function.',\n",
       " 'Q68.\\xa0What is PyTorch?',\n",
       " 'Ans. PyTorch is a Python-based scientific computing package designed to perform numerical calculations using the programming of tensors. It also allows its execution on GPU to speed up calculations. PyTorch is used to replace NumPy and process calculations on GPUs and for research and development in the field of machine learning, mainly focused on the development of neural networks.',\n",
       " 'PyTorch is designed to seamlessly integrate with Python and its popular libraries like NumPy and is easier to learn than other Deep Learning frameworks.\\xa0 PyTorch has a simple Python interface, provides a simple but powerful API, and provides the ability to run models in a production environment, making it a popular deep learning framework.',\n",
       " 'Q69. What are the alternatives to PyTorch?',\n",
       " 'Ans. Some of the best-known alternatives to PyTorch are –',\n",
       " 'Tensorflow – Google Brain Team developed Tensorflow, which is a free software designed for numerical computation using graphs.',\n",
       " 'Caffe – Caffe is a machine learning framework designed with the aim of being used in computer vision or image classification. Caffe is popular for its library of training models that do not require any extra implementation.',\n",
       " 'Microsoft CNTK – Microsoft CNTK is the free software framework developed by Microsoft. It is very popular in the area of \\u200b\\u200bspeech recognition although it can also be used for other fields such as text and images.',\n",
       " 'Theano – Theano is another python library. It helps to define, optimize and evaluate mathematical expressions that involve calculations with multidimensional arrays.',\n",
       " 'Keras – Keras is a high-level API for developing neural networks written in Python. It uses other libraries internally such as Tensorflow, CNTK, and Theano. It was developed to facilitate and speed up the development and experimentation with neural networks.',\n",
       " 'You may consider such data science interview questions to be basic, but such questions are the favorite of interviewers as interviewees often leave behind such data science interview questions while preparing.',\n",
       " 'Q70. What packages are used for data mining in Python and R?',\n",
       " 'Ans. There are various packages in Python and R:',\n",
       " 'Python – Orange, Pandas, NLTK, Matplotlib, and Scikit-learn are some of them.',\n",
       " 'R – Arules, tm, Forecast, and GGPlot are some of the packages.',\n",
       " 'Q71. Which would you prefer – R or Python?',\n",
       " 'Ans.\\xa0 One of the most important data science interview questions.',\n",
       " 'Both R and Python have their own pros and cons. R is mainly used when the data analysis task requires standalone computing or analysis on individual servers. Python, when your data analysis tasks need to be integrated with web apps or if statistics code needs to be incorporated into a production database.',\n",
       " 'Read More – What is Python?',\n",
       " 'Q72. Which package is used to do data import in R and Python? How do you do data import in SAS?',\n",
       " 'Ans. In R, RODBC is used for RDBMS data and data.table for fast-import.',\n",
       " 'In SAS, data and sas7bdat are used to import data.',\n",
       " 'In Python, Pandas package and the commands read_csv, read_sql are used for reading data.',\n",
       " 'Must Read – What is Machine Learning?',\n",
       " 'Q73. What are the various types of classification algorithms?',\n",
       " 'Ans. There are 7 types of classification algorithms, including –\\na) Linear Classifiers: Logistic Regression, Naive Bayes Classifier\\nb) Nearest Neighbor\\nc) Support Vector Machines\\nd) Decision Trees\\ne) Boosted Trees\\nf) Random Forest\\ng) Neural Networks',\n",
       " 'Q74. What is Gradient Descent?',\n",
       " 'Ans. Gradient Descent is a popular algorithm used for training Machine Learning models and find the values of parameters of a function (f), which helps to minimize a cost function.',\n",
       " 'Q75. What is Regularization and what kind of problems does regularization solve?',\n",
       " 'Ans. Regularization is a technique used in an attempt to solve the overfitting problem in statistical models.',\n",
       " 'It helps to solve the overfitting problem in machine learning.',\n",
       " 'Q76. What is a Boltzmann Machine?',\n",
       " 'Ans. Boltzmann Machines have a simple learning algorithm that helps to discover interesting features in training data. These machines represent complex regularities and are used to optimize the weights and the quantity for the problems.',\n",
       " 'This is one of the important data science interview questions that you must prepare for your interview. ',\n",
       " 'Q77. What is hypothesis testing?',\n",
       " 'Ans. Hypothesis testing is an important aspect of any testing procedure in Machine Learning or Data Science to analyze various factors that may have any impact on the outcome of the experiment.',\n",
       " 'Q78. \\xa0What is Pattern Recognition?',\n",
       " 'Ans. Pattern recognition is the process of data classification that includes pattern recognition and identification of data regularities. This methodology involves the extensive use of machine learning algorithms.',\n",
       " 'Q79. \\xa0Where can you use Pattern Recognition?',\n",
       " 'Ans. Pattern Recognition has multiple usabilities, across-',\n",
       " '\\nBio-Informatics\\nComputer Vision\\nData Mining\\nInformal Retrieval\\nStatistics\\nSpeech Recognition\\n',\n",
       " 'Q80. What is an Autoencoder?',\n",
       " 'Ans. These are feedforward learning networks where the input is the same as the output. Autoencoders reduce the number of dimensions in the data to encode it while ensuring minimal error and then reconstruct the output from this representation.',\n",
       " 'Also Explore – Deep Learning Online Courses & Certifications',\n",
       " 'Q81. What is the bias-variance trade-off?',\n",
       " 'Ans. Bias – Bias is the difference between the average prediction of a model and the correct value we are trying to predict.',\n",
       " 'Variance – Variance is the variability of model prediction for a given data point or a value that tells us the spread of our data.',\n",
       " 'Models with high variance focus on training data and such models perform very well on training data. On the other hand, a model with high bias doesn’t focus on training data and oversimplifies the model, leading to increased training and test data error.',\n",
       " '',\n",
       " 'Fig – Optimal balance – Bias vs. Variance (Source – towardsdatascience.com)',\n",
       " 'Q82. When do you need to update the algorithm in Data science?',\n",
       " 'Ans. You need to update an algorithm in the following situation:',\n",
       " '\\nYou want your data model to evolve as data streams using infrastructure\\nThe underlying data source is changing\\nIf it is non-stationarity\\n',\n",
       " 'Q83. Why should you perform dimensionality reduction before fitting an SVM?',\n",
       " 'Ans. These SVMs tend to perform better in reduced space. If the number of features is large as compared to the number of observations, then we should perform dimensionality reduction before fitting an SVM.',\n",
       " 'Q84. Name the different kernels of SVM.',\n",
       " 'Ans. There are nine types of kernels in SVM.',\n",
       " '\\nPolynomial kernel\\nGaussian kernel\\nGaussian radial basis function (RBF)\\nLaplace RBF kernel\\nHyperbolic tangent kernel\\nSigmoid kernel\\nBessel function of the first kind Kernel\\nANOVA radial basis kernel\\nLinear splines kernel in one-dimension\\n',\n",
       " 'Q85. What is the Hierarchical Clustering Algorithm?',\n",
       " 'Ans. Hierarchical grouping algorithm combines and divides the groups that already exist, in this way they create a hierarchical structure that presents the order in which the groups are split or merged.',\n",
       " 'Q86. What is ‘Power Analysis’?',\n",
       " 'Ans. Power Analysis is a type of analysis used to determine what kind of effect a unit will have based simply on its size. Power Analysis can be used to estimate the minimum sample size required for an experiment and is directly related to hypothesis testing. The primary purpose underlying power analysis is to help the investigator determine the smallest sample size that is adequate to detect the effect of a certain test at the desired level of significance.',\n",
       " 'Q87. Have you contributed to any open source project?',\n",
       " 'Ans. This question seeks a continuous learning mindset. It also tells the interviewer that a candidate is curious and how well they work as a team. Good data scientists are collaborative people, sharing new ideas, knowledge, and information with each other to keep up with rapidly changing data science.',\n",
       " 'You must say specifically which projects you have worked on and what was their objective. A good answer would also include what you have learned from participating in open source projects.',\n",
       " 'Q88. How to deal with unbalanced data?',\n",
       " 'Ans. Machine learning algorithms don’t work well with imbalanced data. We can handle this data in a number of ways –\\xa0',\n",
       " '\\nUsing appropriate evaluation metrics for model generated using imbalanced data\\nResampling the training set through undersampling and oversampling\\nProperly applying cross-validation while using the over-sampling method to address imbalance problems\\nUsing more data, primarily by ensembling different resampled datasets\\nResampling with different ratios, where the best ratio majorly depends on data and models used\\nClustering the abundant class\\nDesigning your own models and be creative in using different techniques and approaches to get the best outcome\\n',\n",
       " 'Q89. How will you recover the information from a given data set? What are the most common issues in the process of information retrieval?',\n",
       " 'Ans. The recovery process is carried out through queries to the database where the structured information is stored, using a suitable interrogation language. It is necessary to take into account the key elements that allow the search to be carried out, determining a greater degree of relevance and precision, such as indexes, keywords, thesauri, and the phenomena that can occur in the process such as noise and documentary silence.',\n",
       " 'One of the most common problems that arise when searching for information is whether what we retrieve is “a lot or a little”, that is, depending on the type of search, a multitude of documents or simply a very small number can be retrieved. This phenomenon is called Silence or Documentary Noise.',\n",
       " 'Documentary silence – These are the documents stored in the database but are unrecovered, because the search strategy has been too specific or because the keywords used are not adequate to define the search.',\n",
       " 'Documentary noise – These are the document recovered by the system but are irrelevant. This usually happens when the search strategy has been defined as too generic.',\n",
       " 'Q90. What is Big Data?',\n",
       " 'Ans. Big Data is a set of massive data, a collection of huge in size and exponentially growing data, that cannot be managed, stored, and processed by traditional data management tools.',\n",
       " 'To learn more about Big Data, read our blog – What is Big Data?',\n",
       " 'Q91. What are some of the important tools used in Big Data analytics?',\n",
       " 'Ans. The important Big Data analytics tools are –\\n• NodeXL\\n• KNIME\\n• Tableau\\n• Solver\\n• OpenRefine\\n• Rattle GUI\\n• Qlikview',\n",
       " 'Q92.\\xa0What is a decision tree method?',\n",
       " 'Ans. The Decision Tree method is an analytical method that facilitates better decisions making through a schematic representation of the available alternatives. These decisions trees are very helpful when there are risks, costs, benefits, and multiple options involved. The name is derived from the appearance of the model similar to a tree and its use is widespread in the field of decision making under uncertainty (Decision Theory).',\n",
       " 'Q93.\\xa0What is the importance of the decision tree method?',\n",
       " 'Ans. The decision tree method mitigates the risks of unforeseen consequences and allows you to include smaller details that will lead you to create a step-by-step plan. Once you choose your path, you only need to follow it. Broadly speaking, this is a perfect technique for –',\n",
       " '\\nAnalyzing problems from different perspectives\\nEvaluating all possible solutions\\nEstimating the business costs of each decision\\nMaking reasoned decisions with real and existing information about any company\\nAnalyzing alternatives and probabilities that result in the success of a business\\n',\n",
       " 'Q94.\\xa0How to create a good decision tree?',\n",
       " 'Ans. Following steps are involved in developing a good decision tree –',\n",
       " '\\nIdentify the variables of a central problem\\nList all the factors causing the identified problem or risk\\nPrioritize and limit each decision criterion\\nFind and list the factors from highest to lowest importance\\nEstablish some clear variables to get some factors that include strengths and weaknesses\\nGenerate assumptions in an objective way, taking out their ramifications\\nSelect the most relevant alternatives for your business\\nImplement the alternatives consistent with your possible problems and risks\\nEvaluate the effectiveness of the decision\\n',\n",
       " 'Q95.\\xa0What is Natural Language Processing?',\n",
       " 'Ans. Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret, and manipulate human language. It focuses on the processing of human communications, dividing them into parts, and identifying the most relevant elements of the message. With the Comprehension and Generation of Natural Language, it ensures that machines can understand, interpret and manipulate human language.',\n",
       " 'Q96. Why is natural language processing important?',\n",
       " 'Ans. NLP helps computers communicate with humans in their language and scales other language-related tasks. It contributes towards structuring a highly unstructured data source.',\n",
       " 'Q97.\\xa0What is the usage of natural language processing?',\n",
       " 'Ans. There are several usages of NLP, including –',\n",
       " 'Content categorization – Generate a linguistics-based summary of the document, including search and indexing, content alerts, and duplication detection.',\n",
       " 'Discovery and modeling of themes – Accurately capture meaning and themes in text collections, and apply advanced analytics to text, such as optimization and forecasting.',\n",
       " 'Contextual extraction – Automatically extract structured information from text-based sources.',\n",
       " 'Sentiment analysis – Identification of mood or subjective opinions in large amounts of text, including sentiment mining and average opinions.',\n",
       " 'Speech-to-text and text-to-speech conversion – Transformation of voice commands into written text and vice versa.',\n",
       " 'Document summarization – Automatic generation of synopses of large bodies of text.',\n",
       " 'Machine-based translation – Automatic translation of text or speech from one language to another.',\n",
       " 'Q98. What is Ensemble Learning?',\n",
       " 'Ans.\\xa0This is among the most commonly asked data science interview questions. ',\n",
       " 'Ensemble methods is a machine learning method that contributes to combining base models to create one efficient predictive model. It boosts the overall development of the process. Ensemble learning includes two common techniques',\n",
       " 'Bagging – Bagging includes two machine-learning models, Bootstrapping and Aggregation into a single ensemble model. Here the data set is split for parallel processing of models for accuracy.',\n",
       " 'Boosting – Boosting is a sequential technique where one model is passed to another model with an aim to reduce error and create an efficient model.',\n",
       " 'Q99. What is the main difference between supervised and unsupervised machine learning?',\n",
       " 'Ans. Supervised learning includes training labeled data for a range of tasks such as data classification, while unsupervised learning does not require explicitly labeling data.',\n",
       " 'Q100. What is DBSCAN Clustering?',\n",
       " 'Ans.: \\xa0DBSCAN or density-based spatial clustering is an unsupervised approach that splits vectors into different groups basis minimum distance and a number of points in that range. There are two significant parameters in DBSCAN clustering.',\n",
       " 'Epsilon – Minimum radius or distance between the two data points',\n",
       " 'Min – Sample Points – Minimum sample number within a range to identify as one cluster.',\n",
       " 'Q101. What is data visualization?',\n",
       " 'Ans. Data visualization is the process of presenting datasets and other information through visual mediums like charts, graphs, and others. It enables the user to detect patterns, trends, and correlations that might otherwise go unnoticed in traditional reports, tables, or spreadsheets.',\n",
       " 'Q102. What is Deep Learning?',\n",
       " 'Ans. It is among the most frequently asked data science interview questions. Deep Learning is an artificial intelligence function used in decision-making. Deep Learning imitates the human brain’s functioning to process the data and create the patterns used in decision-making. Deep learning is a key technology behind automated driving, automated machine translation, automated game playing, object classification in photographs, and automated handwriting generation, among others.',\n",
       " 'Read More – What is Deep Learning?',\n",
       " 'Q103. Name different Deep Learning Frameworks.',\n",
       " 'Ans.\\na) Caffe\\nb) Chainer\\nc) Pytorch\\nd) TensorFlow\\ne) Microsoft Cognitive Toolkit\\nf) Keras']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new9 = lst9[5:369]\n",
    "lst_new9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the difference between data science and big data?</td>\n",
       "      <td>Ans. The common differences between data science and big data are –Big DataData ScienceThis question is among the basic data science interview questions and you must prepare for such questions.You may also be interested in exploring:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you check for data quality?</td>\n",
       "      <td>Ans. Some of the definitions used to check for data quality are: Completeness Consistency Uniqueness Integrity Conformity Accuracy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suppose you are given survey data, and it has some missing data, how would you deal with missing values ​​from that survey?</td>\n",
       "      <td>Ans. This is among the important data science interview questions. There are two main techniques for dealing with missing values – Debugging Techniques – It is a Data Cleaning process consisting of evaluating the quality of the information collected, increasing its quality, in order to avoid lax analysis. The most popular debugging techniques are – Searching the list of values: It is about searching the data matrix for values ​​that are outside the response range. These values ​​can be considered as missing, or the correct value can be estimated from other variablesFiltering questions: It is about comparing the number of responses of a filter category and another filtered category. If any anomaly is observed that cannot be solved, it will be considered as a lost value.Checking for Logical Consistencies: The answers that may be considered contradictory to each other are checked.Counting the Level of representativeness: A count is made of the number of responses obtained in each variable. If the number of unanswered questions is very high, it is possible to assume equality between the answers and the non-answers or to make an imputation of the non-answer. Imputation Technique This technique consists of replacing the missing values ​​with valid values ​​or answers by estimating them. There are three types of imputation: Random imputation Hot Deck imputation  Imputation of the mean of subclasses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How would you deal with missing random values ​​from a data set?</td>\n",
       "      <td>Ans. There are two forms of randomly missing values:MCAR or Missing completely at random. Such errors happen when the missing values are randomly distributed across all observations. We can confirm this error by partitioning the data into two parts –After we have partitioned the data, we conduct a t-test of mean difference to check if there is any difference in the sample between the two data sets.In case the data are MCAR, we may choose a pair-wise or a list-wise deletion of missing value cases.   MAR or Missing at random. It is a common occurrence. Here, the missing values are not randomly distributed across observations but are distributed within one or more sub-samples. We cannot predict the probability from the variables in the model. Data imputation is mainly performed to replace them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Hadoop, and why should I care?</td>\n",
       "      <td>Ans. Hadoop is an open-source processing framework that manages data processing and storage for big data applications running on pooled systems.Apache Hadoop is a collection of open-source utility software that makes it easy to use a network of multiple computers to solve problems involving large amounts of data and computation. It provides a software framework for distributed storage and big data processing using the MapReduce programming model.Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packets of code to nodes to process the data in parallel. This allows the data set to be processed faster and more efficiently than if conventional supercomputing architecture were used.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       Questions  \\\n",
       "0                                                                      What is the difference between data science and big data?   \n",
       "1                                                                                             How do you check for data quality?   \n",
       "2    Suppose you are given survey data, and it has some missing data, how would you deal with missing values ​​from that survey?   \n",
       "3                                                               How would you deal with missing random values ​​from a data set?   \n",
       "4                                                                                         What is Hadoop, and why should I care?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Ans. The common differences between data science and big data are –Big DataData ScienceThis question is among the basic data science interview questions and you must prepare for such questions.You may also be interested in exploring:   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Ans. Some of the definitions used to check for data quality are: Completeness Consistency Uniqueness Integrity Conformity Accuracy    \n",
       "2  Ans. This is among the important data science interview questions. There are two main techniques for dealing with missing values – Debugging Techniques – It is a Data Cleaning process consisting of evaluating the quality of the information collected, increasing its quality, in order to avoid lax analysis. The most popular debugging techniques are – Searching the list of values: It is about searching the data matrix for values ​​that are outside the response range. These values ​​can be considered as missing, or the correct value can be estimated from other variablesFiltering questions: It is about comparing the number of responses of a filter category and another filtered category. If any anomaly is observed that cannot be solved, it will be considered as a lost value.Checking for Logical Consistencies: The answers that may be considered contradictory to each other are checked.Counting the Level of representativeness: A count is made of the number of responses obtained in each variable. If the number of unanswered questions is very high, it is possible to assume equality between the answers and the non-answers or to make an imputation of the non-answer. Imputation Technique This technique consists of replacing the missing values ​​with valid values ​​or answers by estimating them. There are three types of imputation: Random imputation Hot Deck imputation  Imputation of the mean of subclasses   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Ans. There are two forms of randomly missing values:MCAR or Missing completely at random. Such errors happen when the missing values are randomly distributed across all observations. We can confirm this error by partitioning the data into two parts –After we have partitioned the data, we conduct a t-test of mean difference to check if there is any difference in the sample between the two data sets.In case the data are MCAR, we may choose a pair-wise or a list-wise deletion of missing value cases.   MAR or Missing at random. It is a common occurrence. Here, the missing values are not randomly distributed across observations but are distributed within one or more sub-samples. We cannot predict the probability from the variables in the model. Data imputation is mainly performed to replace them.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Ans. Hadoop is an open-source processing framework that manages data processing and storage for big data applications running on pooled systems.Apache Hadoop is a collection of open-source utility software that makes it easy to use a network of multiple computers to solve problems involving large amounts of data and computation. It provides a software framework for distributed storage and big data processing using the MapReduce programming model.Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packets of code to nodes to process the data in parallel. This allows the data set to be processed faster and more efficiently than if conventional supercomputing architecture were used.  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^Q\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new9:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"^Q\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df9=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df9.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df9[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n \\n\\n',\n",
       " 'Data Science is one of the hottest jobs today. According to LinkedIn, the Data Scientist jobs are among the top 10 jobs in the United States. According to The Economic Times, the job postings for the  Data Science profiles  have grown over 400 times over the past one year. So, it is obvious that companies today survive on data, and Data Scientists are the rockstars of this era. So, if you want to start your career as a Data Scientist, you must be wondering what sort of questions are asked in the Data Science interview. So, in this interview preparation blog, we will be going through Data Science interview questions and answers.\\n',\n",
       " 'Categories',\n",
       " 'Automation',\n",
       " 'Big Data',\n",
       " 'Business Intelligence',\n",
       " 'Cloud Computing',\n",
       " 'Cyber Security',\n",
       " 'Data Science',\n",
       " 'Database',\n",
       " 'Digital Marketing',\n",
       " 'Mobile Development',\n",
       " 'No-SQL',\n",
       " 'Programming',\n",
       " 'Project Management',\n",
       " 'Salesforce',\n",
       " 'Testing',\n",
       " 'Website Development',\n",
       " 'CTA',\n",
       " 'Data Science is among the leading and most popular technologies in the world today. Major organizations are hiring professionals in this field. With high demand and low availability of these professionals, Data Scientists are among the highest-paid IT professionals. This Data Science Interview preparation blog includes most frequently asked questions in Data Science job interviews. Here is a list of these popular Data Science interview questions:\\nQ1. What do you understand by linear regression?\\nQ2. What do you understand by logistic regression?\\nQ3. What is a confusion matrix?\\nQ4. What do you understand by true positive rate and false positive rate?\\nQ5. What is Data Science?\\nQ6. How is Data Science different from traditional application programming?\\nQ7. Explain the differences between supervised and unsupervised learning.\\nQ8. What is dimensionality reduction?\\nQ9. What is bias in Data Science?\\nQ10. What is variance in Data Science?\\nFollowing are the three categories into which these Data Science interview questions are divided:\\n1. Basic\\n2. Intermediate\\n3. Advanced\\nCheck out this video on Data Science Interview Questions:\\n Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBasic Data Science Interview Questions\\n\\n',\n",
       " 'Data Science is among the leading and most popular technologies in the world today. Major organizations are hiring professionals in this field. With high demand and low availability of these professionals, Data Scientists are among the highest-paid IT professionals. This Data Science Interview preparation blog includes most frequently asked questions in Data Science job interviews. Here is a list of these popular Data Science interview questions:',\n",
       " 'Q1. What do you understand by linear regression?\\nQ2. What do you understand by logistic regression?\\nQ3. What is a confusion matrix?\\nQ4. What do you understand by true positive rate and false positive rate?\\nQ5. What is Data Science?\\nQ6. How is Data Science different from traditional application programming?\\nQ7. Explain the differences between supervised and unsupervised learning.\\nQ8. What is dimensionality reduction?\\nQ9. What is bias in Data Science?\\nQ10. What is variance in Data Science?',\n",
       " 'Following are the three categories into which these Data Science interview questions are divided:\\n1. Basic',\n",
       " '2. Intermediate',\n",
       " '3. Advanced',\n",
       " 'Check out this video on Data Science Interview Questions:',\n",
       " ' Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '1. What do you understand by linear regression?',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.\\nInterested in learning Data Science? Click here to learn more in this Data Science Course!\\n\\n',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.',\n",
       " 'Interested in learning Data Science? Click here to learn more in this Data Science Course!',\n",
       " '2. What do you understand by logistic regression?',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.\\n\\n',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.',\n",
       " '3. What is a confusion matrix?',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.\\n',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.',\n",
       " 'CTA',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:\\n Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:',\n",
       " ' Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '4. What do you understand by true positive rate and false positive rate?',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.\\nCheck out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.',\n",
       " 'Check out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Get 50% Hike!',\n",
       " 'Master Most in Demand Skills Now !',\n",
       " '5. What is Data Science?',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.\\n\\n',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.',\n",
       " '6. How is Data Science different from traditional application programming?',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.\\nIn traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.\\nData Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).\\nAs described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.\\n\\n\\n',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.',\n",
       " 'In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.',\n",
       " 'Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).',\n",
       " 'As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.',\n",
       " '',\n",
       " '7. Explain the differences between supervised and unsupervised learning.',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.\\n\\n\\n\\n\\nSupervised Learning\\nUnsupervised Learning\\n\\n\\nWorks on the data that contains both inputs and the expected output, i.e., the labeled data\\nWorks on the data that contains no mappings from input to output, i.e., the unlabeled data\\n\\n\\nUsed to create models that can be employed to predict or classify things\\nUsed to extract meaningful information out of large volumes of data\\n\\n\\nCommonly used supervised learning algorithms: Linear regression, decision tree, etc.\\nCommonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.\\n\\n\\n\\n\\n\\n',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.',\n",
       " '8. What is dimensionality reduction?',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.\\n\\n',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.',\n",
       " '9. What is bias in Data Science?',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.\\n\\n',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.',\n",
       " '10. Why Python is used for Data Cleaning in DS?',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0\\nPython libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. \\nLearn more about Data Cleaning in Data Science Tutorial!\\n',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0',\n",
       " 'Python libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. ',\n",
       " 'Learn more about Data Cleaning in Data Science Tutorial!',\n",
       " '11. Why R is used in Data Visualization?',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0\\nIt has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0',\n",
       " 'It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '12. What are the popular libraries used in Data Science?',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:\\n\\nTensorFlow: Supports parallel computing with impeccable library management backed by Google.\\xa0\\nSciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts.\\nPandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications.\\nMatplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.\\xa0\\nPyTorch: Best for projects which involve Machine Learning algorithms and Deep Neural Networks.\\xa0\\n\\nInterested to learn more about Data Science, check out our\\xa0Data Science Course in New York!\\n',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:',\n",
       " 'Interested to learn more about Data Science, check out our\\xa0Data Science Course in New York!',\n",
       " '13. What is variance in Data Science?',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.\\n',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.',\n",
       " '14. What is pruning in a decision tree algorithm?',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n\\n',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n',\n",
       " '15. What is entropy in a decision tree algorithm?',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.\\nFor example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.\\n',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.',\n",
       " 'For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.',\n",
       " '16. What is information gain in a decision tree algorithm?',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. \\n',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. ',\n",
       " '17. What is k-fold cross-validation?',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.\\n',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.',\n",
       " '18. Explain how a recommender system works.',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.\\nFor example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.',\n",
       " 'For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '19. What is a normal distribution?',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.\\n',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.',\n",
       " '20. What is Deep Learning?',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '21. What is an RNN (recurrent neural network)?',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.\\n',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.',\n",
       " '22. Explain selection bias.',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.\\n\\nIntermediate Data Science Interview Questions\\n',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.',\n",
       " '23. What is ROC curve?',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: \\n',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: ',\n",
       " '24. What do you understand by a decision tree?',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.\\nAre you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!\\n',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.',\n",
       " 'Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!',\n",
       " '25. What do you understand by a random forest model?',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.\\n\\n',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.',\n",
       " '',\n",
       " '26. Two candidates Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least of them will crack the interview?',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12\\nNow, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)\\nWhere P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)\\n1/8 * 5/12\\n5/96\\nNow, put the value of P(A ∩ B) into equation 1\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B))\\n1/8 + 5/12 -5/96\\nSo, the answer will be 47/96.\\n',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12',\n",
       " 'Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)',\n",
       " 'Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)',\n",
       " '1/8 * 5/12',\n",
       " '5/96',\n",
       " 'Now, put the value of P(A ∩ B) into equation 1',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B))',\n",
       " '1/8 + 5/12 -5/96',\n",
       " 'So, the answer will be 47/96.',\n",
       " '27. How is Data modeling different from Database design?',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.\\n',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.',\n",
       " '28. What are precision?',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:\\n\\n',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:',\n",
       " '',\n",
       " '29. What is recall?',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:\\n\\n',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:',\n",
       " '',\n",
       " '30. What is the F1 score and how to calculate it?',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: \\n',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: ',\n",
       " '31. What is p-value? ',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.\\n',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.',\n",
       " '32. Why do we use p-value?',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:\\n\\n',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:',\n",
       " '',\n",
       " '33. What is the difference between an error and a residual error?',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.\\n',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.',\n",
       " '34. Why do we use the summary function?',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.\\n',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.',\n",
       " '35. How are Data Science and Machine Learning related to each other?',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.\\nData Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.\\nMachine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.\\nIn short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.',\n",
       " 'Data Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.',\n",
       " 'Machine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.',\n",
       " 'In short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '36. Explain univariate, bivariate, and multivariate analyses.',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.\\n\\nUnivariate analysis: Univariate analysis involves analysing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends out of it. Example: Analyzing the weight of a group of people.\\nBivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude.\\nMultivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable). Example: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc.\\n\\n',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.',\n",
       " '37. How can we handle missing data?',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.\\nOne way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.\\nAnother way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.\\nFinally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.\\n',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.',\n",
       " 'One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.',\n",
       " 'Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.',\n",
       " 'Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.',\n",
       " '38. What is the benefit of dimensionality reduction?',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.\\n',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.',\n",
       " '39. What is bias–variance trade-off in Data Science?',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.\\nBias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.\\n',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.',\n",
       " 'Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.',\n",
       " '40. What is RMSE?',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:\\nFirst, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.\\n',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:',\n",
       " 'First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.',\n",
       " '41. What is a kernel function in SVM?',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.\\n',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.',\n",
       " '42. How can we select an appropriate value of k in k-means?',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.\\nThis is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.\\n',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.',\n",
       " 'This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.',\n",
       " '43. How can we deal with outliers?',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.\\nIn case the outliers are not that extreme, then we can try:\\n\\nA different kind of model. For example, if we were using a linear model, then we can choose a non-linear model\\nNormalizing the data, which will shift the extreme values closer to other data points\\nUsing algorithms that are not so affected by outliers, such as random forest, etc.\\n\\n',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.',\n",
       " 'In case the outliers are not that extreme, then we can try:',\n",
       " '44. How to calculate the accuracy of a binary classification algorithm using its confusion matrix?',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:\\n\\nTrue positives: Number of observations correctly classified as True\\nTrue negatives: Number of observations correctly classified as False\\nFalse positives: Number of observations incorrectly classified as True\\nFalse negatives: Number of observations incorrectly classified as False\\n\\nTo calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:\\n',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:',\n",
       " 'To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:',\n",
       " '45. What is ensemble learning?',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.\\n',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.',\n",
       " '46. Explain collaborative filtering in recommender systems.',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.\\n',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.',\n",
       " '47. Explain content-based filtering in recommender systems.',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.\\n',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.',\n",
       " '48. Explain bagging in Data Science.',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.\\n',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.',\n",
       " '49. Explain boosting in Data Science.',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.\\n',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.',\n",
       " '50. Explain stacking in Data Science.',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.\\n',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.',\n",
       " '51. Explain how Machine Learning is different from Deep Learning.',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.\\nDeep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.\\n',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.',\n",
       " 'Deep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.',\n",
       " '52. Why does Naive Bayes have the word ‘naive’ in it?',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.\\nIt has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.\\nTo learn more about Data Science, check out our Data Science Course in Hyderabad.\\nAdvanced Data Science Interview Questions\\n',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.',\n",
       " 'It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.',\n",
       " 'To learn more about Data Science, check out our Data Science Course in Hyderabad.',\n",
       " '53. From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.',\n",
       " '\\nFirst, we will load the ggplot2 package:\\nlibrary(ggplot2)\\nNext, we will use the dplyr package:\\nlibrary(dplyr)// It is based on the grammar of data manipulation.\\nTo extract those particular records, use the below command:\\ndiamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea\\n',\n",
       " '',\n",
       " 'First, we will load the ggplot2 package:',\n",
       " 'Next, we will use the dplyr package:',\n",
       " 'To extract those particular records, use the below command:',\n",
       " '54. Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on y-axis, ’carat’ should be on x-axis, and the ‘color’ of the points should be determined by ‘cut.’',\n",
       " 'We will implement the scatter plot using ggplot.\\nThe ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.\\nSo, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.\\nCode:\\n>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()\\n',\n",
       " 'We will implement the scatter plot using ggplot.',\n",
       " 'The ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.',\n",
       " 'So, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.',\n",
       " 'Code:',\n",
       " '55. Introduce 25 percent missing values in this ‘iris’ datset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’',\n",
       " '\\nTo introduce missing values, we will be using the missForest package:\\nlibrary(missForest)\\nUsing the prodNA function, we will be introducing 25 percent of missing values:\\nIris.mis<-prodNA(iris,noNA=0.25)\\nFor imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:\\nlibrary(Hmisc)\\r\\niris.mis$Sepal.Length<-with(iris.mis, impute(Sepal.Length,mean))\\r\\niris.mis$Petal.Length<-with(iris.mis, impute(Petal.Length,median))\\n',\n",
       " '',\n",
       " 'To introduce missing values, we will be using the missForest package:',\n",
       " 'Using the prodNA function, we will be introducing 25 percent of missing values:',\n",
       " 'For imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:',\n",
       " '56. Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’',\n",
       " '\\nHere, we need to find how ‘mpg’ varies w.r.t displacement of the column.\\nWe need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.\\nSo, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.\\nTherefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.\\nHere, we will use the following code:\\nlibraray(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)\\r\\n\\r\\nmtcars[split_tag,]->train\\r\\n\\r\\nmtcars[-split_tag,]->test\\r\\n\\r\\nlm(mpg-data,data=train)->mod_mtcars\\r\\n\\r\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\r\\n\\r\\n>head(pred_mtcars)\\nExplanation:\\nParameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).\\nSecond is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.\\nOnce we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.\\nSimilarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.\\nSo, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.\\nWe will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.\\nlm(mpg-data,data=train)->mod_mtcars\\nNow, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.\\nThus, we have to predict values for the test set and then store them in pred_mtcars.\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\nOutput:\\nThese are the predicted values of mpg for all of these cars.\\n\\nSo, this is how we can build simple linear model on top of this mtcars dataset.\\n',\n",
       " '',\n",
       " 'Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.',\n",
       " 'We need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.',\n",
       " 'So, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.',\n",
       " 'Therefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.',\n",
       " 'Here, we will use the following code:',\n",
       " 'Explanation:',\n",
       " 'Parameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).',\n",
       " 'Second is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.',\n",
       " 'Once we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.',\n",
       " 'Similarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.',\n",
       " 'So, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.',\n",
       " 'We will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.',\n",
       " 'Now, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.',\n",
       " 'Thus, we have to predict values for the test set and then store them in pred_mtcars.',\n",
       " 'Output:',\n",
       " 'These are the predicted values of mpg for all of these cars.',\n",
       " '',\n",
       " 'So, this is how we can build simple linear model on top of this mtcars dataset.',\n",
       " '57. Calculate the RMSE values for the model built.',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\r\\n\\r\\nas.data.frame(final_data)->final_data\\r\\n\\r\\nerror<-(final_data$Actual-final_data$Prediction)\\r\\n\\r\\ncbind(final_data,error)->final_data\\r\\n\\r\\nsqrt(mean(final_data$error)^2)\\nExplanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\nOur actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:\\nas.data.frame(final_data)->final_data\\nWe will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:\\nerror<-(final_data$Actual-final_data$Prediction)\\nThen, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:\\ncbind(final_data,error)->final_data //binding error object to this final_data\\nHere, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:\\nSqrt(mean(final_data$error)^2)\\nOutput:\\n[1] 4.334423\\nNote: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.\\n',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:',\n",
       " 'Explanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:',\n",
       " 'Our actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:',\n",
       " 'We will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:',\n",
       " 'Then, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:',\n",
       " 'Here, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:',\n",
       " 'Output:',\n",
       " 'Note: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.',\n",
       " '58. Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’',\n",
       " 'Simple Linear Regression\\nimport pandas as pd\\r\\n\\r\\ndata=pd.read_csv(‘Boston.csv’)\\xa0\\xa0\\xa0\\xa0 //loading the Boston dataset\\r\\n\\r\\ndata.head()\\xa0 //having a glance at the head of this data\\r\\n\\r\\ndata.shape\\r\\n\\r\\n\\nLet us take out the dependent and the independent variables from the dataset:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\r\\n\\r\\ndata1.head()\\nVisualizing Variables\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nHere, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.\\nWe will separate the dependent and the independent variable from this entire dataframe:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\nThe only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.\\nNow, we would also do a visualization w.r.t to these two columns:\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nPreparing the Data\\nX=pd.Dataframe(data1[‘lstat’])\\r\\n\\r\\nY=pd.Dataframe(data1[‘medv’])\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\\r\\n\\r\\nfrom sklearn.linear_model import LinearRegression\\r\\n\\r\\nregressor=LinearRegression()\\r\\n\\r\\nregressor.fit(X_train,y_train)\\nprint(regressor.intercept_)\\nOutput :\\n34.12654201\\nprint(regressor.coef_)//this is the slope\\nOutput :\\n[[-0.913293]]\\nBy now, we have built the model. Now, we have to predict the values on top of the test set:\\ny_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in y_pred object\\r\\n\\r\\n\\nNow, let’s have a glance at the rows and columns of the actual values and the predicted values:\\nY_pred.shape, y_test.shape\\nOutput :\\n((102,1),(102,1))\\nFurther, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.\\nfrom sklearn import metrics import NumPy as np\\r\\n\\r\\nprint(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\nOutput:\\nMean Absolute Error: 4.692198\\r\\n\\r\\nMean Squared Error: 43.9198\\r\\n\\r\\nRoot Mean Squared Error: 6.6270\\n\\n',\n",
       " 'Simple Linear Regression',\n",
       " 'Let us take out the dependent and the independent variables from the dataset:',\n",
       " 'Visualizing Variables',\n",
       " 'Here, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.',\n",
       " 'We will separate the dependent and the independent variable from this entire dataframe:',\n",
       " 'The only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.',\n",
       " 'Now, we would also do a visualization w.r.t to these two columns:',\n",
       " 'Preparing the Data',\n",
       " 'Output :',\n",
       " 'Output :',\n",
       " 'By now, we have built the model. Now, we have to predict the values on top of the test set:',\n",
       " 'Now, let’s have a glance at the rows and columns of the actual values and the predicted values:',\n",
       " 'Output :',\n",
       " 'Further, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.',\n",
       " 'Output:',\n",
       " '',\n",
       " '59. Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’',\n",
       " '\\nFor loading the dataset, we will use the read.csv function:\\nread.csv(“D:/heart.csv”)->heart\\r\\n\\r\\nstr(heart)\\nIn the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:\\nas.factor(heart$target)->heart$target\\nNow, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.\\nTo build a logistic regression model, we will use the glm function:\\nglm(target~age, data=heart, family=”binomial”)->log_mod1\\nHere, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.\\nfamily=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.\\nWe will have a glance at the summary of the model that we have just built:\\nsummary(log_mod1)\\n\\nWe can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.\\nNow, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.\\nThis null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.\\nResidual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.\\nThis basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.\\nAs we have built the model, it’s time to predict some values:\\npredict(log_mod1, data.frame(age=30), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=50), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=29:77), type=”response”)\\r\\n\\r\\n\\nNow, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:\\n>library(caret)\\r\\n\\r\\nSplit_tag<- createDataPartition(heart$target, p=0.70, list=F)\\r\\n\\r\\nheart[split_tag,]->train\\r\\n\\r\\nheart[-split_tag,]->test\\r\\n\\r\\nglm(target~age, data=train,family=”binomial”)->log_mod2\\r\\n\\r\\npredict(log_mod2, newdata=test, type=”response”)->pred_heart\\r\\n\\r\\nrange(pred_heart)\\n',\n",
       " '',\n",
       " 'For loading the dataset, we will use the read.csv function:',\n",
       " 'In the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:',\n",
       " 'Now, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.',\n",
       " 'To build a logistic regression model, we will use the glm function:',\n",
       " 'Here, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.',\n",
       " 'family=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.',\n",
       " 'We will have a glance at the summary of the model that we have just built:',\n",
       " '',\n",
       " 'We can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.',\n",
       " 'Now, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.',\n",
       " 'This null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.',\n",
       " 'Residual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.',\n",
       " 'This basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.',\n",
       " 'As we have built the model, it’s time to predict some values:',\n",
       " 'Now, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:',\n",
       " '60. Build an ROC curve for the model built.',\n",
       " 'The below code will help us in building the ROC curve:\\nlibrary(ROCR)\\r\\n\\r\\nprediction(pred_heart, test$target)-> roc_pred_heart\\r\\n\\r\\nperformance(roc_pred_heart, “tpr”, “fpr”)->roc_curve\\r\\n\\r\\nplot(roc_curve, colorize=T)\\nGraph: \\nGo through this Data Science Course in London to get a clear understanding of Data Science!\\n\\n',\n",
       " 'The below code will help us in building the ROC curve:',\n",
       " 'Graph: ',\n",
       " 'Go through this Data Science Course in London to get a clear understanding of Data Science!',\n",
       " '61. Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.',\n",
       " 'Accuracy is calculated as:\\nAccuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)\\nTo build a confusion matrix in R, we will use the table function:\\ntable(test$target,pred_heart>0.6)\\nHere, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.\\nThen, we calculate the accuracy by the formula for calculating Accuracy.\\n\\n',\n",
       " 'Accuracy is calculated as:',\n",
       " 'Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)',\n",
       " 'To build a confusion matrix in R, we will use the table function:',\n",
       " 'Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.',\n",
       " 'Then, we calculate the accuracy by the formula for calculating Accuracy.',\n",
       " '',\n",
       " '62. Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:\\ncustomer_churn=pd.read_csv(“customer_churn.csv”)\\n\\nAfter loading this dataset, we can have a glance at the head of the dataset by using the following command:\\ncustomer_churn.head()\\nNow, we will separate the dependent and the independent variables into two separate objects:\\nx=pd.Dataframe(customer_churn[‘MonthlyCharges’])\\r\\n\\r\\ny=customer_churn[‘ Churn’]\\r\\n\\r\\n#Splitting the data into training and testing sets\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)\\nNow, we will see how to build the model and calculate log_loss.\\nfrom sklearn.linear_model, we have to import LogisticRegression\\r\\n\\r\\nl=LogisticRegression()\\r\\n\\r\\nl.fit(x_train,y_train)\\r\\n\\r\\ny_pred=l.predict_proba(x_test)\\nAs we are supposed to calculate the log_loss, we will import it from sklearn.metrics:\\nfrom sklearn.metrics import log_loss\\r\\n\\r\\nprint(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred\\nOutput:\\n0.5555020595194167\\nBecome a master of Data Science by going through this online Data Science Course in Toronto!\\n',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:',\n",
       " '',\n",
       " 'After loading this dataset, we can have a glance at the head of the dataset by using the following command:',\n",
       " 'Now, we will separate the dependent and the independent variables into two separate objects:',\n",
       " 'Now, we will see how to build the model and calculate log_loss.',\n",
       " 'As we are supposed to calculate the log_loss, we will import it from sklearn.metrics:',\n",
       " 'Output:',\n",
       " 'Become a master of Data Science by going through this online Data Science Course in Toronto!',\n",
       " '63. Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built. ',\n",
       " '\\nTo build a decision tree model, we will be loading the party package:\\n#party package\\r\\n\\r\\nlibrary(party)\\r\\n\\r\\n#splitting the data\\r\\n\\r\\nlibrary(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(iris$Species, p=0.65, list=F)\\r\\n\\r\\niris[split_tag,]->train\\r\\n\\r\\niris[~split_tag,]->test\\r\\n\\r\\n#building model\\r\\n\\r\\nmytree<-ctree(Species~.,train)\\nNow we will plot the model\\nplot(mytree)\\nModel:\\n\\n#predicting the values\\r\\n\\r\\npredict(mytree,test,type=’response’)->mypred\\nAfter this, we will predict the confusion matrix and then calculate the accuracy using the table function:\\ntable(test$Species, mypred)\\n\\n',\n",
       " '',\n",
       " 'To build a decision tree model, we will be loading the party package:',\n",
       " 'Now we will plot the model',\n",
       " 'Model:\\n',\n",
       " 'After this, we will predict the confusion matrix and then calculate the accuracy using the table function:',\n",
       " '',\n",
       " '64. Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.',\n",
       " '\\nWe will load the CTG dataset by using read.csv:\\ndata<-read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)\\r\\n\\r\\nstr(data)\\nConverting the integer type to a factor\\ndata$NSP<-as.factor(data$NSP)\\r\\n\\r\\ntable(data$NSP)\\r\\n\\r\\n#data partition\\r\\n\\r\\nset.seed(123)\\r\\n\\r\\nsplit_tag<-createDataPartition(data$NSP, p=0.65, list=F)\\r\\n\\r\\ndata[split_tag,]->train\\r\\n\\r\\ndata[~split_tag,]->test\\r\\n\\r\\n#random forest -1\\r\\n\\r\\nlibrary(randomForest)\\r\\n\\r\\nset.seed(222)\\r\\n\\r\\nrf<-randomForest(NSP~.,data=train)\\r\\n\\r\\nrf\\r\\n\\r\\n#prediction\\r\\n\\r\\npredict(rf,test)->p1\\nBuilding confusion matrix and calculating accuracy:\\ntable(test$NSP,p1)\\n\\nIf you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!\\n',\n",
       " '',\n",
       " 'We will load the CTG dataset by using read.csv:',\n",
       " 'Converting the integer type to a factor',\n",
       " 'Building confusion matrix and calculating accuracy:',\n",
       " '',\n",
       " 'If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!',\n",
       " '65. Write a function to calculate the Euclidean distance between two points.',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:\\n√(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2))\\nCode for calculating the Euclidean distance is as given below:\\ndef euclidean_distance(P1, P2):\\r\\nreturn (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5\\n',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:',\n",
       " 'Code for calculating the Euclidean distance is as given below:',\n",
       " '66. Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.',\n",
       " 'To calculate the root mean square error (RMSE), we have to:\\n\\n Calculate the errors, i.e., the differences between the actual and the predicted values\\n Square each of these errors\\n Calculate the mean of these squared errors\\n Return the square root of the mean\\n\\nThe code in Python for calculating RMSE is given below:\\ndef rmse(actual, predicted):\\r\\n\\xa0\\xa0errors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]\\r\\n\\xa0\\xa0squared_errors = [x ** 2 for x in errors]\\r\\n\\xa0\\xa0mean = sum(squared_errors) / len(squared_errors)\\r\\n\\xa0\\xa0return mean ** .5\\r\\n\\nCheck out this Machine Learning Course to get an in-depth understanding of Machine Learning.\\n',\n",
       " 'To calculate the root mean square error (RMSE), we have to:',\n",
       " 'The code in Python for calculating RMSE is given below:',\n",
       " 'Check out this Machine Learning Course to get an in-depth understanding of Machine Learning.',\n",
       " '67. Mention the different kernel functions that can be used in SVM.',\n",
       " 'In SVM, there are four types of kernel functions:\\n\\nLinear kernel\\nPolynomial kernel\\nRadial basis kernel\\nSigmoid kernel\\n\\n',\n",
       " 'In SVM, there are four types of kernel functions:',\n",
       " '68. How to detect if the time series data is stationary?',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.\\n',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.',\n",
       " '69. Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:\\ndef accuracy_score(matrix):\\r\\n\\xa0\\xa0true_positives = matrix[0][0]\\r\\n\\xa0\\xa0true_negatives = matrix[1][1]\\r\\n\\xa0\\xa0total_observations = sum(matrix[0]) + sum(matrix[1])\\r\\n\\xa0\\xa0return (true_positives + true_negatives) / total_observations\\r\\n\\n',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:',\n",
       " '70. What does root cause analysis mean?',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.\\n',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.',\n",
       " '71. What is A/B testing?',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.\\nCheck out this Python Course to get deeper into Python programming.\\n',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.',\n",
       " 'Check out this Python Course to get deeper into Python programming.',\n",
       " '72. Out of collaborative filtering and content-based filtering, which one is considered better, and why?',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.\\nIn the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.\\n',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.',\n",
       " 'In the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.',\n",
       " '73. In the following confusion matrix, calculate precision and recall.',\n",
       " '\\n\\n\\n\\nTotal = 510\\nActual\\n\\n\\nPredicted\\n\\nP\\nN\\n\\n\\nP\\n156\\n11\\n\\n\\nN\\n16\\n327\\n\\n\\n\\n\\nThe formulae for precision and recall are given below.\\nPrecision:\\r\\n(True Positive) / (True Positive + False Positive)\\r\\nRecall:\\r\\n(True Positive) / (True Positive + False Negative)\\r\\nBased on the given data, precision and recall are:\\r\\nPrecision: 156 / (156 + 11) = 93.4\\r\\nRecall: 156 / (156 + 16) = 90.7\\r\\n\\n',\n",
       " 'The formulae for precision and recall are given below.',\n",
       " '74. Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.',\n",
       " \"We can use the below for this purpose:\\ndef calculate_precsion_and_recall(matrix):\\r\\n\\xa0\\xa0true_positive\\xa0 = matrix[0][0]\\r\\n\\xa0\\xa0false_positive\\xa0 = matrix[0][1]\\r\\n\\xa0\\xa0false_negative = matrix[1][0]\\r\\n\\xa0\\xa0return {\\r\\n\\xa0\\xa0\\xa0\\xa0'precision': (true_positive) / (true_positive + false_positive),\\r\\n\\xa0\\xa0\\xa0\\xa0'recall': (true_positive) / (true_positive + false_negative)\\r\\n\\xa0\\xa0}\\r\\n\\n\",\n",
       " 'We can use the below for this purpose:',\n",
       " '75. What is reinforcement learning?',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.\\n',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.',\n",
       " '76. Explain TF/IDF vectorization.',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.\\n',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.',\n",
       " '77. What are the assumptions required for linear regression?',\n",
       " 'There are several assumptions required for linear regression. They are as follows:\\n\\nThe data, which is a sample drawn from a population, used to train the model should be representative of the population.\\nThe relationship between independent variables and the mean of dependent variables is linear.\\nThe variance of the residual is going to be the same for any value of an independent variable. It is also represented as X.\\nEach observation is independent of all other observations.\\nFor any value of an independent variable, the independent variable is normally distributed.\\n\\n',\n",
       " 'There are several assumptions required for linear regression. They are as follows:',\n",
       " '78. What happens when some of the assumptions required for linear regression are violated?',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.\\nStrong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.\\n',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.',\n",
       " 'Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.',\n",
       " 'Course Schedule',\n",
       " '\\n19 thoughts on “Top 78 Data Science Interview Questions and Answers for 2021” ',\n",
       " 'Wow, Great collection of Data Science questions. Thanks for sharing.',\n",
       " 'Really helpful.',\n",
       " 'This data science interview questions video as well as this entire set of data science questions both are extremely helpful. Thanks a lot !',\n",
       " 'Highly updated data science interview questions. Thank you so much, these questions helped me to clear my data science interview.',\n",
       " 'Good data science interview questions. Each question explained with good answer including example and output.',\n",
       " 'Great job, very good questions. Really helped me. Thaks.',\n",
       " 'All the questions are updated with all the problems an user can face while learning data science. Video lectures were also great. Everything well explained. Keep it up..!!',\n",
       " 'All the hard work done by intellipaat is really remarkable. All the questions are really important to crack an interview. Thanks you for such a nice material.',\n",
       " 'Just wow…!! Great work, jut loved it. Want to see more stuff like this.',\n",
       " 'All 20 questions were helpful and detailed. Everything was up to the mark. Great work.',\n",
       " 'Remarkable work, I would suggest everyone to go through it. Amazing questions with every explanation in detail.',\n",
       " 'Nice detailed questions, really helpful in cracking an interview. A must read for everyone. Loved it.',\n",
       " 'It’s nice to read the latest Data Science Interview Questions and Answers for 2019',\n",
       " 'Interesting & useful Data Science Interview Q and A. I am doing data science course. so, this gives me a great view.',\n",
       " 'Simply Superb Data Science Interview Ques. It’s useful for beginners and professionals also.',\n",
       " 'All the questions were very helpful in knowing an interview pattern, well explained and detailed.',\n",
       " 'It covers all basic questions helpful in learning data science. All the work done by IntelliPaat is exceptional.',\n",
       " 'All the questions are very professional and helpful in learning data science. Recommended to everyone who’s serious to get into this Field.',\n",
       " 'All the 20 questions were really helpful and well explained. Recommended to clear data science interview. Great Work…!!',\n",
       " 'Leave a Reply Cancel reply',\n",
       " 'Your email address will not be published. Required fields are marked *',\n",
       " 'Comment ',\n",
       " 'Name * ',\n",
       " 'Email * ',\n",
       " ' \\n\\n',\n",
       " '\\n \\n',\n",
       " 'Courses',\n",
       " 'Courses',\n",
       " 'Tutorials',\n",
       " 'Interview Questions',\n",
       " 'Download Salary Trends',\n",
       " 'Learn how professionals like you got upto 100% hike!',\n",
       " 'Course Preview',\n",
       " 'Expert-Led No.1',\n",
       " 'Top 78 Data Science Interview Questions and Answers for 2021']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst10 = []\n",
    "url = \"https://intellipaat.com/blog/interview-question/data-science-interview-questions/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h3','p'])\n",
    "for answer in answers:\n",
    "    lst10.append(answer.text)\n",
    "lst10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What do you understand by linear regression?',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.\\nInterested in learning Data Science? Click here to learn more in this Data Science Course!\\n\\n',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.',\n",
       " 'Interested in learning Data Science? Click here to learn more in this Data Science Course!',\n",
       " '2. What do you understand by logistic regression?',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.\\n\\n',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.',\n",
       " '3. What is a confusion matrix?',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.\\n',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.',\n",
       " 'CTA',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:\\n Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:',\n",
       " ' Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '4. What do you understand by true positive rate and false positive rate?',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.\\nCheck out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.',\n",
       " 'Check out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Get 50% Hike!',\n",
       " 'Master Most in Demand Skills Now !',\n",
       " '5. What is Data Science?',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.\\n\\n',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.',\n",
       " '6. How is Data Science different from traditional application programming?',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.\\nIn traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.\\nData Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).\\nAs described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.\\n\\n\\n',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.',\n",
       " 'In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.',\n",
       " 'Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).',\n",
       " 'As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.',\n",
       " '',\n",
       " '7. Explain the differences between supervised and unsupervised learning.',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.\\n\\n\\n\\n\\nSupervised Learning\\nUnsupervised Learning\\n\\n\\nWorks on the data that contains both inputs and the expected output, i.e., the labeled data\\nWorks on the data that contains no mappings from input to output, i.e., the unlabeled data\\n\\n\\nUsed to create models that can be employed to predict or classify things\\nUsed to extract meaningful information out of large volumes of data\\n\\n\\nCommonly used supervised learning algorithms: Linear regression, decision tree, etc.\\nCommonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.\\n\\n\\n\\n\\n\\n',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.',\n",
       " '8. What is dimensionality reduction?',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.\\n\\n',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.',\n",
       " '9. What is bias in Data Science?',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.\\n\\n',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.',\n",
       " '10. Why Python is used for Data Cleaning in DS?',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0\\nPython libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. \\nLearn more about Data Cleaning in Data Science Tutorial!\\n',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0',\n",
       " 'Python libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. ',\n",
       " 'Learn more about Data Cleaning in Data Science Tutorial!',\n",
       " '11. Why R is used in Data Visualization?',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0\\nIt has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0',\n",
       " 'It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '12. What are the popular libraries used in Data Science?',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:\\n\\nTensorFlow: Supports parallel computing with impeccable library management backed by Google.\\xa0\\nSciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts.\\nPandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications.\\nMatplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.\\xa0\\nPyTorch: Best for projects which involve Machine Learning algorithms and Deep Neural Networks.\\xa0\\n\\nInterested to learn more about Data Science, check out our\\xa0Data Science Course in New York!\\n',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:',\n",
       " 'Interested to learn more about Data Science, check out our\\xa0Data Science Course in New York!',\n",
       " '13. What is variance in Data Science?',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.\\n',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.',\n",
       " '14. What is pruning in a decision tree algorithm?',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n\\n',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n',\n",
       " '15. What is entropy in a decision tree algorithm?',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.\\nFor example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.\\n',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.',\n",
       " 'For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.',\n",
       " '16. What is information gain in a decision tree algorithm?',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. \\n',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. ',\n",
       " '17. What is k-fold cross-validation?',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.\\n',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.',\n",
       " '18. Explain how a recommender system works.',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.\\nFor example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.',\n",
       " 'For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '19. What is a normal distribution?',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.\\n',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.',\n",
       " '20. What is Deep Learning?',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '21. What is an RNN (recurrent neural network)?',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.\\n',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.',\n",
       " '22. Explain selection bias.',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.\\n\\nIntermediate Data Science Interview Questions\\n',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.',\n",
       " '23. What is ROC curve?',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: \\n',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: ',\n",
       " '24. What do you understand by a decision tree?',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.\\nAre you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!\\n',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.',\n",
       " 'Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!',\n",
       " '25. What do you understand by a random forest model?',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.\\n\\n',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.',\n",
       " '',\n",
       " '26. Two candidates Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least of them will crack the interview?',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12\\nNow, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)\\nWhere P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)\\n1/8 * 5/12\\n5/96\\nNow, put the value of P(A ∩ B) into equation 1\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B))\\n1/8 + 5/12 -5/96\\nSo, the answer will be 47/96.\\n',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12',\n",
       " 'Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)',\n",
       " 'Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)',\n",
       " '1/8 * 5/12',\n",
       " '5/96',\n",
       " 'Now, put the value of P(A ∩ B) into equation 1',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B))',\n",
       " '1/8 + 5/12 -5/96',\n",
       " 'So, the answer will be 47/96.',\n",
       " '27. How is Data modeling different from Database design?',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.\\n',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.',\n",
       " '28. What are precision?',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:\\n\\n',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:',\n",
       " '',\n",
       " '29. What is recall?',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:\\n\\n',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:',\n",
       " '',\n",
       " '30. What is the F1 score and how to calculate it?',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: \\n',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: ',\n",
       " '31. What is p-value? ',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.\\n',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.',\n",
       " '32. Why do we use p-value?',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:\\n\\n',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:',\n",
       " '',\n",
       " '33. What is the difference between an error and a residual error?',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.\\n',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.',\n",
       " '34. Why do we use the summary function?',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.\\n',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.',\n",
       " '35. How are Data Science and Machine Learning related to each other?',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.\\nData Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.\\nMachine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.\\nIn short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.',\n",
       " 'Data Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.',\n",
       " 'Machine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.',\n",
       " 'In short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '36. Explain univariate, bivariate, and multivariate analyses.',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.\\n\\nUnivariate analysis: Univariate analysis involves analysing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends out of it. Example: Analyzing the weight of a group of people.\\nBivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude.\\nMultivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable). Example: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc.\\n\\n',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.',\n",
       " '37. How can we handle missing data?',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.\\nOne way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.\\nAnother way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.\\nFinally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.\\n',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.',\n",
       " 'One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.',\n",
       " 'Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.',\n",
       " 'Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.',\n",
       " '38. What is the benefit of dimensionality reduction?',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.\\n',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.',\n",
       " '39. What is bias–variance trade-off in Data Science?',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.\\nBias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.\\n',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.',\n",
       " 'Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.',\n",
       " '40. What is RMSE?',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:\\nFirst, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.\\n',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:',\n",
       " 'First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.',\n",
       " '41. What is a kernel function in SVM?',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.\\n',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.',\n",
       " '42. How can we select an appropriate value of k in k-means?',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.\\nThis is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.\\n',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.',\n",
       " 'This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.',\n",
       " '43. How can we deal with outliers?',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.\\nIn case the outliers are not that extreme, then we can try:\\n\\nA different kind of model. For example, if we were using a linear model, then we can choose a non-linear model\\nNormalizing the data, which will shift the extreme values closer to other data points\\nUsing algorithms that are not so affected by outliers, such as random forest, etc.\\n\\n',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.',\n",
       " 'In case the outliers are not that extreme, then we can try:',\n",
       " '44. How to calculate the accuracy of a binary classification algorithm using its confusion matrix?',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:\\n\\nTrue positives: Number of observations correctly classified as True\\nTrue negatives: Number of observations correctly classified as False\\nFalse positives: Number of observations incorrectly classified as True\\nFalse negatives: Number of observations incorrectly classified as False\\n\\nTo calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:\\n',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:',\n",
       " 'To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:',\n",
       " '45. What is ensemble learning?',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.\\n',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.',\n",
       " '46. Explain collaborative filtering in recommender systems.',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.\\n',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.',\n",
       " '47. Explain content-based filtering in recommender systems.',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.\\n',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.',\n",
       " '48. Explain bagging in Data Science.',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.\\n',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.',\n",
       " '49. Explain boosting in Data Science.',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.\\n',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.',\n",
       " '50. Explain stacking in Data Science.',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.\\n',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.',\n",
       " '51. Explain how Machine Learning is different from Deep Learning.',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.\\nDeep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.\\n',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.',\n",
       " 'Deep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.',\n",
       " '52. Why does Naive Bayes have the word ‘naive’ in it?',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.\\nIt has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.\\nTo learn more about Data Science, check out our Data Science Course in Hyderabad.\\nAdvanced Data Science Interview Questions\\n',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.',\n",
       " 'It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.',\n",
       " 'To learn more about Data Science, check out our Data Science Course in Hyderabad.',\n",
       " '53. From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.',\n",
       " '\\nFirst, we will load the ggplot2 package:\\nlibrary(ggplot2)\\nNext, we will use the dplyr package:\\nlibrary(dplyr)// It is based on the grammar of data manipulation.\\nTo extract those particular records, use the below command:\\ndiamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea\\n',\n",
       " '',\n",
       " 'First, we will load the ggplot2 package:',\n",
       " 'Next, we will use the dplyr package:',\n",
       " 'To extract those particular records, use the below command:',\n",
       " '54. Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on y-axis, ’carat’ should be on x-axis, and the ‘color’ of the points should be determined by ‘cut.’',\n",
       " 'We will implement the scatter plot using ggplot.\\nThe ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.\\nSo, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.\\nCode:\\n>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()\\n',\n",
       " 'We will implement the scatter plot using ggplot.',\n",
       " 'The ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.',\n",
       " 'So, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.',\n",
       " 'Code:',\n",
       " '55. Introduce 25 percent missing values in this ‘iris’ datset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’',\n",
       " '\\nTo introduce missing values, we will be using the missForest package:\\nlibrary(missForest)\\nUsing the prodNA function, we will be introducing 25 percent of missing values:\\nIris.mis<-prodNA(iris,noNA=0.25)\\nFor imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:\\nlibrary(Hmisc)\\r\\niris.mis$Sepal.Length<-with(iris.mis, impute(Sepal.Length,mean))\\r\\niris.mis$Petal.Length<-with(iris.mis, impute(Petal.Length,median))\\n',\n",
       " '',\n",
       " 'To introduce missing values, we will be using the missForest package:',\n",
       " 'Using the prodNA function, we will be introducing 25 percent of missing values:',\n",
       " 'For imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:',\n",
       " '56. Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’',\n",
       " '\\nHere, we need to find how ‘mpg’ varies w.r.t displacement of the column.\\nWe need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.\\nSo, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.\\nTherefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.\\nHere, we will use the following code:\\nlibraray(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)\\r\\n\\r\\nmtcars[split_tag,]->train\\r\\n\\r\\nmtcars[-split_tag,]->test\\r\\n\\r\\nlm(mpg-data,data=train)->mod_mtcars\\r\\n\\r\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\r\\n\\r\\n>head(pred_mtcars)\\nExplanation:\\nParameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).\\nSecond is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.\\nOnce we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.\\nSimilarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.\\nSo, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.\\nWe will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.\\nlm(mpg-data,data=train)->mod_mtcars\\nNow, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.\\nThus, we have to predict values for the test set and then store them in pred_mtcars.\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\nOutput:\\nThese are the predicted values of mpg for all of these cars.\\n\\nSo, this is how we can build simple linear model on top of this mtcars dataset.\\n',\n",
       " '',\n",
       " 'Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.',\n",
       " 'We need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.',\n",
       " 'So, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.',\n",
       " 'Therefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.',\n",
       " 'Here, we will use the following code:',\n",
       " 'Explanation:',\n",
       " 'Parameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).',\n",
       " 'Second is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.',\n",
       " 'Once we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.',\n",
       " 'Similarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.',\n",
       " 'So, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.',\n",
       " 'We will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.',\n",
       " 'Now, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.',\n",
       " 'Thus, we have to predict values for the test set and then store them in pred_mtcars.',\n",
       " 'Output:',\n",
       " 'These are the predicted values of mpg for all of these cars.',\n",
       " '',\n",
       " 'So, this is how we can build simple linear model on top of this mtcars dataset.',\n",
       " '57. Calculate the RMSE values for the model built.',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\r\\n\\r\\nas.data.frame(final_data)->final_data\\r\\n\\r\\nerror<-(final_data$Actual-final_data$Prediction)\\r\\n\\r\\ncbind(final_data,error)->final_data\\r\\n\\r\\nsqrt(mean(final_data$error)^2)\\nExplanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\nOur actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:\\nas.data.frame(final_data)->final_data\\nWe will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:\\nerror<-(final_data$Actual-final_data$Prediction)\\nThen, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:\\ncbind(final_data,error)->final_data //binding error object to this final_data\\nHere, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:\\nSqrt(mean(final_data$error)^2)\\nOutput:\\n[1] 4.334423\\nNote: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.\\n',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:',\n",
       " 'Explanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:',\n",
       " 'Our actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:',\n",
       " 'We will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:',\n",
       " 'Then, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:',\n",
       " 'Here, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:',\n",
       " 'Output:',\n",
       " 'Note: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.',\n",
       " '58. Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’',\n",
       " 'Simple Linear Regression\\nimport pandas as pd\\r\\n\\r\\ndata=pd.read_csv(‘Boston.csv’)\\xa0\\xa0\\xa0\\xa0 //loading the Boston dataset\\r\\n\\r\\ndata.head()\\xa0 //having a glance at the head of this data\\r\\n\\r\\ndata.shape\\r\\n\\r\\n\\nLet us take out the dependent and the independent variables from the dataset:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\r\\n\\r\\ndata1.head()\\nVisualizing Variables\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nHere, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.\\nWe will separate the dependent and the independent variable from this entire dataframe:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\nThe only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.\\nNow, we would also do a visualization w.r.t to these two columns:\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nPreparing the Data\\nX=pd.Dataframe(data1[‘lstat’])\\r\\n\\r\\nY=pd.Dataframe(data1[‘medv’])\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\\r\\n\\r\\nfrom sklearn.linear_model import LinearRegression\\r\\n\\r\\nregressor=LinearRegression()\\r\\n\\r\\nregressor.fit(X_train,y_train)\\nprint(regressor.intercept_)\\nOutput :\\n34.12654201\\nprint(regressor.coef_)//this is the slope\\nOutput :\\n[[-0.913293]]\\nBy now, we have built the model. Now, we have to predict the values on top of the test set:\\ny_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in y_pred object\\r\\n\\r\\n\\nNow, let’s have a glance at the rows and columns of the actual values and the predicted values:\\nY_pred.shape, y_test.shape\\nOutput :\\n((102,1),(102,1))\\nFurther, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.\\nfrom sklearn import metrics import NumPy as np\\r\\n\\r\\nprint(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\nOutput:\\nMean Absolute Error: 4.692198\\r\\n\\r\\nMean Squared Error: 43.9198\\r\\n\\r\\nRoot Mean Squared Error: 6.6270\\n\\n',\n",
       " 'Simple Linear Regression',\n",
       " 'Let us take out the dependent and the independent variables from the dataset:',\n",
       " 'Visualizing Variables',\n",
       " 'Here, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.',\n",
       " 'We will separate the dependent and the independent variable from this entire dataframe:',\n",
       " 'The only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.',\n",
       " 'Now, we would also do a visualization w.r.t to these two columns:',\n",
       " 'Preparing the Data',\n",
       " 'Output :',\n",
       " 'Output :',\n",
       " 'By now, we have built the model. Now, we have to predict the values on top of the test set:',\n",
       " 'Now, let’s have a glance at the rows and columns of the actual values and the predicted values:',\n",
       " 'Output :',\n",
       " 'Further, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.',\n",
       " 'Output:',\n",
       " '',\n",
       " '59. Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’',\n",
       " '\\nFor loading the dataset, we will use the read.csv function:\\nread.csv(“D:/heart.csv”)->heart\\r\\n\\r\\nstr(heart)\\nIn the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:\\nas.factor(heart$target)->heart$target\\nNow, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.\\nTo build a logistic regression model, we will use the glm function:\\nglm(target~age, data=heart, family=”binomial”)->log_mod1\\nHere, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.\\nfamily=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.\\nWe will have a glance at the summary of the model that we have just built:\\nsummary(log_mod1)\\n\\nWe can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.\\nNow, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.\\nThis null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.\\nResidual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.\\nThis basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.\\nAs we have built the model, it’s time to predict some values:\\npredict(log_mod1, data.frame(age=30), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=50), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=29:77), type=”response”)\\r\\n\\r\\n\\nNow, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:\\n>library(caret)\\r\\n\\r\\nSplit_tag<- createDataPartition(heart$target, p=0.70, list=F)\\r\\n\\r\\nheart[split_tag,]->train\\r\\n\\r\\nheart[-split_tag,]->test\\r\\n\\r\\nglm(target~age, data=train,family=”binomial”)->log_mod2\\r\\n\\r\\npredict(log_mod2, newdata=test, type=”response”)->pred_heart\\r\\n\\r\\nrange(pred_heart)\\n',\n",
       " '',\n",
       " 'For loading the dataset, we will use the read.csv function:',\n",
       " 'In the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:',\n",
       " 'Now, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.',\n",
       " 'To build a logistic regression model, we will use the glm function:',\n",
       " 'Here, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.',\n",
       " 'family=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.',\n",
       " 'We will have a glance at the summary of the model that we have just built:',\n",
       " '',\n",
       " 'We can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.',\n",
       " 'Now, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.',\n",
       " 'This null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.',\n",
       " 'Residual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.',\n",
       " 'This basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.',\n",
       " 'As we have built the model, it’s time to predict some values:',\n",
       " 'Now, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:',\n",
       " '60. Build an ROC curve for the model built.',\n",
       " 'The below code will help us in building the ROC curve:\\nlibrary(ROCR)\\r\\n\\r\\nprediction(pred_heart, test$target)-> roc_pred_heart\\r\\n\\r\\nperformance(roc_pred_heart, “tpr”, “fpr”)->roc_curve\\r\\n\\r\\nplot(roc_curve, colorize=T)\\nGraph: \\nGo through this Data Science Course in London to get a clear understanding of Data Science!\\n\\n',\n",
       " 'The below code will help us in building the ROC curve:',\n",
       " 'Graph: ',\n",
       " 'Go through this Data Science Course in London to get a clear understanding of Data Science!',\n",
       " '61. Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.',\n",
       " 'Accuracy is calculated as:\\nAccuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)\\nTo build a confusion matrix in R, we will use the table function:\\ntable(test$target,pred_heart>0.6)\\nHere, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.\\nThen, we calculate the accuracy by the formula for calculating Accuracy.\\n\\n',\n",
       " 'Accuracy is calculated as:',\n",
       " 'Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)',\n",
       " 'To build a confusion matrix in R, we will use the table function:',\n",
       " 'Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.',\n",
       " 'Then, we calculate the accuracy by the formula for calculating Accuracy.',\n",
       " '',\n",
       " '62. Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:\\ncustomer_churn=pd.read_csv(“customer_churn.csv”)\\n\\nAfter loading this dataset, we can have a glance at the head of the dataset by using the following command:\\ncustomer_churn.head()\\nNow, we will separate the dependent and the independent variables into two separate objects:\\nx=pd.Dataframe(customer_churn[‘MonthlyCharges’])\\r\\n\\r\\ny=customer_churn[‘ Churn’]\\r\\n\\r\\n#Splitting the data into training and testing sets\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)\\nNow, we will see how to build the model and calculate log_loss.\\nfrom sklearn.linear_model, we have to import LogisticRegression\\r\\n\\r\\nl=LogisticRegression()\\r\\n\\r\\nl.fit(x_train,y_train)\\r\\n\\r\\ny_pred=l.predict_proba(x_test)\\nAs we are supposed to calculate the log_loss, we will import it from sklearn.metrics:\\nfrom sklearn.metrics import log_loss\\r\\n\\r\\nprint(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred\\nOutput:\\n0.5555020595194167\\nBecome a master of Data Science by going through this online Data Science Course in Toronto!\\n',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:',\n",
       " '',\n",
       " 'After loading this dataset, we can have a glance at the head of the dataset by using the following command:',\n",
       " 'Now, we will separate the dependent and the independent variables into two separate objects:',\n",
       " 'Now, we will see how to build the model and calculate log_loss.',\n",
       " 'As we are supposed to calculate the log_loss, we will import it from sklearn.metrics:',\n",
       " 'Output:',\n",
       " 'Become a master of Data Science by going through this online Data Science Course in Toronto!',\n",
       " '63. Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built. ',\n",
       " '\\nTo build a decision tree model, we will be loading the party package:\\n#party package\\r\\n\\r\\nlibrary(party)\\r\\n\\r\\n#splitting the data\\r\\n\\r\\nlibrary(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(iris$Species, p=0.65, list=F)\\r\\n\\r\\niris[split_tag,]->train\\r\\n\\r\\niris[~split_tag,]->test\\r\\n\\r\\n#building model\\r\\n\\r\\nmytree<-ctree(Species~.,train)\\nNow we will plot the model\\nplot(mytree)\\nModel:\\n\\n#predicting the values\\r\\n\\r\\npredict(mytree,test,type=’response’)->mypred\\nAfter this, we will predict the confusion matrix and then calculate the accuracy using the table function:\\ntable(test$Species, mypred)\\n\\n',\n",
       " '',\n",
       " 'To build a decision tree model, we will be loading the party package:',\n",
       " 'Now we will plot the model',\n",
       " 'Model:\\n',\n",
       " 'After this, we will predict the confusion matrix and then calculate the accuracy using the table function:',\n",
       " '',\n",
       " '64. Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.',\n",
       " '\\nWe will load the CTG dataset by using read.csv:\\ndata<-read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)\\r\\n\\r\\nstr(data)\\nConverting the integer type to a factor\\ndata$NSP<-as.factor(data$NSP)\\r\\n\\r\\ntable(data$NSP)\\r\\n\\r\\n#data partition\\r\\n\\r\\nset.seed(123)\\r\\n\\r\\nsplit_tag<-createDataPartition(data$NSP, p=0.65, list=F)\\r\\n\\r\\ndata[split_tag,]->train\\r\\n\\r\\ndata[~split_tag,]->test\\r\\n\\r\\n#random forest -1\\r\\n\\r\\nlibrary(randomForest)\\r\\n\\r\\nset.seed(222)\\r\\n\\r\\nrf<-randomForest(NSP~.,data=train)\\r\\n\\r\\nrf\\r\\n\\r\\n#prediction\\r\\n\\r\\npredict(rf,test)->p1\\nBuilding confusion matrix and calculating accuracy:\\ntable(test$NSP,p1)\\n\\nIf you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!\\n',\n",
       " '',\n",
       " 'We will load the CTG dataset by using read.csv:',\n",
       " 'Converting the integer type to a factor',\n",
       " 'Building confusion matrix and calculating accuracy:',\n",
       " '',\n",
       " 'If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!',\n",
       " '65. Write a function to calculate the Euclidean distance between two points.',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:\\n√(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2))\\nCode for calculating the Euclidean distance is as given below:\\ndef euclidean_distance(P1, P2):\\r\\nreturn (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5\\n',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:',\n",
       " 'Code for calculating the Euclidean distance is as given below:',\n",
       " '66. Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.',\n",
       " 'To calculate the root mean square error (RMSE), we have to:\\n\\n Calculate the errors, i.e., the differences between the actual and the predicted values\\n Square each of these errors\\n Calculate the mean of these squared errors\\n Return the square root of the mean\\n\\nThe code in Python for calculating RMSE is given below:\\ndef rmse(actual, predicted):\\r\\n\\xa0\\xa0errors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]\\r\\n\\xa0\\xa0squared_errors = [x ** 2 for x in errors]\\r\\n\\xa0\\xa0mean = sum(squared_errors) / len(squared_errors)\\r\\n\\xa0\\xa0return mean ** .5\\r\\n\\nCheck out this Machine Learning Course to get an in-depth understanding of Machine Learning.\\n',\n",
       " 'To calculate the root mean square error (RMSE), we have to:',\n",
       " 'The code in Python for calculating RMSE is given below:',\n",
       " 'Check out this Machine Learning Course to get an in-depth understanding of Machine Learning.',\n",
       " '67. Mention the different kernel functions that can be used in SVM.',\n",
       " 'In SVM, there are four types of kernel functions:\\n\\nLinear kernel\\nPolynomial kernel\\nRadial basis kernel\\nSigmoid kernel\\n\\n',\n",
       " 'In SVM, there are four types of kernel functions:',\n",
       " '68. How to detect if the time series data is stationary?',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.\\n',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.',\n",
       " '69. Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:\\ndef accuracy_score(matrix):\\r\\n\\xa0\\xa0true_positives = matrix[0][0]\\r\\n\\xa0\\xa0true_negatives = matrix[1][1]\\r\\n\\xa0\\xa0total_observations = sum(matrix[0]) + sum(matrix[1])\\r\\n\\xa0\\xa0return (true_positives + true_negatives) / total_observations\\r\\n\\n',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:',\n",
       " '70. What does root cause analysis mean?',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.\\n',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.',\n",
       " '71. What is A/B testing?',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.\\nCheck out this Python Course to get deeper into Python programming.\\n',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.',\n",
       " 'Check out this Python Course to get deeper into Python programming.',\n",
       " '72. Out of collaborative filtering and content-based filtering, which one is considered better, and why?',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.\\nIn the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.\\n',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.',\n",
       " 'In the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.',\n",
       " '73. In the following confusion matrix, calculate precision and recall.',\n",
       " '\\n\\n\\n\\nTotal = 510\\nActual\\n\\n\\nPredicted\\n\\nP\\nN\\n\\n\\nP\\n156\\n11\\n\\n\\nN\\n16\\n327\\n\\n\\n\\n\\nThe formulae for precision and recall are given below.\\nPrecision:\\r\\n(True Positive) / (True Positive + False Positive)\\r\\nRecall:\\r\\n(True Positive) / (True Positive + False Negative)\\r\\nBased on the given data, precision and recall are:\\r\\nPrecision: 156 / (156 + 11) = 93.4\\r\\nRecall: 156 / (156 + 16) = 90.7\\r\\n\\n',\n",
       " 'The formulae for precision and recall are given below.',\n",
       " '74. Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.',\n",
       " \"We can use the below for this purpose:\\ndef calculate_precsion_and_recall(matrix):\\r\\n\\xa0\\xa0true_positive\\xa0 = matrix[0][0]\\r\\n\\xa0\\xa0false_positive\\xa0 = matrix[0][1]\\r\\n\\xa0\\xa0false_negative = matrix[1][0]\\r\\n\\xa0\\xa0return {\\r\\n\\xa0\\xa0\\xa0\\xa0'precision': (true_positive) / (true_positive + false_positive),\\r\\n\\xa0\\xa0\\xa0\\xa0'recall': (true_positive) / (true_positive + false_negative)\\r\\n\\xa0\\xa0}\\r\\n\\n\",\n",
       " 'We can use the below for this purpose:',\n",
       " '75. What is reinforcement learning?',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.\\n',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.',\n",
       " '76. Explain TF/IDF vectorization.',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.\\n',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.',\n",
       " '77. What are the assumptions required for linear regression?',\n",
       " 'There are several assumptions required for linear regression. They are as follows:\\n\\nThe data, which is a sample drawn from a population, used to train the model should be representative of the population.\\nThe relationship between independent variables and the mean of dependent variables is linear.\\nThe variance of the residual is going to be the same for any value of an independent variable. It is also represented as X.\\nEach observation is independent of all other observations.\\nFor any value of an independent variable, the independent variable is normally distributed.\\n\\n',\n",
       " 'There are several assumptions required for linear regression. They are as follows:',\n",
       " '78. What happens when some of the assumptions required for linear regression are violated?',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.\\nStrong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.\\n']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new10 = lst10[28:405]\n",
    "lst_new10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do you understand by linear regression?</td>\n",
       "      <td>Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression. Interested in learning Data Science? Click here to learn more in this Data Science Course!  Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.Interested in learning Data Science? Click here to learn more in this Data Science Course!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do you understand by logistic regression?</td>\n",
       "      <td>Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a confusion matrix?</td>\n",
       "      <td>The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works. The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.CTAWatch this comprehensive Data Science tutorial to learn more:  Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          Watch this comprehensive Data Science tutorial to learn more: Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do you understand by true positive rate and false positive rate?</td>\n",
       "      <td>True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives. Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                             True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                           Get 50% Hike!Master Most in Demand Skills Now !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Data Science?</td>\n",
       "      <td>Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Questions  \\\n",
       "0                             What do you understand by linear regression?   \n",
       "1                           What do you understand by logistic regression?   \n",
       "2                                              What is a confusion matrix?   \n",
       "3    What do you understand by true positive rate and false positive rate?   \n",
       "4                                                    What is Data Science?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression. Interested in learning Data Science? Click here to learn more in this Data Science Course!  Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.Interested in learning Data Science? Click here to learn more in this Data Science Course!  \n",
       "1                                                                         Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  \n",
       "2  The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works. The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.CTAWatch this comprehensive Data Science tutorial to learn more:  Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          Watch this comprehensive Data Science tutorial to learn more: Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                       True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives. Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                             True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                           Get 50% Hike!Master Most in Demand Skills Now !  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"\n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "j=0\n",
    "for i in lst_new10:\n",
    "    j=j+1\n",
    "    w=re.findall(pattern,i)\n",
    "    #print(w)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ans)):\n",
    "    #ques[i]=ques[i].replace('\\n',\" \")\n",
    "    ques[i]=re.sub(r\"[0-9 ]+\\.\",\" \",ques[i])\n",
    "    \n",
    "df10=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df10[70:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10=df10.drop(df10.index[[52,53,54,55,56,57,58,59,60,61,62,63,72]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df10[52:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n \\n\\n',\n",
       " 'Data Science is one of the hottest jobs today. According to LinkedIn, the Data Scientist jobs are among the top 10 jobs in the United States. According to The Economic Times, the job postings for the  Data Science profiles  have grown over 400 times over the past one year. So, it is obvious that companies today survive on data, and Data Scientists are the rockstars of this era. So, if you want to start your career as a Data Scientist, you must be wondering what sort of questions are asked in the Data Science interview. So, in this interview preparation blog, we will be going through Data Science interview questions and answers.\\n',\n",
       " 'Categories',\n",
       " 'Automation',\n",
       " 'Big Data',\n",
       " 'Business Intelligence',\n",
       " 'Cloud Computing',\n",
       " 'Cyber Security',\n",
       " 'Data Science',\n",
       " 'Database',\n",
       " 'Digital Marketing',\n",
       " 'Mobile Development',\n",
       " 'No-SQL',\n",
       " 'Programming',\n",
       " 'Project Management',\n",
       " 'Salesforce',\n",
       " 'Testing',\n",
       " 'Website Development',\n",
       " 'CTA',\n",
       " 'CTA',\n",
       " 'Data Science is among the leading and most popular technologies in the world today. Major organizations are hiring professionals in this field. With high demand and low availability of these professionals, Data Scientists are among the highest-paid IT professionals. This Data Science Interview preparation blog includes most frequently asked questions in Data Science job interviews. Here is a list of these popular Data Science interview questions:\\nQ1. What do you understand by linear regression?\\nQ2. What do you understand by logistic regression?\\nQ3. What is a confusion matrix?\\nQ4. What do you understand by true positive rate and false positive rate?\\nQ5. What is Data Science?\\nQ6. How is Data Science different from traditional application programming?\\nQ7. Explain the differences between supervised and unsupervised learning.\\nQ8. What is dimensionality reduction?\\nQ9. What is bias in Data Science?\\nQ10. What is variance in Data Science?\\nFollowing are the three categories into which these Data Science interview questions are divided:\\n1. Basic\\n2. Intermediate\\n3. Advanced\\nCheck out this video on Data Science Interview Questions:\\n Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBasic Data Science Interview Questions\\n\\n',\n",
       " 'Data Science is among the leading and most popular technologies in the world today. Major organizations are hiring professionals in this field. With high demand and low availability of these professionals, Data Scientists are among the highest-paid IT professionals. This Data Science Interview preparation blog includes most frequently asked questions in Data Science job interviews. Here is a list of these popular Data Science interview questions:',\n",
       " 'Q1. What do you understand by linear regression?\\nQ2. What do you understand by logistic regression?\\nQ3. What is a confusion matrix?\\nQ4. What do you understand by true positive rate and false positive rate?\\nQ5. What is Data Science?\\nQ6. How is Data Science different from traditional application programming?\\nQ7. Explain the differences between supervised and unsupervised learning.\\nQ8. What is dimensionality reduction?\\nQ9. What is bias in Data Science?\\nQ10. What is variance in Data Science?',\n",
       " 'Following are the three categories into which these Data Science interview questions are divided:\\n1. Basic',\n",
       " '2. Intermediate',\n",
       " '3. Advanced',\n",
       " 'Check out this video on Data Science Interview Questions:',\n",
       " ' Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '1. What do you understand by linear regression?',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.\\nInterested in learning Data Science? Click here to learn more in this Data Science Course!\\n\\n',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.',\n",
       " 'Interested in learning Data Science? Click here to learn more in this Data Science Course!',\n",
       " 'Interested in learning Data Science? Click here to learn more in this Data Science Course!',\n",
       " '2. What do you understand by logistic regression?',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.\\n\\n',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.',\n",
       " ' S',\n",
       " '3. What is a confusion matrix?',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.\\n',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.',\n",
       " 'True Positive (d)',\n",
       " 'False Negative (c)',\n",
       " 'False Positive (b):',\n",
       " 'True Negative (a):',\n",
       " 'CTA',\n",
       " 'CTA',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:\\n Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:',\n",
       " ' Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '4. What do you understand by true positive rate and false positive rate?',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.\\nCheck out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.',\n",
       " 'True positive rate',\n",
       " 'Formula',\n",
       " 'False positive rate',\n",
       " 'Formula',\n",
       " 'Check out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Check out this comprehensive Data Science Course in India!',\n",
       " 'Master Most in Demand Skills Now !',\n",
       " '5. What is Data Science?',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.\\n\\n',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.',\n",
       " '6. How is Data Science different from traditional application programming?',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.\\nIn traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.\\nData Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).\\nAs described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.\\n\\n\\n',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.',\n",
       " 'In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.',\n",
       " 'Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).',\n",
       " 'As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.',\n",
       " '',\n",
       " '7. Explain the differences between supervised and unsupervised learning.',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.\\n\\n\\n\\n\\nSupervised Learning\\nUnsupervised Learning\\n\\n\\nWorks on the data that contains both inputs and the expected output, i.e., the labeled data\\nWorks on the data that contains no mappings from input to output, i.e., the unlabeled data\\n\\n\\nUsed to create models that can be employed to predict or classify things\\nUsed to extract meaningful information out of large volumes of data\\n\\n\\nCommonly used supervised learning algorithms: Linear regression, decision tree, etc.\\nCommonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.\\n\\n\\n\\n\\n\\n',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.',\n",
       " '8. What is dimensionality reduction?',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.\\n\\n',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.',\n",
       " '9. What is bias in Data Science?',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.\\n\\n',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.',\n",
       " '10. Why Python is used for Data Cleaning in DS?',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0\\nPython libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. \\nLearn more about Data Cleaning in Data Science Tutorial!\\n',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0',\n",
       " 'Python libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. ',\n",
       " 'Learn more about Data Cleaning in Data Science Tutorial!',\n",
       " 'Learn more about Data Cleaning in Data Science Tutorial!',\n",
       " '11. Why R is used in Data Visualization?',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0\\nIt has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0',\n",
       " 'It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '12. What are the popular libraries used in Data Science?',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:\\n\\nTensorFlow: Supports parallel computing with impeccable library management backed by Google.\\xa0\\nSciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts.\\nPandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications.\\nMatplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.\\xa0\\nPyTorch: Best for projects which involve Machine Learning algorithms and Deep Neural Networks.\\xa0\\n\\nInterested to learn more about Data Science, check out our\\xa0Data Science Course in New York!\\n',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:',\n",
       " 'Interested to learn more about Data Science, check out our\\xa0Data Science Course in New York!',\n",
       " 'Interested to learn more about Data Science, check out our\\xa0Data Science Course in New York!',\n",
       " '13. What is variance in Data Science?',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.\\n',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.',\n",
       " '14. What is pruning in a decision tree algorithm?',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n\\n',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n',\n",
       " '15. What is entropy in a decision tree algorithm?',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.\\nFor example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.\\n',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.',\n",
       " 'For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.',\n",
       " '16. What is information gain in a decision tree algorithm?',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. \\n',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. ',\n",
       " '17. What is k-fold cross-validation?',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.\\n',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.',\n",
       " '18. Explain how a recommender system works.',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.\\nFor example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.',\n",
       " 'For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '19. What is a normal distribution?',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.\\n',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.',\n",
       " '20. What is Deep Learning?',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '21. What is an RNN (recurrent neural network)?',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.\\n',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.',\n",
       " '22. Explain selection bias.',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.\\n\\nIntermediate Data Science Interview Questions\\n',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.',\n",
       " '23. What is ROC curve?',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: \\n',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: ',\n",
       " 'Receiver Operating Characteristic',\n",
       " '24. What do you understand by a decision tree?',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.\\nAre you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!\\n',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.',\n",
       " 'Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!',\n",
       " 'Are you interested in learning Data Science from experts? Enroll in our ',\n",
       " 'Data Science Course in Bangalore now!',\n",
       " '25. What do you understand by a random forest model?',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.\\n\\n',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.',\n",
       " '',\n",
       " '26. Two candidates Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least of them will crack the interview?',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12\\nNow, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)\\nWhere P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)\\n1/8 * 5/12\\n5/96\\nNow, put the value of P(A ∩ B) into equation 1\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B))\\n1/8 + 5/12 -5/96\\nSo, the answer will be 47/96.\\n',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12',\n",
       " 'Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)',\n",
       " 'Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)',\n",
       " '1/8 * 5/12',\n",
       " '5/96',\n",
       " 'Now, put the value of P(A ∩ B) into equation 1',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B))',\n",
       " '1/8 + 5/12 -5/96',\n",
       " 'So, the answer will be 47/96.',\n",
       " '27. How is Data modeling different from Database design?',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.\\n',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.',\n",
       " 'Data Modeling',\n",
       " 'Database Design',\n",
       " '28. What are precision?',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:\\n\\n',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:',\n",
       " 'Precision',\n",
       " '',\n",
       " '29. What is recall?',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:\\n\\n',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:',\n",
       " 'Recall',\n",
       " '',\n",
       " '30. What is the F1 score and how to calculate it?',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: \\n',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: ',\n",
       " '31. What is p-value? ',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.\\n',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.',\n",
       " '32. Why do we use p-value?',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:\\n\\n',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:',\n",
       " '',\n",
       " '33. What is the difference between an error and a residual error?',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.\\n',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.',\n",
       " 'error',\n",
       " 'residual error',\n",
       " '34. Why do we use the summary function?',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.\\n',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.',\n",
       " '35. How are Data Science and Machine Learning related to each other?',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.\\nData Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.\\nMachine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.\\nIn short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.',\n",
       " 'Data Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.',\n",
       " 'Machine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.',\n",
       " 'In short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '36. Explain univariate, bivariate, and multivariate analyses.',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.\\n\\nUnivariate analysis: Univariate analysis involves analysing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends out of it. Example: Analyzing the weight of a group of people.\\nBivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude.\\nMultivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable). Example: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc.\\n\\n',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.',\n",
       " '37. How can we handle missing data?',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.\\nOne way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.\\nAnother way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.\\nFinally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.\\n',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.',\n",
       " 'One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.',\n",
       " 'Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.',\n",
       " 'Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.',\n",
       " '38. What is the benefit of dimensionality reduction?',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.\\n',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.',\n",
       " '39. What is bias–variance trade-off in Data Science?',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.\\nBias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.\\n',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.',\n",
       " 'Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.',\n",
       " '40. What is RMSE?',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:\\nFirst, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.\\n',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:',\n",
       " 'First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.',\n",
       " '41. What is a kernel function in SVM?',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.\\n',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.',\n",
       " '42. How can we select an appropriate value of k in k-means?',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.\\nThis is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.\\n',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.',\n",
       " 'This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.',\n",
       " '43. How can we deal with outliers?',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.\\nIn case the outliers are not that extreme, then we can try:\\n\\nA different kind of model. For example, if we were using a linear model, then we can choose a non-linear model\\nNormalizing the data, which will shift the extreme values closer to other data points\\nUsing algorithms that are not so affected by outliers, such as random forest, etc.\\n\\n',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.',\n",
       " 'In case the outliers are not that extreme, then we can try:',\n",
       " '44. How to calculate the accuracy of a binary classification algorithm using its confusion matrix?',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:\\n\\nTrue positives: Number of observations correctly classified as True\\nTrue negatives: Number of observations correctly classified as False\\nFalse positives: Number of observations incorrectly classified as True\\nFalse negatives: Number of observations incorrectly classified as False\\n\\nTo calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:\\n',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:',\n",
       " 'To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:',\n",
       " '45. What is ensemble learning?',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.\\n',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.',\n",
       " '46. Explain collaborative filtering in recommender systems.',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.\\n',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.',\n",
       " '47. Explain content-based filtering in recommender systems.',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.\\n',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.',\n",
       " '48. Explain bagging in Data Science.',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.\\n',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.',\n",
       " '49. Explain boosting in Data Science.',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.\\n',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.',\n",
       " '50. Explain stacking in Data Science.',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.\\n',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.',\n",
       " '51. Explain how Machine Learning is different from Deep Learning.',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.\\nDeep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.\\n',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.',\n",
       " 'Deep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.',\n",
       " '52. Why does Naive Bayes have the word ‘naive’ in it?',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.\\nIt has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.\\nTo learn more about Data Science, check out our Data Science Course in Hyderabad.\\nAdvanced Data Science Interview Questions\\n',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.',\n",
       " 'It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.',\n",
       " 'To learn more about Data Science, check out our Data Science Course in Hyderabad.',\n",
       " 'To learn more about Data Science, check out our Data Science Course in Hyderabad.',\n",
       " '53. From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.',\n",
       " '\\nFirst, we will load the ggplot2 package:\\nlibrary(ggplot2)\\nNext, we will use the dplyr package:\\nlibrary(dplyr)// It is based on the grammar of data manipulation.\\nTo extract those particular records, use the below command:\\ndiamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea\\n',\n",
       " '',\n",
       " 'First, we will load the ggplot2 package:',\n",
       " 'ggplot2',\n",
       " 'Next, we will use the dplyr package:',\n",
       " 'dplyr',\n",
       " 'To extract those particular records, use the below command:',\n",
       " '54. Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on y-axis, ’carat’ should be on x-axis, and the ‘color’ of the points should be determined by ‘cut.’',\n",
       " 'We will implement the scatter plot using ggplot.\\nThe ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.\\nSo, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.\\nCode:\\n>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()\\n',\n",
       " 'We will implement the scatter plot using ggplot.',\n",
       " 'ggplot',\n",
       " 'The ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.',\n",
       " 'So, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.',\n",
       " 'Code:',\n",
       " 'Code',\n",
       " '55. Introduce 25 percent missing values in this ‘iris’ datset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’',\n",
       " '\\nTo introduce missing values, we will be using the missForest package:\\nlibrary(missForest)\\nUsing the prodNA function, we will be introducing 25 percent of missing values:\\nIris.mis<-prodNA(iris,noNA=0.25)\\nFor imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:\\nlibrary(Hmisc)\\r\\niris.mis$Sepal.Length<-with(iris.mis, impute(Sepal.Length,mean))\\r\\niris.mis$Petal.Length<-with(iris.mis, impute(Petal.Length,median))\\n',\n",
       " '',\n",
       " 'To introduce missing values, we will be using the missForest package:',\n",
       " 'missForest',\n",
       " 'Using the prodNA function, we will be introducing 25 percent of missing values:',\n",
       " 'For imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:',\n",
       " '56. Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’',\n",
       " '\\nHere, we need to find how ‘mpg’ varies w.r.t displacement of the column.\\nWe need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.\\nSo, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.\\nTherefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.\\nHere, we will use the following code:\\nlibraray(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)\\r\\n\\r\\nmtcars[split_tag,]->train\\r\\n\\r\\nmtcars[-split_tag,]->test\\r\\n\\r\\nlm(mpg-data,data=train)->mod_mtcars\\r\\n\\r\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\r\\n\\r\\n>head(pred_mtcars)\\nExplanation:\\nParameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).\\nSecond is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.\\nOnce we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.\\nSimilarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.\\nSo, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.\\nWe will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.\\nlm(mpg-data,data=train)->mod_mtcars\\nNow, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.\\nThus, we have to predict values for the test set and then store them in pred_mtcars.\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\nOutput:\\nThese are the predicted values of mpg for all of these cars.\\n\\nSo, this is how we can build simple linear model on top of this mtcars dataset.\\n',\n",
       " '',\n",
       " 'Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.',\n",
       " 'We need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.',\n",
       " 'So, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.',\n",
       " 'Therefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.',\n",
       " 'caret',\n",
       " 'createdatapartition()',\n",
       " 'Here, we will use the following code:',\n",
       " 'Explanation:',\n",
       " 'Explanation',\n",
       " 'Parameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).',\n",
       " 'Parameters of the createDataPartition function',\n",
       " 'Second is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.',\n",
       " 'Once we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.',\n",
       " 'split_tag',\n",
       " 'mtcars dataframe,',\n",
       " 'true',\n",
       " 'training',\n",
       " 'Similarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.',\n",
       " 'false',\n",
       " ' test',\n",
       " 'So, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.',\n",
       " 'We will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.',\n",
       " 'lm function',\n",
       " 'Now, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.',\n",
       " 'predict',\n",
       " 'Thus, we have to predict values for the test set and then store them in pred_mtcars.',\n",
       " 'Output:',\n",
       " 'Output',\n",
       " 'These are the predicted values of mpg for all of these cars.',\n",
       " '',\n",
       " 'So, this is how we can build simple linear model on top of this mtcars dataset.',\n",
       " '57. Calculate the RMSE values for the model built.',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\r\\n\\r\\nas.data.frame(final_data)->final_data\\r\\n\\r\\nerror<-(final_data$Actual-final_data$Prediction)\\r\\n\\r\\ncbind(final_data,error)->final_data\\r\\n\\r\\nsqrt(mean(final_data$error)^2)\\nExplanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\nOur actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:\\nas.data.frame(final_data)->final_data\\nWe will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:\\nerror<-(final_data$Actual-final_data$Prediction)\\nThen, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:\\ncbind(final_data,error)->final_data //binding error object to this final_data\\nHere, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:\\nSqrt(mean(final_data$error)^2)\\nOutput:\\n[1] 4.334423\\nNote: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.\\n',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:',\n",
       " 'Code:',\n",
       " 'Explanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:',\n",
       " 'Explanation',\n",
       " 'cbind',\n",
       " 'Our actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:',\n",
       " 'mpg',\n",
       " 'pred_mtcars',\n",
       " 'actual. ',\n",
       " 'predicted',\n",
       " ' final_data',\n",
       " 'as.data.frame',\n",
       " 'We will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:',\n",
       " 'Then, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:',\n",
       " 'error',\n",
       " 'Here, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:',\n",
       " 'Calculating RMSE',\n",
       " 'Output:',\n",
       " 'Output',\n",
       " 'Note: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.',\n",
       " 'Note',\n",
       " 'R and Python are two of the most important programming languages for Machine Learning Algorithms.',\n",
       " '58. Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’',\n",
       " 'Simple Linear Regression\\nimport pandas as pd\\r\\n\\r\\ndata=pd.read_csv(‘Boston.csv’)\\xa0\\xa0\\xa0\\xa0 //loading the Boston dataset\\r\\n\\r\\ndata.head()\\xa0 //having a glance at the head of this data\\r\\n\\r\\ndata.shape\\r\\n\\r\\n\\nLet us take out the dependent and the independent variables from the dataset:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\r\\n\\r\\ndata1.head()\\nVisualizing Variables\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nHere, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.\\nWe will separate the dependent and the independent variable from this entire dataframe:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\nThe only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.\\nNow, we would also do a visualization w.r.t to these two columns:\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nPreparing the Data\\nX=pd.Dataframe(data1[‘lstat’])\\r\\n\\r\\nY=pd.Dataframe(data1[‘medv’])\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\\r\\n\\r\\nfrom sklearn.linear_model import LinearRegression\\r\\n\\r\\nregressor=LinearRegression()\\r\\n\\r\\nregressor.fit(X_train,y_train)\\nprint(regressor.intercept_)\\nOutput :\\n34.12654201\\nprint(regressor.coef_)//this is the slope\\nOutput :\\n[[-0.913293]]\\nBy now, we have built the model. Now, we have to predict the values on top of the test set:\\ny_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in y_pred object\\r\\n\\r\\n\\nNow, let’s have a glance at the rows and columns of the actual values and the predicted values:\\nY_pred.shape, y_test.shape\\nOutput :\\n((102,1),(102,1))\\nFurther, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.\\nfrom sklearn import metrics import NumPy as np\\r\\n\\r\\nprint(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\nOutput:\\nMean Absolute Error: 4.692198\\r\\n\\r\\nMean Squared Error: 43.9198\\r\\n\\r\\nRoot Mean Squared Error: 6.6270\\n\\n',\n",
       " 'Simple Linear Regression',\n",
       " 'Simple Linear Regression',\n",
       " 'Let us take out the dependent and the independent variables from the dataset:',\n",
       " 'Visualizing Variables',\n",
       " 'Visualizing Variables',\n",
       " 'Here, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.',\n",
       " 'We will separate the dependent and the independent variable from this entire dataframe:',\n",
       " 'The only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.',\n",
       " 'Now, we would also do a visualization w.r.t to these two columns:',\n",
       " 'Preparing the Data',\n",
       " 'Preparing the Data',\n",
       " 'Output :',\n",
       " 'Output :',\n",
       " 'By now, we have built the model. Now, we have to predict the values on top of the test set:',\n",
       " 'Now, let’s have a glance at the rows and columns of the actual values and the predicted values:',\n",
       " 'Output :',\n",
       " 'Further, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.',\n",
       " 'Output:',\n",
       " 'Mean Absolute Error',\n",
       " 'Mean Squared Error',\n",
       " 'Root Mean Squared Error',\n",
       " '',\n",
       " '59. Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’',\n",
       " '\\nFor loading the dataset, we will use the read.csv function:\\nread.csv(“D:/heart.csv”)->heart\\r\\n\\r\\nstr(heart)\\nIn the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:\\nas.factor(heart$target)->heart$target\\nNow, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.\\nTo build a logistic regression model, we will use the glm function:\\nglm(target~age, data=heart, family=”binomial”)->log_mod1\\nHere, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.\\nfamily=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.\\nWe will have a glance at the summary of the model that we have just built:\\nsummary(log_mod1)\\n\\nWe can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.\\nNow, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.\\nThis null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.\\nResidual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.\\nThis basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.\\nAs we have built the model, it’s time to predict some values:\\npredict(log_mod1, data.frame(age=30), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=50), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=29:77), type=”response”)\\r\\n\\r\\n\\nNow, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:\\n>library(caret)\\r\\n\\r\\nSplit_tag<- createDataPartition(heart$target, p=0.70, list=F)\\r\\n\\r\\nheart[split_tag,]->train\\r\\n\\r\\nheart[-split_tag,]->test\\r\\n\\r\\nglm(target~age, data=train,family=”binomial”)->log_mod2\\r\\n\\r\\npredict(log_mod2, newdata=test, type=”response”)->pred_heart\\r\\n\\r\\nrange(pred_heart)\\n',\n",
       " '',\n",
       " 'For loading the dataset, we will use the read.csv function:',\n",
       " 'read.csv',\n",
       " 'In the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:',\n",
       " 'target column is supposed to be categorical',\n",
       " 'as.factor',\n",
       " 'heart$target',\n",
       " 'heart$target ',\n",
       " 'Now, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.',\n",
       " 'To build a logistic regression model, we will use the glm function:',\n",
       " 'glm ',\n",
       " 'Here, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.',\n",
       " 'target~age ',\n",
       " 'family=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.',\n",
       " 'family=”binomial”',\n",
       " 'log_mod1',\n",
       " 'We will have a glance at the summary of the model that we have just built:',\n",
       " '',\n",
       " 'We can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.',\n",
       " 'Pr',\n",
       " 'Now, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.',\n",
       " 'This null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.',\n",
       " 'Residual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.',\n",
       " 'This basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.',\n",
       " 'As we have built the model, it’s time to predict some values:',\n",
       " 'Now, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:',\n",
       " '60. Build an ROC curve for the model built.',\n",
       " 'The below code will help us in building the ROC curve:\\nlibrary(ROCR)\\r\\n\\r\\nprediction(pred_heart, test$target)-> roc_pred_heart\\r\\n\\r\\nperformance(roc_pred_heart, “tpr”, “fpr”)->roc_curve\\r\\n\\r\\nplot(roc_curve, colorize=T)\\nGraph: \\nGo through this Data Science Course in London to get a clear understanding of Data Science!\\n\\n',\n",
       " 'The below code will help us in building the ROC curve:',\n",
       " 'Graph: ',\n",
       " 'Graph:',\n",
       " 'Go through this Data Science Course in London to get a clear understanding of Data Science!',\n",
       " 'Go through this ',\n",
       " 'Data Science Course in London',\n",
       " ' to get a clear understanding of Data Science!',\n",
       " '61. Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.',\n",
       " 'Accuracy is calculated as:\\nAccuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)\\nTo build a confusion matrix in R, we will use the table function:\\ntable(test$target,pred_heart>0.6)\\nHere, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.\\nThen, we calculate the accuracy by the formula for calculating Accuracy.\\n\\n',\n",
       " 'Accuracy is calculated as:',\n",
       " 'Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)',\n",
       " 'Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)',\n",
       " 'To build a confusion matrix in R, we will use the table function:',\n",
       " 'Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.',\n",
       " 'Then, we calculate the accuracy by the formula for calculating Accuracy.',\n",
       " 'Accuracy',\n",
       " '',\n",
       " '62. Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:\\ncustomer_churn=pd.read_csv(“customer_churn.csv”)\\n\\nAfter loading this dataset, we can have a glance at the head of the dataset by using the following command:\\ncustomer_churn.head()\\nNow, we will separate the dependent and the independent variables into two separate objects:\\nx=pd.Dataframe(customer_churn[‘MonthlyCharges’])\\r\\n\\r\\ny=customer_churn[‘ Churn’]\\r\\n\\r\\n#Splitting the data into training and testing sets\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)\\nNow, we will see how to build the model and calculate log_loss.\\nfrom sklearn.linear_model, we have to import LogisticRegression\\r\\n\\r\\nl=LogisticRegression()\\r\\n\\r\\nl.fit(x_train,y_train)\\r\\n\\r\\ny_pred=l.predict_proba(x_test)\\nAs we are supposed to calculate the log_loss, we will import it from sklearn.metrics:\\nfrom sklearn.metrics import log_loss\\r\\n\\r\\nprint(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred\\nOutput:\\n0.5555020595194167\\nBecome a master of Data Science by going through this online Data Science Course in Toronto!\\n',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:',\n",
       " '',\n",
       " 'After loading this dataset, we can have a glance at the head of the dataset by using the following command:',\n",
       " 'Now, we will separate the dependent and the independent variables into two separate objects:',\n",
       " 'Now, we will see how to build the model and calculate log_loss.',\n",
       " 'log_loss',\n",
       " 'As we are supposed to calculate the log_loss, we will import it from sklearn.metrics:',\n",
       " 'sklearn.metrics',\n",
       " 'Output:',\n",
       " 'Output',\n",
       " 'Become a master of Data Science by going through this online Data Science Course in Toronto!',\n",
       " 'Become a master of Data Science by going through this online Data Science Course in Toronto!',\n",
       " '63. Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built. ',\n",
       " '\\nTo build a decision tree model, we will be loading the party package:\\n#party package\\r\\n\\r\\nlibrary(party)\\r\\n\\r\\n#splitting the data\\r\\n\\r\\nlibrary(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(iris$Species, p=0.65, list=F)\\r\\n\\r\\niris[split_tag,]->train\\r\\n\\r\\niris[~split_tag,]->test\\r\\n\\r\\n#building model\\r\\n\\r\\nmytree<-ctree(Species~.,train)\\nNow we will plot the model\\nplot(mytree)\\nModel:\\n\\n#predicting the values\\r\\n\\r\\npredict(mytree,test,type=’response’)->mypred\\nAfter this, we will predict the confusion matrix and then calculate the accuracy using the table function:\\ntable(test$Species, mypred)\\n\\n',\n",
       " '',\n",
       " 'To build a decision tree model, we will be loading the party package:',\n",
       " 'party ',\n",
       " 'Now we will plot the model',\n",
       " 'Model:\\n',\n",
       " 'Model:',\n",
       " 'After this, we will predict the confusion matrix and then calculate the accuracy using the table function:',\n",
       " '',\n",
       " '64. Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.',\n",
       " '\\nWe will load the CTG dataset by using read.csv:\\ndata<-read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)\\r\\n\\r\\nstr(data)\\nConverting the integer type to a factor\\ndata$NSP<-as.factor(data$NSP)\\r\\n\\r\\ntable(data$NSP)\\r\\n\\r\\n#data partition\\r\\n\\r\\nset.seed(123)\\r\\n\\r\\nsplit_tag<-createDataPartition(data$NSP, p=0.65, list=F)\\r\\n\\r\\ndata[split_tag,]->train\\r\\n\\r\\ndata[~split_tag,]->test\\r\\n\\r\\n#random forest -1\\r\\n\\r\\nlibrary(randomForest)\\r\\n\\r\\nset.seed(222)\\r\\n\\r\\nrf<-randomForest(NSP~.,data=train)\\r\\n\\r\\nrf\\r\\n\\r\\n#prediction\\r\\n\\r\\npredict(rf,test)->p1\\nBuilding confusion matrix and calculating accuracy:\\ntable(test$NSP,p1)\\n\\nIf you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!\\n',\n",
       " '',\n",
       " 'We will load the CTG dataset by using read.csv:',\n",
       " 'read.csv',\n",
       " 'Converting the integer type to a factor',\n",
       " 'Building confusion matrix and calculating accuracy:',\n",
       " '',\n",
       " 'If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!',\n",
       " 'If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our ',\n",
       " 'Data Science Community!',\n",
       " '65. Write a function to calculate the Euclidean distance between two points.',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:\\n√(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2))\\nCode for calculating the Euclidean distance is as given below:\\ndef euclidean_distance(P1, P2):\\r\\nreturn (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5\\n',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:',\n",
       " 'Code for calculating the Euclidean distance is as given below:',\n",
       " '66. Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.',\n",
       " 'To calculate the root mean square error (RMSE), we have to:\\n\\n Calculate the errors, i.e., the differences between the actual and the predicted values\\n Square each of these errors\\n Calculate the mean of these squared errors\\n Return the square root of the mean\\n\\nThe code in Python for calculating RMSE is given below:\\ndef rmse(actual, predicted):\\r\\n\\xa0\\xa0errors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]\\r\\n\\xa0\\xa0squared_errors = [x ** 2 for x in errors]\\r\\n\\xa0\\xa0mean = sum(squared_errors) / len(squared_errors)\\r\\n\\xa0\\xa0return mean ** .5\\r\\n\\nCheck out this Machine Learning Course to get an in-depth understanding of Machine Learning.\\n',\n",
       " 'To calculate the root mean square error (RMSE), we have to:',\n",
       " 'The code in Python for calculating RMSE is given below:',\n",
       " 'Check out this Machine Learning Course to get an in-depth understanding of Machine Learning.',\n",
       " '67. Mention the different kernel functions that can be used in SVM.',\n",
       " 'In SVM, there are four types of kernel functions:\\n\\nLinear kernel\\nPolynomial kernel\\nRadial basis kernel\\nSigmoid kernel\\n\\n',\n",
       " 'In SVM, there are four types of kernel functions:',\n",
       " '68. How to detect if the time series data is stationary?',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.\\n',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.',\n",
       " '69. Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:\\ndef accuracy_score(matrix):\\r\\n\\xa0\\xa0true_positives = matrix[0][0]\\r\\n\\xa0\\xa0true_negatives = matrix[1][1]\\r\\n\\xa0\\xa0total_observations = sum(matrix[0]) + sum(matrix[1])\\r\\n\\xa0\\xa0return (true_positives + true_negatives) / total_observations\\r\\n\\n',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:',\n",
       " '70. What does root cause analysis mean?',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.\\n',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.',\n",
       " '71. What is A/B testing?',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.\\nCheck out this Python Course to get deeper into Python programming.\\n',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.',\n",
       " 'Check out this Python Course to get deeper into Python programming.',\n",
       " '72. Out of collaborative filtering and content-based filtering, which one is considered better, and why?',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.\\nIn the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.\\n',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.',\n",
       " 'In the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.',\n",
       " '73. In the following confusion matrix, calculate precision and recall.',\n",
       " '\\n\\n\\n\\nTotal = 510\\nActual\\n\\n\\nPredicted\\n\\nP\\nN\\n\\n\\nP\\n156\\n11\\n\\n\\nN\\n16\\n327\\n\\n\\n\\n\\nThe formulae for precision and recall are given below.\\nPrecision:\\r\\n(True Positive) / (True Positive + False Positive)\\r\\nRecall:\\r\\n(True Positive) / (True Positive + False Negative)\\r\\nBased on the given data, precision and recall are:\\r\\nPrecision: 156 / (156 + 11) = 93.4\\r\\nRecall: 156 / (156 + 16) = 90.7\\r\\n\\n',\n",
       " 'The formulae for precision and recall are given below.',\n",
       " '74. Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.',\n",
       " \"We can use the below for this purpose:\\ndef calculate_precsion_and_recall(matrix):\\r\\n\\xa0\\xa0true_positive\\xa0 = matrix[0][0]\\r\\n\\xa0\\xa0false_positive\\xa0 = matrix[0][1]\\r\\n\\xa0\\xa0false_negative = matrix[1][0]\\r\\n\\xa0\\xa0return {\\r\\n\\xa0\\xa0\\xa0\\xa0'precision': (true_positive) / (true_positive + false_positive),\\r\\n\\xa0\\xa0\\xa0\\xa0'recall': (true_positive) / (true_positive + false_negative)\\r\\n\\xa0\\xa0}\\r\\n\\n\",\n",
       " 'We can use the below for this purpose:',\n",
       " '75. What is reinforcement learning?',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.\\n',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.',\n",
       " '76. Explain TF/IDF vectorization.',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.\\n',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.',\n",
       " '77. What are the assumptions required for linear regression?',\n",
       " 'There are several assumptions required for linear regression. They are as follows:\\n\\nThe data, which is a sample drawn from a population, used to train the model should be representative of the population.\\nThe relationship between independent variables and the mean of dependent variables is linear.\\nThe variance of the residual is going to be the same for any value of an independent variable. It is also represented as X.\\nEach observation is independent of all other observations.\\nFor any value of an independent variable, the independent variable is normally distributed.\\n\\n',\n",
       " 'There are several assumptions required for linear regression. They are as follows:',\n",
       " '78. What happens when some of the assumptions required for linear regression are violated?',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.\\nStrong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.\\n',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.',\n",
       " 'Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.',\n",
       " 'Course Schedule',\n",
       " '\\n19 thoughts on “Top 78 Data Science Interview Questions and Answers for 2021” ',\n",
       " 'Wow, Great collection of Data Science questions. Thanks for sharing.',\n",
       " 'Really helpful.',\n",
       " 'This data science interview questions video as well as this entire set of data science questions both are extremely helpful. Thanks a lot !',\n",
       " 'Highly updated data science interview questions. Thank you so much, these questions helped me to clear my data science interview.',\n",
       " 'Good data science interview questions. Each question explained with good answer including example and output.',\n",
       " 'Great job, very good questions. Really helped me. Thaks.',\n",
       " 'All the questions are updated with all the problems an user can face while learning data science. Video lectures were also great. Everything well explained. Keep it up..!!',\n",
       " 'All the hard work done by intellipaat is really remarkable. All the questions are really important to crack an interview. Thanks you for such a nice material.',\n",
       " 'Just wow…!! Great work, jut loved it. Want to see more stuff like this.',\n",
       " 'All 20 questions were helpful and detailed. Everything was up to the mark. Great work.',\n",
       " 'Remarkable work, I would suggest everyone to go through it. Amazing questions with every explanation in detail.',\n",
       " 'Nice detailed questions, really helpful in cracking an interview. A must read for everyone. Loved it.',\n",
       " 'It’s nice to read the latest Data Science Interview Questions and Answers for 2019',\n",
       " 'Interesting & useful Data Science Interview Q and A. I am doing data science course. so, this gives me a great view.',\n",
       " 'Simply Superb Data Science Interview Ques. It’s useful for beginners and professionals also.',\n",
       " 'All the questions were very helpful in knowing an interview pattern, well explained and detailed.',\n",
       " 'It covers all basic questions helpful in learning data science. All the work done by IntelliPaat is exceptional.',\n",
       " 'All the questions are very professional and helpful in learning data science. Recommended to everyone who’s serious to get into this Field.',\n",
       " 'All the 20 questions were really helpful and well explained. Recommended to clear data science interview. Great Work…!!',\n",
       " 'Leave a Reply Cancel reply',\n",
       " 'Your email address will not be published. Required fields are marked *',\n",
       " 'Comment ',\n",
       " 'Name * ',\n",
       " 'Email * ',\n",
       " ' \\n\\n',\n",
       " '\\n \\n',\n",
       " 'Courses',\n",
       " 'Courses',\n",
       " 'Tutorials',\n",
       " 'Interview Questions',\n",
       " 'Download Salary Trends',\n",
       " 'Learn how professionals like you got upto 100% hike!',\n",
       " 'Course Preview',\n",
       " 'Preview',\n",
       " 'Expert-Led No.1',\n",
       " 'Top 78 Data Science Interview Questions and Answers for 2021']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst11= []\n",
    "url1 = \"https://www.edureka.co/blog/interview-questions/data-science-interview-questions/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['p','strong'])\n",
    "for answer in answers:\n",
    "    lst11.append(answer.text)\n",
    "lst11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What do you understand by linear regression?',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.\\nInterested in learning Data Science? Click here to learn more in this Data Science Course!\\n\\n',\n",
       " 'Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.',\n",
       " 'Interested in learning Data Science? Click here to learn more in this Data Science Course!',\n",
       " 'Interested in learning Data Science? Click here to learn more in this Data Science Course!',\n",
       " '2. What do you understand by logistic regression?',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.\\n\\n',\n",
       " 'Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.',\n",
       " ' S',\n",
       " '3. What is a confusion matrix?',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.\\n',\n",
       " 'The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.',\n",
       " 'True Positive (d)',\n",
       " 'False Negative (c)',\n",
       " 'False Positive (b):',\n",
       " 'True Negative (a):',\n",
       " 'CTA',\n",
       " 'CTA',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:\\n Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Watch this comprehensive Data Science tutorial to learn more:',\n",
       " ' Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " '',\n",
       " '4. What do you understand by true positive rate and false positive rate?',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.\\nCheck out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.',\n",
       " 'True positive rate',\n",
       " 'Formula',\n",
       " 'False positive rate',\n",
       " 'Formula',\n",
       " 'Check out this comprehensive Data Science Course in India!\\nGet 50% Hike!Master Most in Demand Skills Now !\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Check out this comprehensive Data Science Course in India!',\n",
       " 'Master Most in Demand Skills Now !',\n",
       " '5. What is Data Science?',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.\\n\\n',\n",
       " 'Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.',\n",
       " '6. How is Data Science different from traditional application programming?',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.\\nIn traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.\\nData Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).\\nAs described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.\\n\\n\\n',\n",
       " 'Data Science takes a fundamentally different approach to building systems that provide value than traditional application development.',\n",
       " 'In traditional programming paradigms, we used to analyze the input, figure out the expected output, and write code, which contains rules and statements needed to transform the provided input into the expected output. As we can imagine, these rules were not easy to write, especially for those data that even computers had a hard time understanding, e.g., images, videos, etc.',\n",
       " 'Data Science shifts this process a little bit. In it, we need access to large volumes of data that contain the necessary inputs and their mappings to the expected outputs. Then, we use Data Science algorithms, which use mathematical analysis to generate rules to map the given inputs to outputs. This process of rule generation is called training. After training, we use some data that was set aside before the training phase to test and check the system’s accuracy. The generated rules are a kind of a black box, and we cannot understand how the inputs are being transformed into outputs. However. if the accuracy is good enough, then we can use the system (also called a model).',\n",
       " 'As described above, in traditional programming, we had to write the rules to map the input to the output, but in Data Science, the rules are automatically generated or learned from the given data. This helped solve some really difficult challenges that were being faced by several companies.',\n",
       " '',\n",
       " '7. Explain the differences between supervised and unsupervised learning.',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.\\n\\n\\n\\n\\nSupervised Learning\\nUnsupervised Learning\\n\\n\\nWorks on the data that contains both inputs and the expected output, i.e., the labeled data\\nWorks on the data that contains no mappings from input to output, i.e., the unlabeled data\\n\\n\\nUsed to create models that can be employed to predict or classify things\\nUsed to extract meaningful information out of large volumes of data\\n\\n\\nCommonly used supervised learning algorithms: Linear regression, decision tree, etc.\\nCommonly used unsupervised learning algorithms: K-means clustering, Apriori algorithm, etc.\\n\\n\\n\\n\\n\\n',\n",
       " 'Supervised and unsupervised learning are two types of Machine Learning techniques. They both allow us to build models. However, they are used for solving different kinds of problems.',\n",
       " '8. What is dimensionality reduction?',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.\\n\\n',\n",
       " 'Dimensionality reduction is the process of converting a dataset with a high number of dimensions (fields) to a dataset with a lower number of dimensions. This is done by dropping some fields or columns from the dataset. However, this is not done haphazardly. In this process, the dimensions or fields are dropped only after making sure that the remaining information will still be enough to succinctly describe similar information.',\n",
       " '9. What is bias in Data Science?',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.\\n\\n',\n",
       " 'Bias is a type of error that occurs in a Data Science model because of using an algorithm that is not strong enough to capture the underlying patterns or trends that exist in the data. In other words, this error occurs when the data is too complicated for the algorithm to understand, so it ends up building a model that makes simple assumptions. This leads to lower accuracy because of underfitting. Algorithms that can lead to high bias are linear regression, logistic regression, etc.',\n",
       " '10. Why Python is used for Data Cleaning in DS?',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0\\nPython libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. \\nLearn more about Data Cleaning in Data Science Tutorial!\\n',\n",
       " 'Data Scientists have to clean and transform the huge data sets in a form that they can work with. It’s is important to deal with the redundant data for better results by removing nonsensical outliers, malformed records, missing values, inconsistent formatting, etc.\\xa0',\n",
       " 'Python libraries such as\\xa0 Matplotlib, Pandas, Numpy, Keras, and SciPy are extensively used for Data cleaning and analysis. These libraries are used to load and clean the data doe effective analysis. For example, a CSV file named “Student” has information about the students of an institute like their names, standard, address, phone number, grades, marks, etc. ',\n",
       " 'Learn more about Data Cleaning in Data Science Tutorial!',\n",
       " 'Learn more about Data Cleaning in Data Science Tutorial!',\n",
       " '11. Why R is used in Data Visualization?',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0\\nIt has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'R provides the best ecosystem for data analysis and visualization with more than 12,000 packages in Open-source repositories. It has huge community support, which means you can easily find the solution to your problems on various platforms like StackOverflow.\\xa0',\n",
       " 'It has better data management and supports distributed computing by splitting the operations between multiple tasks and nodes, which eventually decreases the complexity and execution time of large datasets. \\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '12. What are the popular libraries used in Data Science?',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:\\n\\nTensorFlow: Supports parallel computing with impeccable library management backed by Google.\\xa0\\nSciPy: Mainly used for solving differential equations, multidimensional programming, data manipulation, and visualization through graphs and charts.\\nPandas: Used to implement the ETL(Extracting, Transforming, and Loading the datasets) capabilities in business applications.\\nMatplotlib: Being free and open-source, it can be used as a replacement for MATLAB, which results in better performance and low memory consumption.\\xa0\\nPyTorch: Best for projects which involve Machine Learning algorithms and Deep Neural Networks.\\xa0\\n\\nInterested to learn more about Data Science, check out our\\xa0Data Science Course in New York!\\n',\n",
       " 'Below are the popular libraries used for data extraction, cleaning, visualization, and deploying DS models:',\n",
       " 'Interested to learn more about Data Science, check out our\\xa0Data Science Course in New York!',\n",
       " 'Interested to learn more about Data Science, check out our\\xa0Data Science Course in New York!',\n",
       " '13. What is variance in Data Science?',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.\\n',\n",
       " 'Variance is a type of error that occurs in a Data Science model when the model ends up being too complex and learns features from data, along with the noise that exists in it. This kind of error can occur if the algorithm used to train the model has high complexity, even though the data and the underlying patterns and trends are quite easy to discover. This makes the model a very sensitive one that performs well on the training dataset but poorly on the testing dataset, and on any kind of data that the model has not yet seen. Variance generally leads to poor accuracy in testing and results in overfitting.',\n",
       " '14. What is pruning in a decision tree algorithm?',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n\\n',\n",
       " 'Pruning a decision tree is the process of removing the sections of the tree that are not necessary or are redundant. Pruning leads to a smaller decision tree, which performs better and gives higher accuracy and speed.\\n',\n",
       " '15. What is entropy in a decision tree algorithm?',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.\\nFor example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.\\n',\n",
       " 'In a decision tree algorithm, entropy is the measure of impurity or randomness. The entropy of a given dataset tells us how pure or impure the values of the dataset are. In simple terms, it tells us about the variance in the dataset.',\n",
       " 'For example, suppose we are given a box with 10 blue marbles. Then, the entropy of the box is 0 as it contains marbles of the same color, i.e., there is no impurity. If we need to draw a marble from the box, the probability of it being blue will be 1.0. However, if we replace 4 of the blue marbles with 4 red marbles in the box, then the entropy increases to 0.4 for drawing blue marbles.',\n",
       " '16. What is information gain in a decision tree algorithm?',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. \\n',\n",
       " 'When building a decision tree, at each step, we have to create a node that decides which feature we should use to split data, i.e., which feature would best separate our data so that we can make predictions. This decision is made using information gain, which is a measure of how much entropy is reduced when a particular feature is used to split the data. The feature that gives the highest information gain is the one that is chosen to split the data. ',\n",
       " '17. What is k-fold cross-validation?',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.\\n',\n",
       " 'In k-fold cross-validation, we divide the dataset into k equal parts. After this, we loop over the entire dataset k times. In each iteration of the loop, one of the k parts is used for testing, and the other k − 1 parts are used for training. Using k-fold cross-validation, each one of the k parts of the dataset ends up being used for training and testing purposes.',\n",
       " '18. Explain how a recommender system works.',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.\\nFor example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'A recommender system is a system that many consumer-facing, content-driven, online platforms employ to generate recommendations for users from a library of available content. These systems generate recommendations based on what they know about the users’ tastes from their activities on the platform.',\n",
       " 'For example, imagine that we have a movie streaming platform, similar to Netflix or Amazon Prime. If a user has previously watched and liked movies from action and horror genres, then it means that the user likes watching the movies of these genres. In that case, it would be better to recommend such movies to this particular user. These recommendations can also be generated based on what users with a similar taste like watching.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '19. What is a normal distribution?',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.\\n',\n",
       " 'Data distribution is a visualization tool to analyze how data is spread out or distributed. Data can be distributed in various ways. For instance, it could be with a bias to the left or to the right, or it could all be jumbled up. Data may also be distributed around a central value, i.e., mean, median, etc. This kind of distribution has no bias either to the left or to the right and is in the form of a bell-shaped curve. This distribution also has its mean equal to the median. This kind of distribution is called a normal distribution.',\n",
       " '20. What is Deep Learning?',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Deep Learning is a kind of Machine Learning, in which neural networks are used to imitate the structure of the human brain, and just like how a brain learns from information, machines are also made to learn from the information that is provided to them. Deep Learning is an advanced version of neural networks to make machines learn from data. In Deep Learning, the neural networks comprise many hidden layers (which is why it is called ‘deep’ learning) that are connected to each other, and the output of the previous layer is the input of the current layer.\\n\\nCareer Transition\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       " 'Career Transition',\n",
       " '21. What is an RNN (recurrent neural network)?',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.\\n',\n",
       " 'A recurrent neural network, or RNN for short, is a kind of Machine Learning algorithm that makes use of the artificial neural network. RNNs are used to find patterns from a sequence of data, such as time series, stock market, temperature, etc. RNNs are a kind of feedforward network, in which information from one layer passes to another layer, and each node in the network performs mathematical operations on the data. These operations are temporal, i.e., RNNs store contextual information about previous computations in the network. It is called recurrent because it performs the same operations on some data every time it is passed. However, the output may be different based on past computations and their results.',\n",
       " '22. Explain selection bias.',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.\\n\\nIntermediate Data Science Interview Questions\\n',\n",
       " 'Selection bias is the bias that occurs during the sampling of data. This kind of bias occurs when a sample is not representative of the population, which is going to be analyzed in a statistical study.',\n",
       " '23. What is ROC curve?',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: \\n',\n",
       " 'It stands for Receiver Operating Characteristic. It is basically a plot between a true positive rate and a false positive rate, and it helps us to find out the right tradeoff between the true positive rate and the false positive rate for different probability thresholds of the predicted values. So, the closer the curve to the upper left corner, the better the model is. In other words, whichever curve has greater area under it that would be the better model. You can see this in the below graph: ',\n",
       " 'Receiver Operating Characteristic',\n",
       " '24. What do you understand by a decision tree?',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.\\nAre you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!\\n',\n",
       " 'A decision tree is a supervised learning algorithm that is used for both classification and regression. Hence, in this case, the dependent variable can be both a numerical value and a categorical value.  Here, each node denotes the test on an attribute, and each edge denotes the outcome of that attribute, and each leaf node holds the class label. So, in this case, we have a series of test conditions which gives the final decision according to the condition.',\n",
       " 'Are you interested in learning Data Science from experts? Enroll in our Data Science Course in Bangalore now!',\n",
       " 'Are you interested in learning Data Science from experts? Enroll in our ',\n",
       " 'Data Science Course in Bangalore now!',\n",
       " '25. What do you understand by a random forest model?',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.\\n\\n',\n",
       " 'It combines multiple models together to get the final output or, to be more precise, it combines multiple decision trees together to get the final output. So, decision trees are the building blocks of the random forest model.',\n",
       " '',\n",
       " '26. Two candidates Aman and Mohan appear for a Data Science Job interview. The probability of Aman cracking the interview is 1/8 and that of Mohan is 5/12. What is the probability that at least of them will crack the interview?',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12\\nNow, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)\\nWhere P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)\\n1/8 * 5/12\\n5/96\\nNow, put the value of P(A ∩ B) into equation 1\\nP(A U B) =P(A)+ P(B) – (P(A ∩ B))\\n1/8 + 5/12 -5/96\\nSo, the answer will be 47/96.\\n',\n",
       " 'The probability of Aman getting selected for the interview is 1/8\\nP(A) = 1/8\\nThe probability of Mohan getting selected for the interview is 5/12\\nP(B)=5/12',\n",
       " 'Now, the probability of at least one of them getting selected can be denoted at the Union of A and B, which means',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B)) ………………………(1)',\n",
       " 'Where P(A ∩ B) stands for the probability of both Aman and Mohan getting selected for the job.\\nTo calculate the final answer, we first have to find out the value of P(A ∩ B)\\nSo, P(A ∩ B) = P(A) * P(B)',\n",
       " '1/8 * 5/12',\n",
       " '5/96',\n",
       " 'Now, put the value of P(A ∩ B) into equation 1',\n",
       " 'P(A U B) =P(A)+ P(B) – (P(A ∩ B))',\n",
       " '1/8 + 5/12 -5/96',\n",
       " 'So, the answer will be 47/96.',\n",
       " '27. How is Data modeling different from Database design?',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.\\n',\n",
       " 'Data Modeling: It can be considered as the first step towards the design of a database. Data modeling creates a conceptual model based on the relationship between various data models. The process involves moving from the conceptual stage to the logical model to the physical schema. It involves the systematic method of applying data modeling techniques. Database Design: This is the process of designing the database. The database design creates an output which is a detailed data model of the database. Strictly speaking, database design includes the detailed logical model of a database but it can also include physical design choices and storage parameters.',\n",
       " 'Data Modeling',\n",
       " 'Database Design',\n",
       " '28. What are precision?',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:\\n\\n',\n",
       " 'Precision: When we are implementing algorithms for the classification of data or the retrieval of information, precision helps us get a portion of positive class values that are positively predicted. Basically, it measures the accuracy of correct positive predictions. Below is the formula to calculate precision:',\n",
       " 'Precision',\n",
       " '',\n",
       " '29. What is recall?',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:\\n\\n',\n",
       " 'Recall: It is the set of all positive predictions out of the total number of positive instances. Recall helps us identify the misclassified positive predictions. We use the below formula to calculate recall:',\n",
       " 'Recall',\n",
       " '',\n",
       " '30. What is the F1 score and how to calculate it?',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: \\n',\n",
       " 'F1 score helps us calculate the harmonic mean of precision and recall that gives us the test’s accuracy. If F1 = 1, then precision and recall are accurate. If F1 < 1 or equal to 0, then precision or recall is less accurate, or they are completely inaccurate. See below for the formula to calculate the F1 score: ',\n",
       " '31. What is p-value? ',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.\\n',\n",
       " 'P-value is the measure of the statistical importance of an observation. It is the probability that shows the significance of output to the data. We compute the p-value to know the test statistics of a model. Typically, it helps us choose whether we can accept or reject the null hypothesis.',\n",
       " '32. Why do we use p-value?',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:\\n\\n',\n",
       " 'We use the p-value to understand whether the given data really describe the observed effect or not. We use the below formula to calculate the p-value for the effect ‘E’ and the null hypothesis ‘H0’ as true:',\n",
       " '',\n",
       " '33. What is the difference between an error and a residual error?',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.\\n',\n",
       " 'An error occurs in values while the prediction gives us the difference between the observed values and the true values of a dataset. Whereas, the residual error\\xa0is the difference between the observed values and the predicted values. The reason we use the residual error to evaluate the performance of an algorithm is that the true values are never known. Hence, we use the observed values to measure the error using residuals. It helps us get an accurate estimate of the error.',\n",
       " 'error',\n",
       " 'residual error',\n",
       " '34. Why do we use the summary function?',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.\\n',\n",
       " 'The summary function in R gives us the statistics of the implemented algorithm on a particular dataset. It consists of various objects, variables, data attributes, etc. It provides summary statistics for individual objects when fed into the function. We use a summary function when we want information about the values present in the dataset. It gives us the summary statistics in the following form:  Here, it gives the minimum and maximum values from a specific column of the dataset. Also, it provides the median, mean, 1st quartile, and 3rd quartile values that help us understand the values better.',\n",
       " '35. How are Data Science and Machine Learning related to each other?',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.\\nData Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.\\nMachine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.\\nIn short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n\\n',\n",
       " 'Data Science and Machine Learning are two terms that are closely related but are often misunderstood. Both of them deal with data. However, there are some fundamental distinctions that show us how they are different from each other.',\n",
       " 'Data Science is a broad field that deals with large volumes of data and allows us to draw insights out of this voluminous data. The entire process of Data Science takes care of multiple steps that are involved in drawing insights out of the available data. This process includes crucial steps such as data gathering, data analysis, data manipulation, data visualization, etc.',\n",
       " 'Machine Learning, on the other hand, can be thought of as a sub-field of Data Science. It also deals with data, but here, we are solely focused on learning how to convert the processed data into a functional model, which can be used to map inputs to outputs, e.g., a model that can expect an image as an input and tell us if that image contains a flower as an output.',\n",
       " 'In short, Data Science deals with gathering data, processing it, and finally, drawing insights from it. The field of Data Science that deals with building models using algorithms is called Machine Learning. Therefore, Machine Learning is an integral part of Data Science.\\n\\nCourses you may like\\n\\n\\n\\n\\n',\n",
       " 'Courses you may like',\n",
       " '36. Explain univariate, bivariate, and multivariate analyses.',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.\\n\\nUnivariate analysis: Univariate analysis involves analysing data with only one variable or, in other words, a single column or a vector of the data. This analysis allows us to understand the data and extract patterns and trends out of it. Example: Analyzing the weight of a group of people.\\nBivariate analysis: Bivariate analysis involves analyzing the data with exactly two variables or, in other words, the data can be put into a two-column table. This kind of analysis allows us to figure out the relationship between the variables. Example: Analyzing the data that contains temperature and altitude.\\nMultivariate analysis: Multivariate analysis involves analyzing the data with more than two variables. The number of columns of the data can be anything more than two. This kind of analysis allows us to figure out the effects of all other variables (input variables) on a single variable (the output variable). Example: Analyzing data about house prices, which contains information about the houses, such as locality, crime rate, area, the number of floors, etc.\\n\\n',\n",
       " 'When we are dealing with data analysis, we often come across terms such as univariate, bivariate, and multivariate. Let’s try and understand what these mean.',\n",
       " '37. How can we handle missing data?',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.\\nOne way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.\\nAnother way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.\\nFinally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.\\n',\n",
       " 'To be able to handle missing data, we first need to know the percentage of data missing in a particular column so that we can choose an appropriate strategy to handle the situation. For example, if in a column the majority of the data is missing, then dropping the column is the best option, unless we have some means to make educated guesses about the missing values. However, if the amount of missing data is low, then we have several strategies to fill them up.',\n",
       " 'One way would be to fill them all up with a default value or a value that has the highest frequency in that column, such as 0 or 1, etc. This may be useful if the majority of the data in that column contain these values.',\n",
       " 'Another way is to fill up the missing values in the column with the mean of all the values in that column. This technique is usually preferred as the missing values have a higher chance of being closer to the mean than to the mode.',\n",
       " 'Finally, if we have a huge dataset and a few rows have values missing in some columns, then the easiest and fastest way is to drop those columns. Since the dataset is large, dropping a few columns should not be a problem in any way.',\n",
       " '38. What is the benefit of dimensionality reduction?',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.\\n',\n",
       " 'Dimensionality reduction reduces the dimensions and size of the entire dataset. It drops unnecessary features while retaining the overall information in the data intact. Reduction in dimensions leads to faster processing of the data. The reason why data with high dimensions is considered so difficult to deal with is that it leads to high time-consumption while processing the data and training a model on it. Reducing dimensions speeds up this process, removes noise, and also leads to better model accuracy.',\n",
       " '39. What is bias–variance trade-off in Data Science?',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.\\nBias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.\\n',\n",
       " 'When building a model using Data Science or Machine Learning, our goal is to build one that has low bias and variance. We know that bias and variance are both errors that occur due to either an overly simplistic model or an overly complicated model. Therefore, when we are building a model, the goal of getting high accuracy is only going to be accomplished if we are aware of the tradeoff between bias and variance.',\n",
       " 'Bias is an error that occurs when a model is too simple to capture the patterns in a dataset. To reduce bias, we need to make our model more complex. Although making our model more complex can lead to reducing bias, if we make our model too complex, it may end up becoming too rigid, leading to high variance. So, the tradeoff between bias and variance is that if we increase the complexity, we reduce bias and increase variance, and if we reduce complexity, then we increase bias and reduce variance. Our goal is to find a point at which our model is complex enough to give low bias but not so complex to end up having high variance.',\n",
       " '40. What is RMSE?',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:\\nFirst, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.\\n',\n",
       " 'RMSE stands for the root mean square error. It is a measure of accuracy in regression. RMSE allows us to calculate the magnitude of error produced by a regression model. The way RMSE is calculated is as follows:',\n",
       " 'First, we calculate the errors in the predictions made by the regression model. For this, we calculate the differences between the actual and the predicted values. Then, we square the errors. After this step, we calculate the mean of the squared errors, and finally, we take the square root of the mean of these squared errors. This number is the RMSE, and a model with a lower value of RMSE is considered to produce lower errors, i.e., the model will be more accurate.',\n",
       " '41. What is a kernel function in SVM?',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.\\n',\n",
       " 'In the SVM algorithm, a kernel function is a special mathematical function. In simple terms, a kernel function takes data as input and converts it into a required form. This transformation of the data is based on something called a kernel trick, which is what gives the kernel function its name. Using the kernel function, we can transform the data that is not linearly separable (cannot be separated using a straight line) into one that is linearly separable.',\n",
       " '42. How can we select an appropriate value of k in k-means?',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.\\nThis is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.\\n',\n",
       " 'Selecting the correct value of k is an important aspect of k-means clustering. We can make use of the elbow method to pick the appropriate k value. To do this, we run the k-means algorithm on a range of values, e.g., 1 to 15. For each value of k, we compute an average score. This score is also called inertia or the inter-cluster variance.',\n",
       " 'This is calculated as the sum of squares of the distances of all values in a cluster. As k starts from a low value and goes up to a high value, we start seeing a sharp decrease in the inertia value. After a certain value of k, in the range, the drop in the inertia value becomes quite small. This is the value of k that we need to choose for the k-means clustering algorithm.',\n",
       " '43. How can we deal with outliers?',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.\\nIn case the outliers are not that extreme, then we can try:\\n\\nA different kind of model. For example, if we were using a linear model, then we can choose a non-linear model\\nNormalizing the data, which will shift the extreme values closer to other data points\\nUsing algorithms that are not so affected by outliers, such as random forest, etc.\\n\\n',\n",
       " 'Outliers can be dealt with in several ways. One way is to drop them. We can only drop the outliers if they have values that are incorrect or extreme. For example, if a dataset with the weights of babies has a value 98.6-degree Fahrenheit, then it is incorrect. Now, if the value is 187 kg, then it is an extreme value, which is not useful for our model.',\n",
       " 'In case the outliers are not that extreme, then we can try:',\n",
       " '44. How to calculate the accuracy of a binary classification algorithm using its confusion matrix?',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:\\n\\nTrue positives: Number of observations correctly classified as True\\nTrue negatives: Number of observations correctly classified as False\\nFalse positives: Number of observations incorrectly classified as True\\nFalse negatives: Number of observations incorrectly classified as False\\n\\nTo calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:\\n',\n",
       " 'In a binary classification algorithm, we have only two labels, which are True and False. Before we can calculate the accuracy, we need to understand a few key terms:',\n",
       " 'To calculate the accuracy, we need to divide the sum of the correctly classified observations by the number of total observations. This can be expressed as follows:',\n",
       " '45. What is ensemble learning?',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.\\n',\n",
       " 'When we are building models using Data Science and Machine Learning, our goal is to get a model that can understand the underlying trends in the training data and can make predictions or classifications with a high level of accuracy. However, sometimes some datasets are very complex, and it is difficult for one model to be able to grasp the underlying trends in these datasets. In such situations, we combine several individual models together to improve performance. This is what is called ensemble learning.',\n",
       " '46. Explain collaborative filtering in recommender systems.',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.\\n',\n",
       " 'Collaborative filtering is a technique used to build recommender systems. In this technique, to generate recommendations, we make use of data about the likes and dislikes of users similar to other users. This similarity is estimated based on several varying factors, such as age, gender, locality, etc. If User A, similar to User B, watched and liked a movie, then that movie will be recommended to User B, and similarly, if User B watched and liked a movie, then that would be recommended to User A. In other words, the content of the movie does not matter much. When recommending it to a user what matters is if other users similar to that particular user liked the content of the movie or not.',\n",
       " '47. Explain content-based filtering in recommender systems.',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.\\n',\n",
       " 'Content-based filtering is one of the techniques used to build recommender systems. In this technique, recommendations are generated by making use of the properties of the content that a user is interested in. For example, if a user is watching movies belonging to the action and mystery genre and giving them good ratings, it is a clear indication that the user likes movies of this kind. If shown movies of a similar genre as recommendations, there is a higher probability that the user would like those recommendations as well. In other words, here, the content of the movie is taken into consideration when generating recommendations for users.',\n",
       " '48. Explain bagging in Data Science.',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.\\n',\n",
       " 'Bagging is an ensemble learning method. It stands for bootstrap aggregating. In this technique, we generate some data using the bootstrap method, in which we use an already existing dataset and generate multiple samples of the N size. This bootstrapped data is then used to train multiple models in parallel, which makes the bagging model more robust than a simple model. Once all the models are trained, when we have to make a prediction, we make predictions using all the trained models and then average the result in the case of regression, and for classification, we choose the result, generated by models, that has the highest frequency.',\n",
       " '49. Explain boosting in Data Science.',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.\\n',\n",
       " 'Boosting is one of the ensemble learning methods. Unlike bagging, it is not a technique used to parallelly train our models. In boosting, we create multiple models and sequentially train them by combining weak models iteratively in a way that training a new model depends on the models trained before it. In doing so, we take the patterns learned by a previous model and test them on a dataset when training the new model. In each iteration, we give more importance to observations in the dataset that are incorrectly handled or predicted by previous models. Boosting is useful in reducing bias in models as well.',\n",
       " '50. Explain stacking in Data Science.',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.\\n',\n",
       " 'Just like bagging and boosting, stacking is also an ensemble learning method. In bagging and boosting, we could only combine weak models that used the same learning algorithms, e.g., logistic regression. These models are called homogeneous learners. However, in stacking, we can combine weak models that use different learning algorithms as well. These learners are called heterogeneous learners. Stacking works by training multiple (and different) weak models or learners and then using them together by training another model, called a meta-model, to make predictions based on the multiple outputs of predictions returned by these multiple weak models.',\n",
       " '51. Explain how Machine Learning is different from Deep Learning.',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.\\nDeep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.\\n',\n",
       " 'A field of computer science, Machine Learning is a subfield of Data Science that deals with using existing data to help systems automatically learn new skills to perform different tasks without having rules to be explicitly programmed.',\n",
       " 'Deep Learning, on the other hand, is a field in Machine Learning that deals with building Machine Learning models using algorithms that try to imitate the process of how the human brain learns from the information in a system for it to attain new capabilities. In Deep Learning, we make heavy use of deeply connected neural networks with many layers.',\n",
       " '52. Why does Naive Bayes have the word ‘naive’ in it?',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.\\nIt has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.\\nTo learn more about Data Science, check out our Data Science Course in Hyderabad.\\nAdvanced Data Science Interview Questions\\n',\n",
       " 'Naive Bayes is a Data Science algorithm. It has the word ‘Bayes’ in it because it is based on the Bayes theorem, which deals with the probability of an event occurring given that another event has already occurred.',\n",
       " 'It has ‘naive’ in it because it makes the assumption that each variable in the dataset is independent of the other. This kind of assumption is unrealistic for real-world data. However, even with this assumption, it is very useful for solving a range of complicated problems, e.g., spam email classification, etc.',\n",
       " 'To learn more about Data Science, check out our Data Science Course in Hyderabad.',\n",
       " 'To learn more about Data Science, check out our Data Science Course in Hyderabad.',\n",
       " '53. From the below given ‘diamonds’ dataset, extract only those rows where the ‘price’ value is greater than 1000 and the ‘cut’ is ideal.',\n",
       " '\\nFirst, we will load the ggplot2 package:\\nlibrary(ggplot2)\\nNext, we will use the dplyr package:\\nlibrary(dplyr)// It is based on the grammar of data manipulation.\\nTo extract those particular records, use the below command:\\ndiamonds %>% filter(price>1000 & cut==”Ideal”)-> diamonds_1000_idea\\n',\n",
       " '',\n",
       " 'First, we will load the ggplot2 package:',\n",
       " 'ggplot2',\n",
       " 'Next, we will use the dplyr package:',\n",
       " 'dplyr',\n",
       " 'To extract those particular records, use the below command:',\n",
       " '54. Make a scatter plot between ‘price’ and ‘carat’ using ggplot. ‘Price’ should be on y-axis, ’carat’ should be on x-axis, and the ‘color’ of the points should be determined by ‘cut.’',\n",
       " 'We will implement the scatter plot using ggplot.\\nThe ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.\\nSo, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.\\nCode:\\n>ggplot(data=diamonds, aes(x=caret, y=price, col=cut))+geom_point()\\n',\n",
       " 'We will implement the scatter plot using ggplot.',\n",
       " 'ggplot',\n",
       " 'The ggplot is based on the grammar of data visualization, and it helps us stack multiple layers on top of each other.',\n",
       " 'So, we will start with the data layer, and on top of the data layer we will stack the aesthetic layer. Finally, on top of the aesthetic layer we will stack the geometry layer.',\n",
       " 'Code:',\n",
       " 'Code',\n",
       " '55. Introduce 25 percent missing values in this ‘iris’ datset and impute the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median.’',\n",
       " '\\nTo introduce missing values, we will be using the missForest package:\\nlibrary(missForest)\\nUsing the prodNA function, we will be introducing 25 percent of missing values:\\nIris.mis<-prodNA(iris,noNA=0.25)\\nFor imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:\\nlibrary(Hmisc)\\r\\niris.mis$Sepal.Length<-with(iris.mis, impute(Sepal.Length,mean))\\r\\niris.mis$Petal.Length<-with(iris.mis, impute(Petal.Length,median))\\n',\n",
       " '',\n",
       " 'To introduce missing values, we will be using the missForest package:',\n",
       " 'missForest',\n",
       " 'Using the prodNA function, we will be introducing 25 percent of missing values:',\n",
       " 'For imputing the ‘Sepal.Length’ column with ‘mean’ and the ‘Petal.Length’ column with ‘median,’ we will be using the Hmisc package and the impute function:',\n",
       " '56. Implement simple linear regression in R on this ‘mtcars’ dataset, where the dependent variable is ‘mpg’ and the independent variable is ‘disp.’',\n",
       " '\\nHere, we need to find how ‘mpg’ varies w.r.t displacement of the column.\\nWe need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.\\nSo, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.\\nTherefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.\\nHere, we will use the following code:\\nlibraray(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(mtcars$mpg, p=0.65, list=F)\\r\\n\\r\\nmtcars[split_tag,]->train\\r\\n\\r\\nmtcars[-split_tag,]->test\\r\\n\\r\\nlm(mpg-data,data=train)->mod_mtcars\\r\\n\\r\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\r\\n\\r\\n>head(pred_mtcars)\\nExplanation:\\nParameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).\\nSecond is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.\\nOnce we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.\\nSimilarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.\\nSo, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.\\nWe will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.\\nlm(mpg-data,data=train)->mod_mtcars\\nNow, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.\\nThus, we have to predict values for the test set and then store them in pred_mtcars.\\npredict(mod_mtcars,newdata=test)->pred_mtcars\\nOutput:\\nThese are the predicted values of mpg for all of these cars.\\n\\nSo, this is how we can build simple linear model on top of this mtcars dataset.\\n',\n",
       " '',\n",
       " 'Here, we need to find how ‘mpg’ varies w.r.t displacement of the column.',\n",
       " 'We need to divide this data into the training dataset and the testing dataset so that the model does not overfit the data.',\n",
       " 'So, what happens is when we do not divide the dataset into these two components, it overfits the dataset. Hence, when we add new data, it fails miserably on that new data.',\n",
       " 'Therefore, to divide this dataset, we would require the caret package. This caret package comprises the createdatapartition() function. This function will give the true or false labels.',\n",
       " 'caret',\n",
       " 'createdatapartition()',\n",
       " 'Here, we will use the following code:',\n",
       " 'Explanation:',\n",
       " 'Explanation',\n",
       " 'Parameters of the createDataPartition function: First is the column which determines the split (it is the mpg column).',\n",
       " 'Parameters of the createDataPartition function',\n",
       " 'Second is the split ratio which is 0.65, i.e., 65 percent of records will have true labels and 35 percent will have false labels. We will store this in split_tag object.',\n",
       " 'Once we have split_tag object ready, from this entire mtcars dataframe, we will select all those records where the split tag value is true and store those records in the training set.',\n",
       " 'split_tag',\n",
       " 'mtcars dataframe,',\n",
       " 'true',\n",
       " 'training',\n",
       " 'Similarly, from the mtcars dataframe, we will select all those record where the split_tag value is false and store those records in the test set.',\n",
       " 'false',\n",
       " ' test',\n",
       " 'So, the split tag will have true values in it, and when we put ‘-’ symbol in front of it, ‘-split_tag’ will contain all of the false labels. We will select all those records and store them in the test set.',\n",
       " 'We will go ahead and build a model on top of the training set, and for the simple linear model we will require the lm function.',\n",
       " 'lm function',\n",
       " 'Now, we have built the model on top of the train set. It’s time to predict the values on top of the test set. For that, we will use the predict function that takes in two parameters: first is the model which we have built and second is the dataframe on which we have to predict values.',\n",
       " 'predict',\n",
       " 'Thus, we have to predict values for the test set and then store them in pred_mtcars.',\n",
       " 'Output:',\n",
       " 'Output',\n",
       " 'These are the predicted values of mpg for all of these cars.',\n",
       " '',\n",
       " 'So, this is how we can build simple linear model on top of this mtcars dataset.',\n",
       " '57. Calculate the RMSE values for the model built.',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\r\\n\\r\\nas.data.frame(final_data)->final_data\\r\\n\\r\\nerror<-(final_data$Actual-final_data$Prediction)\\r\\n\\r\\ncbind(final_data,error)->final_data\\r\\n\\r\\nsqrt(mean(final_data$error)^2)\\nExplanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:\\ncbind(Actual=test$mpg, predicted=pred_mtcars)->final_data\\nOur actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:\\nas.data.frame(final_data)->final_data\\nWe will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:\\nerror<-(final_data$Actual-final_data$Prediction)\\nThen, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:\\ncbind(final_data,error)->final_data //binding error object to this final_data\\nHere, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:\\nSqrt(mean(final_data$error)^2)\\nOutput:\\n[1] 4.334423\\nNote: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.\\n',\n",
       " 'When we build a regression model, it predicts certain y values associated with the given x values, but there is always an error associated with this prediction. So, to get an estimate of the average error in prediction, RMSE is used. Code:',\n",
       " 'Code:',\n",
       " 'Explanation: We have the actual and the predicted values. We will bind both of them into a single dataframe. For that, we will use the cbind function:',\n",
       " 'Explanation',\n",
       " 'cbind',\n",
       " 'Our actual values are present in the mpg column from the test set, and our predicted values are stored in the pred_mtcars object which we have created in the previous question. Hence, we will create this new column and name the column actual. Similarly, we will create another column and name it predicted which will have predicted values and then store the predicted values in the new object which is final_data. After that, we will convert a matrix into a dataframe. So, we will use the as.data.frame function and convert this object (predicted values) into a dataframe:',\n",
       " 'mpg',\n",
       " 'pred_mtcars',\n",
       " 'actual. ',\n",
       " 'predicted',\n",
       " ' final_data',\n",
       " 'as.data.frame',\n",
       " 'We will pass this object which is final_data and store the result in final_data again. We will then calculate the error in prediction for each of the records by subtracting the predicted values from the actual values:',\n",
       " 'Then, store this result on a new object and name that object as error. After this, we will bind this error calculated to the same final_data dataframe:',\n",
       " 'error',\n",
       " 'Here, we bind the error object to this final_data, and store this into final_data again. Calculating RMSE:',\n",
       " 'Calculating RMSE',\n",
       " 'Output:',\n",
       " 'Output',\n",
       " 'Note: Lower the value of RMSE, the better the model. R and Python are two of the most important programming languages for Machine Learning Algorithms.',\n",
       " 'Note',\n",
       " 'R and Python are two of the most important programming languages for Machine Learning Algorithms.',\n",
       " '58. Implement simple linear regression in Python on this ‘Boston’ dataset where the dependent variable is ‘medv’ and the independent variable is ‘lstat.’',\n",
       " 'Simple Linear Regression\\nimport pandas as pd\\r\\n\\r\\ndata=pd.read_csv(‘Boston.csv’)\\xa0\\xa0\\xa0\\xa0 //loading the Boston dataset\\r\\n\\r\\ndata.head()\\xa0 //having a glance at the head of this data\\r\\n\\r\\ndata.shape\\r\\n\\r\\n\\nLet us take out the dependent and the independent variables from the dataset:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\r\\n\\r\\ndata1.head()\\nVisualizing Variables\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nHere, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.\\nWe will separate the dependent and the independent variable from this entire dataframe:\\ndata1=data.loc[:,[‘lstat’,’medv’]]\\nThe only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.\\nNow, we would also do a visualization w.r.t to these two columns:\\nimport matplotlib.pyplot as plt\\r\\n\\r\\ndata1.plot(x=’lstat’,y=’medv’,style=’o’)\\r\\n\\r\\nplt.xlabel(‘lstat’)\\r\\n\\r\\nplt.ylabel(‘medv’)\\r\\n\\r\\nplt.show()\\nPreparing the Data\\nX=pd.Dataframe(data1[‘lstat’])\\r\\n\\r\\nY=pd.Dataframe(data1[‘medv’])\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100)\\r\\n\\r\\nfrom sklearn.linear_model import LinearRegression\\r\\n\\r\\nregressor=LinearRegression()\\r\\n\\r\\nregressor.fit(X_train,y_train)\\nprint(regressor.intercept_)\\nOutput :\\n34.12654201\\nprint(regressor.coef_)//this is the slope\\nOutput :\\n[[-0.913293]]\\nBy now, we have built the model. Now, we have to predict the values on top of the test set:\\ny_pred=regressor.predict(X_test)//using the instance and the predict function and pass the X_test object inside the function and store this in y_pred object\\r\\n\\r\\n\\nNow, let’s have a glance at the rows and columns of the actual values and the predicted values:\\nY_pred.shape, y_test.shape\\nOutput :\\n((102,1),(102,1))\\nFurther, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.\\nfrom sklearn import metrics import NumPy as np\\r\\n\\r\\nprint(‘Mean Absolute Error: ’, metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Mean Squared Error: ’, metrics.mean_squared_error(y_test, y_pred))\\r\\n\\r\\nprint(‘Root Mean Squared Error: ’, np.sqrt(metrics.mean_absolute_error(y_test, y_pred))\\r\\n\\nOutput:\\nMean Absolute Error: 4.692198\\r\\n\\r\\nMean Squared Error: 43.9198\\r\\n\\r\\nRoot Mean Squared Error: 6.6270\\n\\n',\n",
       " 'Simple Linear Regression',\n",
       " 'Simple Linear Regression',\n",
       " 'Let us take out the dependent and the independent variables from the dataset:',\n",
       " 'Visualizing Variables',\n",
       " 'Visualizing Variables',\n",
       " 'Here, ‘medv’ is basically the median values of the price of the houses, and we are trying to find out the median values of the price of the houses w.r.t to the lstat column.',\n",
       " 'We will separate the dependent and the independent variable from this entire dataframe:',\n",
       " 'The only columns we want from all of this record are ‘lstat’ and ‘medv,’ and we need to store these results in data1.',\n",
       " 'Now, we would also do a visualization w.r.t to these two columns:',\n",
       " 'Preparing the Data',\n",
       " 'Preparing the Data',\n",
       " 'Output :',\n",
       " 'Output :',\n",
       " 'By now, we have built the model. Now, we have to predict the values on top of the test set:',\n",
       " 'Now, let’s have a glance at the rows and columns of the actual values and the predicted values:',\n",
       " 'Output :',\n",
       " 'Further, we will go ahead and calculate some metrics so that we can find out the Mean Absolute Error, Mean Squared Error, and RMSE.',\n",
       " 'Output:',\n",
       " 'Mean Absolute Error',\n",
       " 'Mean Squared Error',\n",
       " 'Root Mean Squared Error',\n",
       " '',\n",
       " '59. Implement logistic regression on this ‘heart’ dataset in R where the dependent variable is ‘target’ and the independent variable is ‘age.’',\n",
       " '\\nFor loading the dataset, we will use the read.csv function:\\nread.csv(“D:/heart.csv”)->heart\\r\\n\\r\\nstr(heart)\\nIn the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:\\nas.factor(heart$target)->heart$target\\nNow, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.\\nTo build a logistic regression model, we will use the glm function:\\nglm(target~age, data=heart, family=”binomial”)->log_mod1\\nHere, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.\\nfamily=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.\\nWe will have a glance at the summary of the model that we have just built:\\nsummary(log_mod1)\\n\\nWe can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.\\nNow, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.\\nThis null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.\\nResidual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.\\nThis basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.\\nAs we have built the model, it’s time to predict some values:\\npredict(log_mod1, data.frame(age=30), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=50), type=”response”)\\r\\n\\r\\npredict(log_mod1, data.frame(age=29:77), type=”response”)\\r\\n\\r\\n\\nNow, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:\\n>library(caret)\\r\\n\\r\\nSplit_tag<- createDataPartition(heart$target, p=0.70, list=F)\\r\\n\\r\\nheart[split_tag,]->train\\r\\n\\r\\nheart[-split_tag,]->test\\r\\n\\r\\nglm(target~age, data=train,family=”binomial”)->log_mod2\\r\\n\\r\\npredict(log_mod2, newdata=test, type=”response”)->pred_heart\\r\\n\\r\\nrange(pred_heart)\\n',\n",
       " '',\n",
       " 'For loading the dataset, we will use the read.csv function:',\n",
       " 'read.csv',\n",
       " 'In the structure of this dataframe, most of the values are integers. However, since we are building a logistic regression model on top of this dataset, the final target column is supposed to be categorical. It cannot be an integer. So, we will go ahead and convert them into a factor.\\nThus, we will use the as.factor function and convert these integer values into categorical data.\\nWe will pass on heart$target column over here and store the result in heart$target as follows:',\n",
       " 'target column is supposed to be categorical',\n",
       " 'as.factor',\n",
       " 'heart$target',\n",
       " 'heart$target ',\n",
       " 'Now, we will build a logistic regression model and see the different probability values for the person to have heart disease on the basis of different age values.',\n",
       " 'To build a logistic regression model, we will use the glm function:',\n",
       " 'glm ',\n",
       " 'Here, target~age indicates that the target is the dependent variable and the age is the independent variable, and we are building this model on top of the dataframe.',\n",
       " 'target~age ',\n",
       " 'family=”binomial” means we are basically telling R that this is the logistic regression model, and we will store the result in log_mod1.',\n",
       " 'family=”binomial”',\n",
       " 'log_mod1',\n",
       " 'We will have a glance at the summary of the model that we have just built:',\n",
       " '',\n",
       " 'We can see Pr value here, and there are three stars associated with this Pr value. This basically means that we can reject the null hypothesis which states that there is no relationship between the age and the target columns. But since we have three stars over here, this null hypothesis can be rejected. There is a strong relationship between the age column and the target column.',\n",
       " 'Pr',\n",
       " 'Now, we have other parameters like null deviance and residual deviance. Lower the deviance value, the better the model.',\n",
       " 'This null deviance basically tells the deviance of the model, i.e., when we don’t have any independent variable and we are trying to predict the value of the target column with only the intercept. When that’s the case, the null deviance is 417.64.',\n",
       " 'Residual deviance is wherein we include the independent variables and try to predict the target columns. Hence, when we include the independent variable which is age, we see that the residual deviance drops. Initially, when there are no independent variables, the null deviance was 417. After we include the age column, we see that the null deviance is reduced to 401.',\n",
       " 'This basically means that there is a strong relationship between the age column and the target column and that is why the deviance is reduced.',\n",
       " 'As we have built the model, it’s time to predict some values:',\n",
       " 'Now, we will divide this dataset into train and test sets and build a model on top of the train set and predict the values on top of the test set:',\n",
       " '60. Build an ROC curve for the model built.',\n",
       " 'The below code will help us in building the ROC curve:\\nlibrary(ROCR)\\r\\n\\r\\nprediction(pred_heart, test$target)-> roc_pred_heart\\r\\n\\r\\nperformance(roc_pred_heart, “tpr”, “fpr”)->roc_curve\\r\\n\\r\\nplot(roc_curve, colorize=T)\\nGraph: \\nGo through this Data Science Course in London to get a clear understanding of Data Science!\\n\\n',\n",
       " 'The below code will help us in building the ROC curve:',\n",
       " 'Graph: ',\n",
       " 'Graph:',\n",
       " 'Go through this Data Science Course in London to get a clear understanding of Data Science!',\n",
       " 'Go through this ',\n",
       " 'Data Science Course in London',\n",
       " ' to get a clear understanding of Data Science!',\n",
       " '61. Build a confusion matrix for the model where the threshold value for the probability of predicted values is 0.6, and also find the accuracy of the model.',\n",
       " 'Accuracy is calculated as:\\nAccuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)\\nTo build a confusion matrix in R, we will use the table function:\\ntable(test$target,pred_heart>0.6)\\nHere, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.\\nThen, we calculate the accuracy by the formula for calculating Accuracy.\\n\\n',\n",
       " 'Accuracy is calculated as:',\n",
       " 'Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)',\n",
       " 'Accuracy = (True positives + true negatives)/(True positives+ true negatives + false positives + false negatives)',\n",
       " 'To build a confusion matrix in R, we will use the table function:',\n",
       " 'Here, we are setting the probability threshold as 0.6. So, wherever the probability of pred_heart is greater than 0.6, it will be classified as 0, and wherever it is less than 0.6 it will be classified as 1.',\n",
       " 'Then, we calculate the accuracy by the formula for calculating Accuracy.',\n",
       " 'Accuracy',\n",
       " '',\n",
       " '62. Build a logistic regression model on the ‘customer_churn’ dataset in Python. The dependent variable is ‘Churn’ and the independent variable is ‘MonthlyCharges.’ Find the log_loss of the model.',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:\\ncustomer_churn=pd.read_csv(“customer_churn.csv”)\\n\\nAfter loading this dataset, we can have a glance at the head of the dataset by using the following command:\\ncustomer_churn.head()\\nNow, we will separate the dependent and the independent variables into two separate objects:\\nx=pd.Dataframe(customer_churn[‘MonthlyCharges’])\\r\\n\\r\\ny=customer_churn[‘ Churn’]\\r\\n\\r\\n#Splitting the data into training and testing sets\\r\\n\\r\\nfrom sklearn.model_selection import train_test_split\\r\\n\\r\\nx_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.3, random_state=0)\\nNow, we will see how to build the model and calculate log_loss.\\nfrom sklearn.linear_model, we have to import LogisticRegression\\r\\n\\r\\nl=LogisticRegression()\\r\\n\\r\\nl.fit(x_train,y_train)\\r\\n\\r\\ny_pred=l.predict_proba(x_test)\\nAs we are supposed to calculate the log_loss, we will import it from sklearn.metrics:\\nfrom sklearn.metrics import log_loss\\r\\n\\r\\nprint(log_loss(y_test,y_pred)//actual values are in y_test and predicted are in y_pred\\nOutput:\\n0.5555020595194167\\nBecome a master of Data Science by going through this online Data Science Course in Toronto!\\n',\n",
       " 'First, we will load the pandas dataframe and the customer_churn.csv file:',\n",
       " '',\n",
       " 'After loading this dataset, we can have a glance at the head of the dataset by using the following command:',\n",
       " 'Now, we will separate the dependent and the independent variables into two separate objects:',\n",
       " 'Now, we will see how to build the model and calculate log_loss.',\n",
       " 'log_loss',\n",
       " 'As we are supposed to calculate the log_loss, we will import it from sklearn.metrics:',\n",
       " 'sklearn.metrics',\n",
       " 'Output:',\n",
       " 'Output',\n",
       " 'Become a master of Data Science by going through this online Data Science Course in Toronto!',\n",
       " 'Become a master of Data Science by going through this online Data Science Course in Toronto!',\n",
       " '63. Build a decision tree model on ‘Iris’ dataset where the dependent variable is ‘Species,’ and all other columns are independent variables. Find the accuracy of the model built. ',\n",
       " '\\nTo build a decision tree model, we will be loading the party package:\\n#party package\\r\\n\\r\\nlibrary(party)\\r\\n\\r\\n#splitting the data\\r\\n\\r\\nlibrary(caret)\\r\\n\\r\\nsplit_tag<-createDataPartition(iris$Species, p=0.65, list=F)\\r\\n\\r\\niris[split_tag,]->train\\r\\n\\r\\niris[~split_tag,]->test\\r\\n\\r\\n#building model\\r\\n\\r\\nmytree<-ctree(Species~.,train)\\nNow we will plot the model\\nplot(mytree)\\nModel:\\n\\n#predicting the values\\r\\n\\r\\npredict(mytree,test,type=’response’)->mypred\\nAfter this, we will predict the confusion matrix and then calculate the accuracy using the table function:\\ntable(test$Species, mypred)\\n\\n',\n",
       " '',\n",
       " 'To build a decision tree model, we will be loading the party package:',\n",
       " 'party ',\n",
       " 'Now we will plot the model',\n",
       " 'Model:\\n',\n",
       " 'Model:',\n",
       " 'After this, we will predict the confusion matrix and then calculate the accuracy using the table function:',\n",
       " '',\n",
       " '64. Build a random forest model on top of this ‘CTG’ dataset, where ‘NSP’ is the dependent variable and all other columns are independent variables.',\n",
       " '\\nWe will load the CTG dataset by using read.csv:\\ndata<-read.csv(“C:/Users/intellipaat/Downloads/CTG.csv”,header=True)\\r\\n\\r\\nstr(data)\\nConverting the integer type to a factor\\ndata$NSP<-as.factor(data$NSP)\\r\\n\\r\\ntable(data$NSP)\\r\\n\\r\\n#data partition\\r\\n\\r\\nset.seed(123)\\r\\n\\r\\nsplit_tag<-createDataPartition(data$NSP, p=0.65, list=F)\\r\\n\\r\\ndata[split_tag,]->train\\r\\n\\r\\ndata[~split_tag,]->test\\r\\n\\r\\n#random forest -1\\r\\n\\r\\nlibrary(randomForest)\\r\\n\\r\\nset.seed(222)\\r\\n\\r\\nrf<-randomForest(NSP~.,data=train)\\r\\n\\r\\nrf\\r\\n\\r\\n#prediction\\r\\n\\r\\npredict(rf,test)->p1\\nBuilding confusion matrix and calculating accuracy:\\ntable(test$NSP,p1)\\n\\nIf you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!\\n',\n",
       " '',\n",
       " 'We will load the CTG dataset by using read.csv:',\n",
       " 'read.csv',\n",
       " 'Converting the integer type to a factor',\n",
       " 'Building confusion matrix and calculating accuracy:',\n",
       " '',\n",
       " 'If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our Data Science Community!',\n",
       " 'If you have any doubts or queries related to Data Science, get them clarified from Data Science experts on our ',\n",
       " 'Data Science Community!',\n",
       " '65. Write a function to calculate the Euclidean distance between two points.',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:\\n√(((x1 - x2) ^ 2) + ((y1 - y2) ^ 2))\\nCode for calculating the Euclidean distance is as given below:\\ndef euclidean_distance(P1, P2):\\r\\nreturn (((P1[0] - P2[0]) ** 2) + ((P1[1] - P2[1]) ** 2)) ** .5\\n',\n",
       " 'The formula for calculating the Euclidean distance between two points (x1, y1) and (x2, y2) is as follows:',\n",
       " 'Code for calculating the Euclidean distance is as given below:',\n",
       " '66. Write code to calculate the root mean square error (RMSE) given the lists of values as actual and predicted.',\n",
       " 'To calculate the root mean square error (RMSE), we have to:\\n\\n Calculate the errors, i.e., the differences between the actual and the predicted values\\n Square each of these errors\\n Calculate the mean of these squared errors\\n Return the square root of the mean\\n\\nThe code in Python for calculating RMSE is given below:\\ndef rmse(actual, predicted):\\r\\n\\xa0\\xa0errors = [abs(actual[i] - predicted[i]) for i in range(0, len(actual))]\\r\\n\\xa0\\xa0squared_errors = [x ** 2 for x in errors]\\r\\n\\xa0\\xa0mean = sum(squared_errors) / len(squared_errors)\\r\\n\\xa0\\xa0return mean ** .5\\r\\n\\nCheck out this Machine Learning Course to get an in-depth understanding of Machine Learning.\\n',\n",
       " 'To calculate the root mean square error (RMSE), we have to:',\n",
       " 'The code in Python for calculating RMSE is given below:',\n",
       " 'Check out this Machine Learning Course to get an in-depth understanding of Machine Learning.',\n",
       " '67. Mention the different kernel functions that can be used in SVM.',\n",
       " 'In SVM, there are four types of kernel functions:\\n\\nLinear kernel\\nPolynomial kernel\\nRadial basis kernel\\nSigmoid kernel\\n\\n',\n",
       " 'In SVM, there are four types of kernel functions:',\n",
       " '68. How to detect if the time series data is stationary?',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.\\n',\n",
       " 'Time series data is considered stationary when variance or mean is constant with time. If the variance or mean does not change over a period of time in the dataset, then we can draw the conclusion that, for that period, the data is stationary.',\n",
       " '69. Write code to calculate the accuracy of a binary classification algorithm using its confusion matrix.',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:\\ndef accuracy_score(matrix):\\r\\n\\xa0\\xa0true_positives = matrix[0][0]\\r\\n\\xa0\\xa0true_negatives = matrix[1][1]\\r\\n\\xa0\\xa0total_observations = sum(matrix[0]) + sum(matrix[1])\\r\\n\\xa0\\xa0return (true_positives + true_negatives) / total_observations\\r\\n\\n',\n",
       " 'We can use the code given below to calculate the accuracy of a binary classification algorithm:',\n",
       " '70. What does root cause analysis mean?',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.\\n',\n",
       " 'Root cause analysis is the process of figuring out the root causes that lead to certain faults or failures. A factor is considered to be a root cause if, after eliminating it, a sequence of operations, leading to a fault, error, or undesirable result, ends up working correctly. Root cause analysis is a technique that was initially developed and used in the analysis of industrial accidents, but now, it is used in a wide variety of areas.',\n",
       " '71. What is A/B testing?',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.\\nCheck out this Python Course to get deeper into Python programming.\\n',\n",
       " 'A/B testing is a kind of statistical hypothesis testing for randomized experiments with two variables. These variables are represented as A and B. A/B testing is used when we wish to test a new feature in a product. In the A/B test, we give users two variants of the product, and we label these variants as A and B. The A variant can be the product with the new feature added, and the B variant can be the product without the new feature. After users use these two products, we capture their ratings for the product. If the rating of the product variant A is statistically and significantly higher, then the new feature is considered an improvement and useful and is accepted. Otherwise, the new feature is removed from the product.',\n",
       " 'Check out this Python Course to get deeper into Python programming.',\n",
       " '72. Out of collaborative filtering and content-based filtering, which one is considered better, and why?',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.\\nIn the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.\\n',\n",
       " 'Content-based filtering is considered to be better than collaborative filtering for generating recommendations. It does not mean that collaborative filtering generates bad recommendations. However, as collaborative filtering is based on the likes and dislikes of other users we cannot rely on it much. Also, users’ likes and dislikes may change in the future. For example, there may be a movie that a user likes right now but did not like 10 years ago. Moreover, users who are similar in some features may not have the same taste in the kind of content that the platform provides.',\n",
       " 'In the case of content-based filtering, we make use of users’ own likes and dislikes that are much more reliable and yield more positive results. This is why platforms such as Netflix, Amazon Prime, Spotify, etc. make use of content-based filtering for generating recommendations for their users.',\n",
       " '73. In the following confusion matrix, calculate precision and recall.',\n",
       " '\\n\\n\\n\\nTotal = 510\\nActual\\n\\n\\nPredicted\\n\\nP\\nN\\n\\n\\nP\\n156\\n11\\n\\n\\nN\\n16\\n327\\n\\n\\n\\n\\nThe formulae for precision and recall are given below.\\nPrecision:\\r\\n(True Positive) / (True Positive + False Positive)\\r\\nRecall:\\r\\n(True Positive) / (True Positive + False Negative)\\r\\nBased on the given data, precision and recall are:\\r\\nPrecision: 156 / (156 + 11) = 93.4\\r\\nRecall: 156 / (156 + 16) = 90.7\\r\\n\\n',\n",
       " 'The formulae for precision and recall are given below.',\n",
       " '74. Write a function that when called with a confusion matrix for a binary classification model returns a dictionary with its precision and recall.',\n",
       " \"We can use the below for this purpose:\\ndef calculate_precsion_and_recall(matrix):\\r\\n\\xa0\\xa0true_positive\\xa0 = matrix[0][0]\\r\\n\\xa0\\xa0false_positive\\xa0 = matrix[0][1]\\r\\n\\xa0\\xa0false_negative = matrix[1][0]\\r\\n\\xa0\\xa0return {\\r\\n\\xa0\\xa0\\xa0\\xa0'precision': (true_positive) / (true_positive + false_positive),\\r\\n\\xa0\\xa0\\xa0\\xa0'recall': (true_positive) / (true_positive + false_negative)\\r\\n\\xa0\\xa0}\\r\\n\\n\",\n",
       " 'We can use the below for this purpose:',\n",
       " '75. What is reinforcement learning?',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.\\n',\n",
       " 'Reinforcement learning is a kind of Machine Learning, which is concerned with building software agents that perform actions to attain the most number of cumulative rewards. A reward here is used for letting the model know (during training) if a particular action leads to the attainment of or brings it closer to the goal. For example, if we are creating an ML model that plays a video game, the reward is going to be either the points collected during the play or the level reached in it. Reinforcement learning is used to build these kinds of agents that can make real-world decisions that should move the model toward the attainment of a clearly defined goal.',\n",
       " '76. Explain TF/IDF vectorization.',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.\\n',\n",
       " 'The expression ‘TF/IDF’ stands for Term Frequency–Inverse Document Frequency. It is a numerical measure that allows us to determine how important a word is to a document in a collection of documents called a corpus. TF/IDF is used often in text mining and information retrieval.',\n",
       " '77. What are the assumptions required for linear regression?',\n",
       " 'There are several assumptions required for linear regression. They are as follows:\\n\\nThe data, which is a sample drawn from a population, used to train the model should be representative of the population.\\nThe relationship between independent variables and the mean of dependent variables is linear.\\nThe variance of the residual is going to be the same for any value of an independent variable. It is also represented as X.\\nEach observation is independent of all other observations.\\nFor any value of an independent variable, the independent variable is normally distributed.\\n\\n',\n",
       " 'There are several assumptions required for linear regression. They are as follows:',\n",
       " '78. What happens when some of the assumptions required for linear regression are violated?',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.\\nStrong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.\\n',\n",
       " 'These assumptions may be violated lightly (i.e., some minor violations) or strongly (i.e., the majority of the data has violations). Both of these violations will have different effects on a linear regression model.',\n",
       " 'Strong violations of these assumptions make the results entirely redundant. Light violations of these assumptions make the results have greater bias or variance.']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new11 = lst11[29:494]\n",
    "lst_new11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "78\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do you understand by linear regression?</td>\n",
       "      <td>Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression. Interested in learning Data Science? Click here to learn more in this Data Science Course!  Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.Interested in learning Data Science? Click here to learn more in this Data Science Course!Interested in learning Data Science? Click here to learn more in this Data Science Course!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do you understand by logistic regression?</td>\n",
       "      <td>Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works. S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is a confusion matrix?</td>\n",
       "      <td>The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works. The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.True Positive (d)False Negative (c)False Positive (b):True Negative (a):CTACTAWatch this comprehensive Data Science tutorial to learn more:  Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          Watch this comprehensive Data Science tutorial to learn more: Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do you understand by true positive rate and false positive rate?</td>\n",
       "      <td>True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives. Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                             True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.True positive rateFormulaFalse positive rateFormulaCheck out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                           Check out this comprehensive Data Science Course in India!Master Most in Demand Skills Now !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Data Science?</td>\n",
       "      <td>Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 Questions  \\\n",
       "0                             What do you understand by linear regression?   \n",
       "1                           What do you understand by logistic regression?   \n",
       "2                                              What is a confusion matrix?   \n",
       "3    What do you understand by true positive rate and false positive rate?   \n",
       "4                                                    What is Data Science?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression. Interested in learning Data Science? Click here to learn more in this Data Science Course!  Linear regression helps in understanding the linear relationship between the dependent and the independent variables. Linear regression is a supervised learning algorithm, which helps in finding the linear relationship between two variables. One is the predictor or the independent variable and the other is the response or the dependent variable. In Linear Regression, we try to understand how the dependent variable changes w.r.t the independent variable. If there is only one independent variable, then it is called simple linear regression, and if there is more than one independent variable then it is known as multiple linear regression.Interested in learning Data Science? Click here to learn more in this Data Science Course!Interested in learning Data Science? Click here to learn more in this Data Science Course!  \n",
       "1                                                                                                                                                  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works.  Logistic regression is a classification algorithm that can be used when the dependent variable is binary. Let’s take an example. Here, we are trying to determine whether it will rain or not on the basis of temperature and humidity.  Temperature and humidity are the independent variables, and rain would be our dependent variable. So, the logistic regression algorithm actually produces an S shape curve. Now, let us look at another scenario: Let’s suppose that x-axis represents the runs scored by Virat Kohli and the y-axis represents the probability of the team India winning the match. From this graph, we can say that if Virat Kohli scores more than 50 runs, then there is a greater probability for team India to win the match. Similarly, if he scores less than 50 runs then the probability of team India winning the match is less than 50 percent.  So, basically in logistic regression, the y value lies within the range of 0 and 1. This is how logistic regression works. S  \n",
       "2  The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works. The confusion matrix is a table that is used to estimate the performance of a model. It tabulates the actual values and the predicted values in a 2×2 matrix.  True Positive (d): This denotes all of those records where the actual values are true and the predicted values are also true. So, these denote all of the true positives. False Negative (c): This denotes all of those records where the actual values are true, but the predicted values are false. False Positive (b): In this, the actual values are false, but the predicted values are true. True Negative (a): Here, the actual values are false and the predicted values are also false. So, if you want to get the correct values, then correct values would basically represent all of the true positives and the true negatives. This is how the confusion matrix works.True Positive (d)False Negative (c)False Positive (b):True Negative (a):CTACTAWatch this comprehensive Data Science tutorial to learn more:  Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          Watch this comprehensive Data Science tutorial to learn more: Top 78 Data Science Interview Questions and Answers for 2021 Top 78 Data Science Interview Questions and Answers for 2021          \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                  True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives. Check out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                             True positive rate: In Machine Learning, true positives rates, which are also referred to as sensitivity or recall, are used to measure the percentage of actual positives which are correctly indentified. Formula: True Positive Rate = True Positives/Positives False positive rate: False positive rate is basically the probability of falsely rejecting the null hypothesis for a particular test. The false positive rate is calculated as the ratio between the number of negative events wrongly categorized as positive (false positive) upon the total number of actual events. Formula: False Positive Rate = False Positives/Negatives.True positive rateFormulaFalse positive rateFormulaCheck out this comprehensive Data Science Course in India! Get 50% Hike!Master Most in Demand Skills Now !                                           Check out this comprehensive Data Science Course in India!Master Most in Demand Skills Now !  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  Data Science is a field of computer science that explicitly deals with turning data into information and extracting meaningful insights out of it. The reason why Data Science is so popular is that the kind of insights it allows us to draw from the available data has led to some major innovations in several products and companies. Using these insights, we are able to determine the taste of a particular customer, the likelihood of a product succeeding in a particular market, etc.  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+[\\? | \\.]*\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new11:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "        \n",
    "        \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df11=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df11.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df11[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df11=df11.drop(df11.index[[52,53,54,55,56,57,58,59,62,63]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df11[60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To become a successful data scientist in the industry, understanding the end-to-end workflow of the data science pipeline (understanding data, data pre-processing, model building, model evaluation, and model deployment) is essential. Assuming you do not want to overwhelm yourself with fancy machine learning algorithms, mastering the concepts of logistic regression should be your primary step to get familiar with the end-to-end data science pipeline. Logistic regression should be the first thing to master when becoming a data scientist or a machine learning engineer. Logistic regression is a robust machine learning algorithm that can do a fantastic job even at solving a very complex problem with 95% accuracy. Logistic regression is popularly used for classification problems when the dependent or target variable has only two (or a discrete number of) possible outcomes. It is essentially a modification of linear regression used for classification purposes.',\n",
       " '',\n",
       " 'There is a lot to learn if you want to become a data scientist or a machine learning engineer, but the first step is to master the most common machine learning algorithms in the data science pipeline. These interview questions on logistic regression would be your go-to resource when preparing for your next machine learning or data science interview.',\n",
       " '',\n",
       " '1) Is logistic regression a generative or a descriptive classifier? Why?',\n",
       " \"Logistic regression is a descriptive model. Logistic regression learns to classify by knowing what features differentiate two or more classes of objects. For example, to classify between an apple and an orange, it will learn that the orange is orange in color and an apple is not. On the other hand, a generative classifier like a Naive Bayes will store all the classes' critical features and then classify based on the features the test case best fits.\",\n",
       " '2) Can you use logistic regression for classification between more than two classes?',\n",
       " 'Yes, it is possible to use logistic regression for classification between more than two classes, and it is called multinomial logistic regression. However, this is not possible to implement without modifications to the vanilla logistic regression model.',\n",
       " '',\n",
       " '3) How do you implement multinomial logistic regression?',\n",
       " 'The multinomial logistic classifier can be implemented using a generalization of the sigmoid, called the softmax function. The softmax represents each class with a value in the range (0,1), with all the values summing to 1. Alternatively, you could use the one-vs-all or one-vs-one approach using multiple simple binary classifiers.',\n",
       " '4) Suppose that you are trying to predict whether a consumer will recommend a particular brand of chocolate or not. Let us say your hypothesis function outputs h(x)=0.55 where h(x) is the probability that y=1 (or that a consumer recommends the chocolate) given any input x. Does this mean that the consumer will recommend the chocolate?',\n",
       " \"The answer to this question is 'cannot be determined.' And this will remain the case unless you are provided additional data on the decision boundary. Let us say that you set the decision boundary such that y=1 is h(x)≥0.5 and 0; otherwise, then the answer for this question would be a resounding YES. However, if you set the decision boundary (although this is not very common practice) such that y=1 is h(x)≥0.6 and 0, otherwise the answer will be a NO.\",\n",
       " 'Get Closer To Your Dream of Becoming a Data Scientist with 70+ Solved End-to-End ML Projects',\n",
       " \"5) Why can't we use the mean square error cost function used in linear regression for logistic regression?\",\n",
       " 'If we use mean square error in logistic regression, the resultant cost function will be non-convex, i.e., a function with many local minima, owing to the presence of the sigmoid function in h(x). As a result, an attempt to find the parameters using gradient descent may fail to optimize cost function properly. It may end up choosing a local minima instead of the actual global minima.',\n",
       " '6) If you observe that the cost function decreases rapidly before increasing or stagnating at a specific high value, what could you infer?',\n",
       " 'A trend pattern of the cost curve exhibiting a rapid decrease before then increasing or stagnating at a specific high value indicates that the learning rate is too high. The gradient descent is bouncing around the global minimum but missing it owing to the larger than necessary step size.',\n",
       " '7) What alternative could you suggest using a for loop (which is time-consuming) when using Gradient Descent to find the optimum parameters for logistic regression?',\n",
       " 'One commonly used efficient alternative to using for loop is vectorization, i.e., representing the parameter values to be optimized in a vector. By using this approach, all the vectors can be updated instead of iterating over them in a for loop.',\n",
       " 'Get FREE Access to Machine Learning Example Codes for Data Cleaning, Data Munging, and Data Visualization',\n",
       " '8) Are there alternatives to find optimum parameters for logistic regression besides using Gradient Descent?',\n",
       " 'Yes, Gradient Descent is merely one of the many available optimization algorithms. Other advanced optimization algorithms can often help arrive at the optimum parameters faster and help with scaling for significant machine learning problems. A few such algorithms are Conjugate Gradient, BFGS, and L-BFGS algorithms.',\n",
       " '9) How many binary classifiers would you need to implement one-vs-all for three classes? How does it work?',\n",
       " 'You would need three binary classifiers to implement one-vs-all for three classes since the number of binary classifiers is precisely equal to the number of classes with this approach. If you have three classes given by y=1, y=2, and y=3, then the three classifiers in the one-vs-all approach would consist of\\xa0h(1)(x),\\xa0which classifies the test cases as 1 or not 1,\\xa0h(2)(x)\\xa0which classifies the test cases as 2 or not 2 and so on. You can then take the results together to arrive at the correct classification. For example, with three categories, Cats, Dogs, and Rabbits, to implement the one-vs-all approach, we need to make the following comparisons:',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 1: Cats vs. Dogs, Rabbits (or not Cats)',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 2: Dogs vs. Cats, Rabbits (or not Dogs)',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 3: Rabbits vs. Cats, Dogs (or not Rabbits)',\n",
       " 'Recommended Reading:\\xa0',\n",
       " '10) How many binary classifiers would you need to implement one-vs-one for four classes? How does it work?',\n",
       " 'To implement one-vs-one for four classes, you will require six binary classifiers. This is because you will need to compare each class with each other class. In general, the formula for calculating the number of binary classifiers b is given as b=(no. of classes * (no. of classes -1))/ 2.',\n",
       " 'Suppose we have four different categories into which we need to classify the weather for a particular day: Sun, Rain, Snow, Overcast. Then to implement the one-vs-one approach, we need to make the following comparisons:',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 1: Sun vs. Rain',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 2: Sun vs. Snow',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 3: Sun vs. Overcast',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 4: Rain vs. Snow',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 5: Rain vs. Overcast',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 6: Snow vs. Overcast',\n",
       " '11) What is the importance of regularisation?\\xa0',\n",
       " 'Regularisation is a technique that can help alleviate the problem of overfitting a model. It is beneficial when a large number of parameters are present, which help predict the target function. In these circumstances, it is difficult to select which features to keep manually.\\xa0',\n",
       " 'Regularisation essentially involves adding coefficient terms to the cost function so that the terms are penalized and are small in magnitude. This helps, in turn, to preserve the overall trends in the data while not letting the model become too complex. These penalties, in effect, restrict the influence a predictor variable can have over the target by compressing the coefficients, thereby preventing overfitting.',\n",
       " '',\n",
       " '12) Why is the Wald Test useful in logistic regression but not in linear regression?\\xa0',\n",
       " 'The Wald test, also known as the Wald Chi-Squared Test, is a method to find whether the independent variables in a model are of significance. The significance of variables is decided by whether they contribute to the predictions or not. The variables that add no value to the model can therefore be deleted without risking severe adverse effects to the model. The Wald test is unnecessary in linear regression because it is easy to compare a more complicated model to a simpler model to check the influence of the added independent variables. After all, we can use the R2 value to make this comparison. However, this is not possible with logistic regression as we use Maximum Likelihood Estimate, which uses the previously mentioned method infeasible. The Wald test can be used for many different models, including those with binary variables or continuous variables, and has the added advantage that it only requires estimating one model.',\n",
       " '13) Will the decision boundary be linear or non-linear in logistic regression models? Explain with an example.',\n",
       " 'The decision boundary is essentially a line or a plane that demarcates the boundary between the classes to which linear regression classifies the dependent variables. The shape of the decision boundary will depend entirely on the logistic regression model.',\n",
       " 'For logistic regression model given by hypothesis function h(x)=g(Tx)where g is the sigmoid function, if the hypothesis function is h(x)=g(1+2x2+3x3)then the decision boundary is linear. Alternatively, if h(x)=g(1+2x22+3x32)then the decision boundary is non-linear.',\n",
       " '14) What are odds? Why is it used in logistic regression?',\n",
       " 'Odds are the ratio of the probability of success to the probability of failure. The odds serve to provide the constant effect a particular predictor or independent variable has on the output prediction. Expressing the effect of a predictor on the likelihood of the target having a particular value through probability does not describe this constant effect. In linear regression models, we often want to measure the unique effect of each independent variable on the output for which the odds are very useful.',\n",
       " 'Get More Practice, More Data Science and Machine Learning Projects, and More guidance.Fast-Track Your Career Transition with ProjectPro',\n",
       " '15) Given fair die, what are the odds of occurrence of odd numbers?',\n",
       " 'The\\xa0odds of occurrence of odd numbers is 1.\\xa0',\n",
       " 'There are three odd and three even numbers in a fair die, and therefore, the probability of occurrence of odd numbers is 3/6 or 0.5. Similarly, the odds of occurrence of numbers that are not odd is 0.5. Since odds is the ratio of the probability of success and that of failure,\\xa0',\n",
       " 'Odds = 0.5/0.5=1.',\n",
       " '16) In classification problems like logistic regression, classification accuracy alone is not considered a good measure. Why?',\n",
       " 'Classification accuracy considers both true positives and false positives with equal significance. If this were just another machine learning problem of not too much consequence, this would be acceptable. However, when the problems involve deciding whether to consider a candidate for life-saving treatment, false positives might not be as bad as false negatives. The opposite can also be true in some cases. Therefore, while there is no single best way to evaluate a classifier, accuracy alone may not serve as a good measure.',\n",
       " '17) It is common practice that when the number of features or independent variables is larger in comparison to the training set, it is common to use logistic regression or support vector machine (SVM) with a linear kernel. What is the reasoning behind this?',\n",
       " 'It is common to use logistic regression or SVM with a linear kernel because when there are many features with a limited number of training examples, a linear function should be able to perform reasonably well. Besides, there is not enough training data to allow for the training of more complex functions.',\n",
       " '18) Between SVM and logistic regression, which algorithm is most likely to work better in the presence of outliers? Why?',\n",
       " 'SVM is capable of handling outliers better than logistic regression. SVM is affected only by the points closest to the decision boundary. Logistic regression, on the other hand, tries to maximize the conditional likelihood of the training data and is therefore strongly affected by the presence of outliers.',\n",
       " 'Access Data Science and Machine Learning Project Code Examples',\n",
       " '19) Which is the most preferred algorithm for variable selection?',\n",
       " 'Lasso is the most preferred for variable selection because it performs regression analysis using a shrinkage parameter where the data is shrunk to a point, and variable selection is made by forcing the coefficients of not so significant variables to be set to zero through a penalty.',\n",
       " '20) What according to you is the method to best fit the data in logistic regression?',\n",
       " 'Maximum Likelihood Estimation to obtain the model coefficients which relate to the predictors and target.',\n",
       " 'PREVIOUS',\n",
       " 'NEXT',\n",
       " '',\n",
       " '',\n",
       " 'Trending Project Categories',\n",
       " 'Trending Projects',\n",
       " 'Trending Blogs',\n",
       " 'Trending Recipes ',\n",
       " 'Trending Tutorials',\n",
       " '© 2021 Iconiq Inc.',\n",
       " 'About us',\n",
       " 'Contact us',\n",
       " 'Privacy policy',\n",
       " 'User policy',\n",
       " 'Write for ProjectPro']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regresion\n",
    "lst12= []\n",
    "url = \"https://www.projectpro.io/article/logistic-regression-interview-questions-/448\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['p'])\n",
    "for answer in answers:\n",
    "    lst12.append(answer.text)\n",
    "lst12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1) Is logistic regression a generative or a descriptive classifier? Why?',\n",
       " \"Logistic regression is a descriptive model. Logistic regression learns to classify by knowing what features differentiate two or more classes of objects. For example, to classify between an apple and an orange, it will learn that the orange is orange in color and an apple is not. On the other hand, a generative classifier like a Naive Bayes will store all the classes' critical features and then classify based on the features the test case best fits.\",\n",
       " '2) Can you use logistic regression for classification between more than two classes?',\n",
       " 'Yes, it is possible to use logistic regression for classification between more than two classes, and it is called multinomial logistic regression. However, this is not possible to implement without modifications to the vanilla logistic regression model.',\n",
       " '',\n",
       " '3) How do you implement multinomial logistic regression?',\n",
       " 'The multinomial logistic classifier can be implemented using a generalization of the sigmoid, called the softmax function. The softmax represents each class with a value in the range (0,1), with all the values summing to 1. Alternatively, you could use the one-vs-all or one-vs-one approach using multiple simple binary classifiers.',\n",
       " '4) Suppose that you are trying to predict whether a consumer will recommend a particular brand of chocolate or not. Let us say your hypothesis function outputs h(x)=0.55 where h(x) is the probability that y=1 (or that a consumer recommends the chocolate) given any input x. Does this mean that the consumer will recommend the chocolate?',\n",
       " \"The answer to this question is 'cannot be determined.' And this will remain the case unless you are provided additional data on the decision boundary. Let us say that you set the decision boundary such that y=1 is h(x)≥0.5 and 0; otherwise, then the answer for this question would be a resounding YES. However, if you set the decision boundary (although this is not very common practice) such that y=1 is h(x)≥0.6 and 0, otherwise the answer will be a NO.\",\n",
       " 'Get Closer To Your Dream of Becoming a Data Scientist with 70+ Solved End-to-End ML Projects',\n",
       " \"5) Why can't we use the mean square error cost function used in linear regression for logistic regression?\",\n",
       " 'If we use mean square error in logistic regression, the resultant cost function will be non-convex, i.e., a function with many local minima, owing to the presence of the sigmoid function in h(x). As a result, an attempt to find the parameters using gradient descent may fail to optimize cost function properly. It may end up choosing a local minima instead of the actual global minima.',\n",
       " '6) If you observe that the cost function decreases rapidly before increasing or stagnating at a specific high value, what could you infer?',\n",
       " 'A trend pattern of the cost curve exhibiting a rapid decrease before then increasing or stagnating at a specific high value indicates that the learning rate is too high. The gradient descent is bouncing around the global minimum but missing it owing to the larger than necessary step size.',\n",
       " '7) What alternative could you suggest using a for loop (which is time-consuming) when using Gradient Descent to find the optimum parameters for logistic regression?',\n",
       " 'One commonly used efficient alternative to using for loop is vectorization, i.e., representing the parameter values to be optimized in a vector. By using this approach, all the vectors can be updated instead of iterating over them in a for loop.',\n",
       " 'Get FREE Access to Machine Learning Example Codes for Data Cleaning, Data Munging, and Data Visualization',\n",
       " '8) Are there alternatives to find optimum parameters for logistic regression besides using Gradient Descent?',\n",
       " 'Yes, Gradient Descent is merely one of the many available optimization algorithms. Other advanced optimization algorithms can often help arrive at the optimum parameters faster and help with scaling for significant machine learning problems. A few such algorithms are Conjugate Gradient, BFGS, and L-BFGS algorithms.',\n",
       " '9) How many binary classifiers would you need to implement one-vs-all for three classes? How does it work?',\n",
       " 'You would need three binary classifiers to implement one-vs-all for three classes since the number of binary classifiers is precisely equal to the number of classes with this approach. If you have three classes given by y=1, y=2, and y=3, then the three classifiers in the one-vs-all approach would consist of\\xa0h(1)(x),\\xa0which classifies the test cases as 1 or not 1,\\xa0h(2)(x)\\xa0which classifies the test cases as 2 or not 2 and so on. You can then take the results together to arrive at the correct classification. For example, with three categories, Cats, Dogs, and Rabbits, to implement the one-vs-all approach, we need to make the following comparisons:',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 1: Cats vs. Dogs, Rabbits (or not Cats)',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 2: Dogs vs. Cats, Rabbits (or not Dogs)',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 3: Rabbits vs. Cats, Dogs (or not Rabbits)',\n",
       " 'Recommended Reading:\\xa0',\n",
       " '10) How many binary classifiers would you need to implement one-vs-one for four classes? How does it work?',\n",
       " 'To implement one-vs-one for four classes, you will require six binary classifiers. This is because you will need to compare each class with each other class. In general, the formula for calculating the number of binary classifiers b is given as b=(no. of classes * (no. of classes -1))/ 2.',\n",
       " 'Suppose we have four different categories into which we need to classify the weather for a particular day: Sun, Rain, Snow, Overcast. Then to implement the one-vs-one approach, we need to make the following comparisons:',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 1: Sun vs. Rain',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 2: Sun vs. Snow',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 3: Sun vs. Overcast',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 4: Rain vs. Snow',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 5: Rain vs. Overcast',\n",
       " '\\xa0 \\xa0\\xa0Binary Classification Problem 6: Snow vs. Overcast',\n",
       " '11) What is the importance of regularisation?\\xa0',\n",
       " 'Regularisation is a technique that can help alleviate the problem of overfitting a model. It is beneficial when a large number of parameters are present, which help predict the target function. In these circumstances, it is difficult to select which features to keep manually.\\xa0',\n",
       " 'Regularisation essentially involves adding coefficient terms to the cost function so that the terms are penalized and are small in magnitude. This helps, in turn, to preserve the overall trends in the data while not letting the model become too complex. These penalties, in effect, restrict the influence a predictor variable can have over the target by compressing the coefficients, thereby preventing overfitting.',\n",
       " '',\n",
       " '12) Why is the Wald Test useful in logistic regression but not in linear regression?\\xa0',\n",
       " 'The Wald test, also known as the Wald Chi-Squared Test, is a method to find whether the independent variables in a model are of significance. The significance of variables is decided by whether they contribute to the predictions or not. The variables that add no value to the model can therefore be deleted without risking severe adverse effects to the model. The Wald test is unnecessary in linear regression because it is easy to compare a more complicated model to a simpler model to check the influence of the added independent variables. After all, we can use the R2 value to make this comparison. However, this is not possible with logistic regression as we use Maximum Likelihood Estimate, which uses the previously mentioned method infeasible. The Wald test can be used for many different models, including those with binary variables or continuous variables, and has the added advantage that it only requires estimating one model.',\n",
       " '13) Will the decision boundary be linear or non-linear in logistic regression models? Explain with an example.',\n",
       " 'The decision boundary is essentially a line or a plane that demarcates the boundary between the classes to which linear regression classifies the dependent variables. The shape of the decision boundary will depend entirely on the logistic regression model.',\n",
       " 'For logistic regression model given by hypothesis function h(x)=g(Tx)where g is the sigmoid function, if the hypothesis function is h(x)=g(1+2x2+3x3)then the decision boundary is linear. Alternatively, if h(x)=g(1+2x22+3x32)then the decision boundary is non-linear.',\n",
       " '14) What are odds? Why is it used in logistic regression?',\n",
       " 'Odds are the ratio of the probability of success to the probability of failure. The odds serve to provide the constant effect a particular predictor or independent variable has on the output prediction. Expressing the effect of a predictor on the likelihood of the target having a particular value through probability does not describe this constant effect. In linear regression models, we often want to measure the unique effect of each independent variable on the output for which the odds are very useful.',\n",
       " 'Get More Practice, More Data Science and Machine Learning Projects, and More guidance.Fast-Track Your Career Transition with ProjectPro',\n",
       " '15) Given fair die, what are the odds of occurrence of odd numbers?',\n",
       " 'The\\xa0odds of occurrence of odd numbers is 1.\\xa0',\n",
       " 'There are three odd and three even numbers in a fair die, and therefore, the probability of occurrence of odd numbers is 3/6 or 0.5. Similarly, the odds of occurrence of numbers that are not odd is 0.5. Since odds is the ratio of the probability of success and that of failure,\\xa0',\n",
       " 'Odds = 0.5/0.5=1.',\n",
       " '16) In classification problems like logistic regression, classification accuracy alone is not considered a good measure. Why?',\n",
       " 'Classification accuracy considers both true positives and false positives with equal significance. If this were just another machine learning problem of not too much consequence, this would be acceptable. However, when the problems involve deciding whether to consider a candidate for life-saving treatment, false positives might not be as bad as false negatives. The opposite can also be true in some cases. Therefore, while there is no single best way to evaluate a classifier, accuracy alone may not serve as a good measure.',\n",
       " '17) It is common practice that when the number of features or independent variables is larger in comparison to the training set, it is common to use logistic regression or support vector machine (SVM) with a linear kernel. What is the reasoning behind this?',\n",
       " 'It is common to use logistic regression or SVM with a linear kernel because when there are many features with a limited number of training examples, a linear function should be able to perform reasonably well. Besides, there is not enough training data to allow for the training of more complex functions.',\n",
       " '18) Between SVM and logistic regression, which algorithm is most likely to work better in the presence of outliers? Why?',\n",
       " 'SVM is capable of handling outliers better than logistic regression. SVM is affected only by the points closest to the decision boundary. Logistic regression, on the other hand, tries to maximize the conditional likelihood of the training data and is therefore strongly affected by the presence of outliers.',\n",
       " 'Access Data Science and Machine Learning Project Code Examples',\n",
       " '19) Which is the most preferred algorithm for variable selection?',\n",
       " 'Lasso is the most preferred for variable selection because it performs regression analysis using a shrinkage parameter where the data is shrunk to a point, and variable selection is made by forcing the coefficients of not so significant variables to be set to zero through a penalty.',\n",
       " '20) What according to you is the method to best fit the data in logistic regression?',\n",
       " 'Maximum Likelihood Estimation to obtain the model coefficients which relate to the predictors and target.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new12 = lst12[4:65]\n",
    "lst_new12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is logistic regression a generative or a descriptive classifier? Why?</td>\n",
       "      <td>Logistic regression is a descriptive model. Logistic regression learns to classify by knowing what features differentiate two or more classes of objects. For example, to classify between an apple and an orange, it will learn that the orange is orange in color and an apple is not. On the other hand, a generative classifier like a Naive Bayes will store all the classes' critical features and then classify based on the features the test case best fits.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you use logistic regression for classification between more than two classes?</td>\n",
       "      <td>Yes, it is possible to use logistic regression for classification between more than two classes, and it is called multinomial logistic regression. However, this is not possible to implement without modifications to the vanilla logistic regression model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do you implement multinomial logistic regression?</td>\n",
       "      <td>The multinomial logistic classifier can be implemented using a generalization of the sigmoid, called the softmax function. The softmax represents each class with a value in the range (0,1), with all the values summing to 1. Alternatively, you could use the one-vs-all or one-vs-one approach using multiple simple binary classifiers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suppose that you are trying to predict whether a consumer will recommend a particular brand of chocolate or not. Let us say your hypothesis function outputs h(x)=0.55 where h(x) is the probability that y=1 (or that a consumer recommends the chocolate) given any input x. Does this mean that the consumer will recommend the chocolate?</td>\n",
       "      <td>The answer to this question is 'cannot be determined.' And this will remain the case unless you are provided additional data on the decision boundary. Let us say that you set the decision boundary such that y=1 is h(x)≥0.5 and 0; otherwise, then the answer for this question would be a resounding YES. However, if you set the decision boundary (although this is not very common practice) such that y=1 is h(x)≥0.6 and 0, otherwise the answer will be a NO.Get Closer To Your Dream of Becoming a Data Scientist with 70+ Solved End-to-End ML Projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why can't we use the mean square error cost function used in linear regression for logistic regression?</td>\n",
       "      <td>If we use mean square error in logistic regression, the resultant cost function will be non-convex, i.e., a function with many local minima, owing to the presence of the sigmoid function in h(x). As a result, an attempt to find the parameters using gradient descent may fail to optimize cost function properly. It may end up choosing a local minima instead of the actual global minima.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                         Questions  \\\n",
       "0                                                                                                                                                                                                                                                                            Is logistic regression a generative or a descriptive classifier? Why?   \n",
       "1                                                                                                                                                                                                                                                                Can you use logistic regression for classification between more than two classes?   \n",
       "2                                                                                                                                                                                                                                                                                            How do you implement multinomial logistic regression?   \n",
       "3    Suppose that you are trying to predict whether a consumer will recommend a particular brand of chocolate or not. Let us say your hypothesis function outputs h(x)=0.55 where h(x) is the probability that y=1 (or that a consumer recommends the chocolate) given any input x. Does this mean that the consumer will recommend the chocolate?   \n",
       "4                                                                                                                                                                                                                                          Why can't we use the mean square error cost function used in linear regression for logistic regression?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Answer  \n",
       "0                                                                                                Logistic regression is a descriptive model. Logistic regression learns to classify by knowing what features differentiate two or more classes of objects. For example, to classify between an apple and an orange, it will learn that the orange is orange in color and an apple is not. On the other hand, a generative classifier like a Naive Bayes will store all the classes' critical features and then classify based on the features the test case best fits.  \n",
       "1                                                                                                                                                                                                                                                                                                        Yes, it is possible to use logistic regression for classification between more than two classes, and it is called multinomial logistic regression. However, this is not possible to implement without modifications to the vanilla logistic regression model.  \n",
       "2                                                                                                                                                                                                                         The multinomial logistic classifier can be implemented using a generalization of the sigmoid, called the softmax function. The softmax represents each class with a value in the range (0,1), with all the values summing to 1. Alternatively, you could use the one-vs-all or one-vs-one approach using multiple simple binary classifiers.  \n",
       "3  The answer to this question is 'cannot be determined.' And this will remain the case unless you are provided additional data on the decision boundary. Let us say that you set the decision boundary such that y=1 is h(x)≥0.5 and 0; otherwise, then the answer for this question would be a resounding YES. However, if you set the decision boundary (although this is not very common practice) such that y=1 is h(x)≥0.6 and 0, otherwise the answer will be a NO.Get Closer To Your Dream of Becoming a Data Scientist with 70+ Solved End-to-End ML Projects  \n",
       "4                                                                                                                                                                    If we use mean square error in logistic regression, the resultant cost function will be non-convex, i.e., a function with many local minima, owing to the presence of the sigmoid function in h(x). As a result, an attempt to find the parameters using gradient descent may fail to optimize cost function properly. It may end up choosing a local minima instead of the actual global minima.  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\)[\\w\\d\\s]+\\?*\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new12:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "        \n",
    "        \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\)\",\" \",ques[i])\n",
    "    \n",
    "df12=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df12[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This article was published as a part of the\\xa0Data Science Blogathon',\n",
       " 'Introduction',\n",
       " 'Logistic Regression, a statistical model is a very popular and easy-to-understand algorithm that is mainly used to find out the probability of an outcome.',\n",
       " 'Therefore it becomes necessary for every aspiring Data Scientist and Machine Learning Engineer to have a good knowledge of Logistic Regression.',\n",
       " 'In this article, we will discuss the most important questions on Logistic Regression\\xa0which is helpful to get you a clear understanding of the techniques, and also for Data Science Interviews, which covers its very fundamental level to complex concepts.',\n",
       " 'Let’s get started,',\n",
       " '\\xa0',\n",
       " '1. What do you mean by the Logistic Regression?',\n",
       " 'It’s a classification algorithm that is used where the target variable is of categorical nature. The main objective behind Logistic Regression is to determine the relationship between features and the probability of a particular outcome.',\n",
       " 'For Example, when we need to predict whether a student passes or fails in an exam given the number of hours spent studying as a feature, the target variable comprises two values i.e. pass and fail.',\n",
       " 'Therefore, we can solve classification problem statements which is a supervised machine learning technique using Logistic Regression.',\n",
       " '2. What are the different types of Logistic Regression?',\n",
       " 'Three different types of Logistic Regression are as follows:',\n",
       " '1. Binary Logistic Regression: In this, the target variable has only two 2 possible outcomes.',\n",
       " 'For Example, 0 and 1, or pass and fail or true and false.',\n",
       " '2. Multinomial Logistic Regression:\\xa0In this, the target variable can have three or more possible values without any order.',\n",
       " 'For Example, Predicting preference of food i.e. Veg, Non-Veg, Vegan.',\n",
       " '3. Ordinal Logistic Regression: In this,\\xa0the target variable can have three or more values with ordering.',\n",
       " 'For Example, Movie rating from 1 to 5.',\n",
       " '3. Explain the intuition behind Logistic Regression in detail.',\n",
       " 'Given:',\n",
       " 'By using the training dataset, we can find the dependent(x) and independent variables(y), so if we can determine the parameters w (Normal) and b (y-intercept), then we can easily find a decision boundary that can almost separate both the classes in a linear fashion.',\n",
       " 'Objective: ',\n",
       " 'In order to train a Logistic Regression model, we just need w and b to find a line(in 2D), plane(3D), or hyperplane(in more than 3-D dimension) that can separate both the classes point as perfect as possible so that when it encounters with any new unseen data point, it can easily classify, from which class the unseen data point belongs to.',\n",
       " 'For Example, Let us consider we have only two features as x1 and x2.',\n",
       " 'Let’s take any of the +ve class points (figure below) and find the shortest distance from that point to the plane. Here, the shortest distance is computed using:',\n",
       " 'di = wT*xi / ||w||',\n",
       " 'If weight vector is a unit vector i.e, ||w||=1. Then,',\n",
       " 'di = wT*xi',\n",
       " 'Since w and xi are on the same side of the decision boundary therefore distance will be +ve. Now for a negative point, we have to compute dj = wT*xj. For point xj, distance will be -ve since this point is the opposite side of w.',\n",
       " 'Thus we can conclude, points that are in the same direction of w are considered as +ve points and the points which are in the opposite direction of w are considered as -ve points.',\n",
       " '',\n",
       " 'Now, we can easily classify the unseen data points as -ve and +ve points. If the value of wT*xi>0, then y =+1 and if value of wT*xi < 0 then y = -1.',\n",
       " 'Now, by observing all the cases above now our objective is that our classifier minimizes the miss-classification error, i.e, we want the values of yi*wT*xi to be greater than 0.',\n",
       " 'In our problem, xi and yi are fixed because these are coming from the dataset.',\n",
       " 'As we change the values of the parameters w, and b the sum will change and we want to find that w and b that maximize the sum given below. To calculate the parameters w and b, we can use the Gradient Descent optimizer. Therefore, the optimization function for logistic regression is:',\n",
       " '',\n",
       " '4. What are the odds?',\n",
       " 'Odds are defined as the ratio of the probability of an event occurring to the probability of the event not occurring.',\n",
       " 'For Example, let’s assume that the probability of winning a game is 0.02. Then, the probability of not winning is 1- 0.02 = 0.98.',\n",
       " '5. What factors can attribute to the popularity of Logistic Regression?',\n",
       " 'Logistic Regression is a popular algorithm as it converts the values of the log of odds which can range from -inf to +inf to a range between 0 and 1.',\n",
       " 'Since logistic functions output the probability of occurrence of an event, they can be applied to many real-life scenarios therefore these models are very popular.',\n",
       " '\\xa0',\n",
       " '6. Is the decision boundary Linear or Non-linear in the case of a Logistic Regression model?',\n",
       " 'The decision boundary is a line or a plane that separates the target variables into different classes that can be either linear or nonlinear. In the case of a Logistic Regression model, the decision boundary is a straight line.',\n",
       " 'Logistic Regression model formula = α+1X1+2X2+….+kXk. This clearly represents a straight line.',\n",
       " 'It is suitable in cases where a straight line is able to separate the different classes. However, in cases where a straight line does not suffice then nonlinear algorithms are used to achieve better results.',\n",
       " '7. What is the Impact of Outliers on Logistic Regression?',\n",
       " 'The estimates of the Logistic Regression are sensitive to unusual observations such as outliers, high leverage, and influential observations. Therefore, to solve the problem of outliers, a sigmoid function is used in Logistic Regression.',\n",
       " '8. What is the difference between the outputs of the Logistic model and the Logistic function?',\n",
       " 'The Logistic model outputs the logits, i.e. log-odds; whereas the Logistic function outputs the probabilities.',\n",
       " 'Logistic model = α+1X1+2X2+….+kXk. Therefore, the output of the Logistic model will be logits.',\n",
       " 'Logistic function = f(z) = 1/(1+e-(α+1X1+2X2+….+kXk)). Therefore, the output of the Logistic function will be the probabilities.',\n",
       " '9. How do we handle categorical variables in Logistic Regression?',\n",
       " 'The inputs given to a Logistic Regression model need to be numeric. The algorithm cannot handle categorical variables directly. So, we need to convert the categorical data into a numerical format that is suitable for the algorithm to process.',\n",
       " 'Each level of the categorical variable will be assigned a unique numeric value also known as a dummy variable. These dummy variables are handled by the Logistic Regression model in the same manner as any other numeric value.',\n",
       " '\\n10. Which algorithm is better in the case of outliers present in the dataset i.e., Logistic Regression or SVM?',\n",
       " 'SVM (Support Vector Machines) handles the outliers in a better manner than the Logistic Regression.',\n",
       " 'Logistic Regression: Logistic Regression will identify a linear boundary if it exists to accommodate the outliers. To accommodate the outliers, it will shift the linear boundary.',\n",
       " 'SVM: SVM is insensitive to individual samples. So, to accommodate an outlier there will not be a major shift in the linear boundary. SVM comes with inbuilt complexity controls, which take care of overfitting, which is not true in the case of Logistic Regression.',\n",
       " '11. What are the assumptions made in Logistic Regression?',\n",
       " 'Some of the assumptions of Logistic Regression are as follows:',\n",
       " '1. It assumes that there is minimal or no multicollinearity among the independent variables i.e, predictors are not correlated.',\n",
       " '2. There should be a linear relationship between the logit of the outcome and each predictor variable. The logit function is described as logit(p) = log(p/(1-p)), where p is the probability of the target outcome.',\n",
       " '3. Sometimes to predict properly, it usually requires a large sample size.',\n",
       " '4. The Logistic Regression which has binary classification i.e, two classes assume that the target variable is binary, and ordered Logistic Regression requires the target variable to be ordered.',\n",
       " 'For example, Too Little, About Right, Too Much.',\n",
       " '5. It assumes there is no dependency between the observations.',\n",
       " '12. Can we solve the multiclass classification problems using Logistic Regression? If Yes then How?',\n",
       " 'Yes, in order to deal with multiclass classification using Logistic Regression, the most famous method is known as the one-vs-all approach. In this approach, a number of models are trained, which is equal to the number of classes. These models work in a specific way.',\n",
       " 'For Example, the first model classifies the datapoint depending on whether it belongs to class 1 or some other class(not class 1); the second model classifies the datapoint into class 2 or some other class(not class 2) and so-on for all other classes.',\n",
       " 'So, in this manner, each data point can be checked over all the classes.',\n",
       " '13. How can we express the probability of a Logistic Regression model as conditional probability?',\n",
       " 'We define probability P(Discrete value of Target variable | X1, X2, X3…., Xk) as the probability of the target variable that takes up a discrete value (either 0 or 1 in the case of binary classification problems) when the values of independent variables are given.',\n",
       " 'For Example, the probability an employee will attain (target variable) given his attributes such as his age, salary, etc.',\n",
       " '14. Discuss the space complexity of Logistic Regression.',\n",
       " 'During training: We need to store four things in memory: x, y, w, and b during training a Logistic Regression model.',\n",
       " 'Therefore, the space complexity of Logistic Regression while training is O(nd + n +d).',\n",
       " 'During Runtime or Testing: After training the model what we just need to keep in memory is w. We just need to perform wT*xi to classify the points.',\n",
       " 'Hence, the space complexity during runtime is in the order of d, i.e, O(d).',\n",
       " '15. Discuss the Test or Runtime complexity of Logistic Regression.',\n",
       " 'At the end of the training, we test our model on unseen data and calculate the accuracy of our model. At that time knowing about runtime complexity is very important. After the training of Logistic Regression, we get the parameters w and b.',\n",
       " 'To classify any new point, we have to just perform the operation wT * xi. If wT*xi>0, the point is +ve, and if wT*xi < 0, the point is negative. As w is a vector of size d, performing the operation wT*xi takes O(d) steps as discussed earlier.',\n",
       " 'Therefore, the testing complexity of the Logistic Regression is O(d).',\n",
       " 'Hence, Logistic Regression is very good for low latency applications, i.e, for applications where the dimension of the data is small.',\n",
       " '16. Why is Logistic Regression termed as Regression and not classification?',\n",
       " 'The major difference between Regression and classification problem statements is that the target variable in the Regression is numerical (or continuous) whereas in classification it is categorical (or discrete).',\n",
       " 'Logistic Regression is basically a supervised classification algorithm. However, the Logistic Regression builds a model just like linear regression in order to predict the probability that a given data point belongs to the category numbered as “1”.',\n",
       " 'For Example, Let’s have a binary classification problem, and ‘x’ be some feature and ‘y’ be the target outcome which can be either 0 or 1.',\n",
       " 'The probability that the target outcome is 1 given its input can be represented as:',\n",
       " '',\n",
       " 'If we predict the probability by using linear Regression, we can describe it as:',\n",
       " '',\n",
       " 'where, p(x) = p(y=1|x)',\n",
       " 'Logistic regression models generate predicted probabilities as any number ranging from neg to pos infinity while the probability of an outcome can only lie between 0< P(x)<1.',\n",
       " 'However, to solve the problem of outliers, a sigmoid function is used in Logistic Regression. The Linear equation is put in the sigmoid function.',\n",
       " '',\n",
       " '17. Discuss the Train complexity of Logistic Regression.',\n",
       " 'In order to train a Logistic Regression model, we just need w and b to find a line(in 2-D), plane(in 3-D), or hyperplane(in more than 3-D dimension) that can separate both the classes point as perfect as possible so that when it encounters with any new point, it can easily classify, from which class the unseen data point belongs to.',\n",
       " 'The value of w and b should be such that it maximizes the sum yi*wT*xi > 0.',\n",
       " 'Now, let’s calculate its time complexity in terms of Big O notation:',\n",
       " '',\n",
       " 'Therefore, the overall time complexity of the Logistic Regression during training is n(O(d))=O(nd).',\n",
       " '18. Why can’t we use Mean Square Error (MSE) as a cost function for Logistic Regression?',\n",
       " 'In Logistic Regression, we use the sigmoid function to perform a non-linear transformation to obtain the probabilities. If we square this nonlinear transformation, then it will lead to the problem of non-convexity with local minimums and by using gradient descent in such cases, it is not possible to find the global minimum. As a result, MSE is not suitable for Logistic Regression.',\n",
       " 'So, in the Logistic Regression algorithm, we used Cross-entropy or log loss as a cost function. The property of the cost function for Logistic Regression is that:',\n",
       " 'By optimizing this cost function, convergence is achieved.',\n",
       " '',\n",
       " '19. Why can’t we use Linear Regression in place of Logistic Regression for Binary classification?',\n",
       " 'Linear Regressions cannot be used in the case of binary classification due to the following reasons:',\n",
       " '1. Distribution of error terms: The distribution of data in the case of Linear and Logistic Regression is different. It assumes that error terms are normally distributed. But this assumption does not hold true in the case of binary classification.',\n",
       " '2. Model output: In Linear Regression, the output is continuous(or numeric) while in the case of binary classification, an output of a continuous value does not make sense. For binary classification problems, Linear Regression may predict values that can go beyond the range between 0 and 1. In order to get the output in the form of probabilities, we can map these values to two different classes, then its range should be restricted to 0 and 1. As the Logistic Regression model can output probabilities with Logistic or sigmoid function, it is preferred over linear Regression.',\n",
       " '3. The variance of Residual errors: Linear Regression assumes that the variance of random errors is constant. This assumption is also not held in the case of Logistic Regression.',\n",
       " '20. What are the advantages of Logistic Regression?',\n",
       " 'The advantages of the logistic regression are as follows:',\n",
       " '1. Logistic Regression is very easy to understand.',\n",
       " '2. It requires less training.',\n",
       " '3. It performs well for simple datasets as well as when the data set is linearly separable.',\n",
       " '4. It doesn’t make any assumptions about the distributions of classes in feature space.',\n",
       " '5. A Logistic Regression model is less likely to be over-fitted but it can overfit in high dimensional datasets. To avoid over-fitting these scenarios, One may consider regularization.',\n",
       " '6. They are easier to implement, interpret, and very efficient to train.',\n",
       " '21. What are the disadvantages of Logistic Regression?',\n",
       " 'The disadvantages of the logistic regression are as follows:\\n',\n",
       " '1. Sometimes a lot of Feature Engineering is required.',\n",
       " '2. If the independent features are correlated with each other it may affect the performance of the classifier.',\n",
       " '3. It is quite sensitive to noise and overfitting.',\n",
       " '4. Logistic Regression should not be used if the number of observations is lesser than the number of features, otherwise, it may lead to overfitting.',\n",
       " '5. By using Logistic Regression, non-linear problems can’t be solved because it has a linear decision surface. But in real-world scenarios, the linearly separable data is rarely found.',\n",
       " '6. By using Logistic Regression, it is tough to obtain complex relationships. Some algorithms such as neural networks, which are more powerful, and compact can easily outperform Logistic Regression algorithms.',\n",
       " '7. In Linear Regression, there is a linear relationship between independent and dependent variables but in Logistic Regression, independent variables are linearly related to the log odds (log(p/(1-p)).',\n",
       " 'End Notes',\n",
       " 'Thanks for reading!',\n",
       " 'I hope you enjoyed the questions and were able to test your knowledge about Logistic Regression.\\n',\n",
       " 'If you liked this and want to know more, go visit my other articles on Data Science and Machine Learning by clicking on the Link',\n",
       " 'Please feel free to contact me on Linkedin, Email.',\n",
       " 'Something not mentioned or want to share your thoughts? Feel free to comment below And I’ll get back to you.',\n",
       " 'Chirag Goyal',\n",
       " 'Currently, I am pursuing my Bachelor of Technology (B.Tech) in Computer Science and Engineering from the Indian Institute of Technology Jodhpur(IITJ). I am very enthusiastic about Machine learning, Deep Learning, and Artificial Intelligence.',\n",
       " 'The media shown in this article are not owned by Analytics Vidhya and is used at the Author’s discretion.',\n",
       " '',\n",
       " ' Notify me of follow-up comments by email.',\n",
       " ' Notify me of new posts by email.',\n",
       " '',\n",
       " '',\n",
       " 'Top Resources',\n",
       " 'Basic Concepts of Object-Oriented Programming in Python',\n",
       " 'Python Tutorial: Working with CSV file for Data Science',\n",
       " 'Commonly used Machine Learning Algorithms (with Python and R Codes)',\n",
       " '3 Interesting Python Projects With Code for Beginners!',\n",
       " '\\n\\n\\n×\\n\\n',\n",
       " '© Copyright 2013-2021 Analytics Vidhya.']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst13= []\n",
    "url = \"https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-logistic-regression/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h2','p'])\n",
    "for answer in answers:\n",
    "    lst13.append(answer.text)\n",
    "lst13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What do you mean by the Logistic Regression?',\n",
       " 'It’s a classification algorithm that is used where the target variable is of categorical nature. The main objective behind Logistic Regression is to determine the relationship between features and the probability of a particular outcome.',\n",
       " 'For Example, when we need to predict whether a student passes or fails in an exam given the number of hours spent studying as a feature, the target variable comprises two values i.e. pass and fail.',\n",
       " 'Therefore, we can solve classification problem statements which is a supervised machine learning technique using Logistic Regression.',\n",
       " '2. What are the different types of Logistic Regression?',\n",
       " 'Three different types of Logistic Regression are as follows:',\n",
       " '1. Binary Logistic Regression: In this, the target variable has only two 2 possible outcomes.',\n",
       " 'For Example, 0 and 1, or pass and fail or true and false.',\n",
       " '2. Multinomial Logistic Regression:\\xa0In this, the target variable can have three or more possible values without any order.',\n",
       " 'For Example, Predicting preference of food i.e. Veg, Non-Veg, Vegan.',\n",
       " '3. Ordinal Logistic Regression: In this,\\xa0the target variable can have three or more values with ordering.',\n",
       " 'For Example, Movie rating from 1 to 5.',\n",
       " '3. Explain the intuition behind Logistic Regression in detail.',\n",
       " 'Given:',\n",
       " 'By using the training dataset, we can find the dependent(x) and independent variables(y), so if we can determine the parameters w (Normal) and b (y-intercept), then we can easily find a decision boundary that can almost separate both the classes in a linear fashion.',\n",
       " 'Objective: ',\n",
       " 'In order to train a Logistic Regression model, we just need w and b to find a line(in 2D), plane(3D), or hyperplane(in more than 3-D dimension) that can separate both the classes point as perfect as possible so that when it encounters with any new unseen data point, it can easily classify, from which class the unseen data point belongs to.',\n",
       " 'For Example, Let us consider we have only two features as x1 and x2.',\n",
       " 'Let’s take any of the +ve class points (figure below) and find the shortest distance from that point to the plane. Here, the shortest distance is computed using:',\n",
       " 'di = wT*xi / ||w||',\n",
       " 'If weight vector is a unit vector i.e, ||w||=1. Then,',\n",
       " 'di = wT*xi',\n",
       " 'Since w and xi are on the same side of the decision boundary therefore distance will be +ve. Now for a negative point, we have to compute dj = wT*xj. For point xj, distance will be -ve since this point is the opposite side of w.',\n",
       " 'Thus we can conclude, points that are in the same direction of w are considered as +ve points and the points which are in the opposite direction of w are considered as -ve points.',\n",
       " '',\n",
       " 'Now, we can easily classify the unseen data points as -ve and +ve points. If the value of wT*xi>0, then y =+1 and if value of wT*xi < 0 then y = -1.',\n",
       " 'Now, by observing all the cases above now our objective is that our classifier minimizes the miss-classification error, i.e, we want the values of yi*wT*xi to be greater than 0.',\n",
       " 'In our problem, xi and yi are fixed because these are coming from the dataset.',\n",
       " 'As we change the values of the parameters w, and b the sum will change and we want to find that w and b that maximize the sum given below. To calculate the parameters w and b, we can use the Gradient Descent optimizer. Therefore, the optimization function for logistic regression is:',\n",
       " '',\n",
       " '4. What are the odds?',\n",
       " 'Odds are defined as the ratio of the probability of an event occurring to the probability of the event not occurring.',\n",
       " 'For Example, let’s assume that the probability of winning a game is 0.02. Then, the probability of not winning is 1- 0.02 = 0.98.',\n",
       " '5. What factors can attribute to the popularity of Logistic Regression?',\n",
       " 'Logistic Regression is a popular algorithm as it converts the values of the log of odds which can range from -inf to +inf to a range between 0 and 1.',\n",
       " 'Since logistic functions output the probability of occurrence of an event, they can be applied to many real-life scenarios therefore these models are very popular.',\n",
       " '\\xa0',\n",
       " '6. Is the decision boundary Linear or Non-linear in the case of a Logistic Regression model?',\n",
       " 'The decision boundary is a line or a plane that separates the target variables into different classes that can be either linear or nonlinear. In the case of a Logistic Regression model, the decision boundary is a straight line.',\n",
       " 'Logistic Regression model formula = α+1X1+2X2+….+kXk. This clearly represents a straight line.',\n",
       " 'It is suitable in cases where a straight line is able to separate the different classes. However, in cases where a straight line does not suffice then nonlinear algorithms are used to achieve better results.',\n",
       " '7. What is the Impact of Outliers on Logistic Regression?',\n",
       " 'The estimates of the Logistic Regression are sensitive to unusual observations such as outliers, high leverage, and influential observations. Therefore, to solve the problem of outliers, a sigmoid function is used in Logistic Regression.',\n",
       " '8. What is the difference between the outputs of the Logistic model and the Logistic function?',\n",
       " 'The Logistic model outputs the logits, i.e. log-odds; whereas the Logistic function outputs the probabilities.',\n",
       " 'Logistic model = α+1X1+2X2+….+kXk. Therefore, the output of the Logistic model will be logits.',\n",
       " 'Logistic function = f(z) = 1/(1+e-(α+1X1+2X2+….+kXk)). Therefore, the output of the Logistic function will be the probabilities.',\n",
       " '9. How do we handle categorical variables in Logistic Regression?',\n",
       " 'The inputs given to a Logistic Regression model need to be numeric. The algorithm cannot handle categorical variables directly. So, we need to convert the categorical data into a numerical format that is suitable for the algorithm to process.',\n",
       " 'Each level of the categorical variable will be assigned a unique numeric value also known as a dummy variable. These dummy variables are handled by the Logistic Regression model in the same manner as any other numeric value.',\n",
       " '\\n10. Which algorithm is better in the case of outliers present in the dataset i.e., Logistic Regression or SVM?',\n",
       " 'SVM (Support Vector Machines) handles the outliers in a better manner than the Logistic Regression.',\n",
       " 'Logistic Regression: Logistic Regression will identify a linear boundary if it exists to accommodate the outliers. To accommodate the outliers, it will shift the linear boundary.',\n",
       " 'SVM: SVM is insensitive to individual samples. So, to accommodate an outlier there will not be a major shift in the linear boundary. SVM comes with inbuilt complexity controls, which take care of overfitting, which is not true in the case of Logistic Regression.',\n",
       " '11. What are the assumptions made in Logistic Regression?',\n",
       " 'Some of the assumptions of Logistic Regression are as follows:',\n",
       " '1. It assumes that there is minimal or no multicollinearity among the independent variables i.e, predictors are not correlated.',\n",
       " '2. There should be a linear relationship between the logit of the outcome and each predictor variable. The logit function is described as logit(p) = log(p/(1-p)), where p is the probability of the target outcome.',\n",
       " '3. Sometimes to predict properly, it usually requires a large sample size.',\n",
       " '4. The Logistic Regression which has binary classification i.e, two classes assume that the target variable is binary, and ordered Logistic Regression requires the target variable to be ordered.',\n",
       " 'For example, Too Little, About Right, Too Much.',\n",
       " '5. It assumes there is no dependency between the observations.',\n",
       " '12. Can we solve the multiclass classification problems using Logistic Regression? If Yes then How?',\n",
       " 'Yes, in order to deal with multiclass classification using Logistic Regression, the most famous method is known as the one-vs-all approach. In this approach, a number of models are trained, which is equal to the number of classes. These models work in a specific way.',\n",
       " 'For Example, the first model classifies the datapoint depending on whether it belongs to class 1 or some other class(not class 1); the second model classifies the datapoint into class 2 or some other class(not class 2) and so-on for all other classes.',\n",
       " 'So, in this manner, each data point can be checked over all the classes.',\n",
       " '13. How can we express the probability of a Logistic Regression model as conditional probability?',\n",
       " 'We define probability P(Discrete value of Target variable | X1, X2, X3…., Xk) as the probability of the target variable that takes up a discrete value (either 0 or 1 in the case of binary classification problems) when the values of independent variables are given.',\n",
       " 'For Example, the probability an employee will attain (target variable) given his attributes such as his age, salary, etc.',\n",
       " '14. Discuss the space complexity of Logistic Regression.',\n",
       " 'During training: We need to store four things in memory: x, y, w, and b during training a Logistic Regression model.',\n",
       " 'Therefore, the space complexity of Logistic Regression while training is O(nd + n +d).',\n",
       " 'During Runtime or Testing: After training the model what we just need to keep in memory is w. We just need to perform wT*xi to classify the points.',\n",
       " 'Hence, the space complexity during runtime is in the order of d, i.e, O(d).',\n",
       " '15. Discuss the Test or Runtime complexity of Logistic Regression.',\n",
       " 'At the end of the training, we test our model on unseen data and calculate the accuracy of our model. At that time knowing about runtime complexity is very important. After the training of Logistic Regression, we get the parameters w and b.',\n",
       " 'To classify any new point, we have to just perform the operation wT * xi. If wT*xi>0, the point is +ve, and if wT*xi < 0, the point is negative. As w is a vector of size d, performing the operation wT*xi takes O(d) steps as discussed earlier.',\n",
       " 'Therefore, the testing complexity of the Logistic Regression is O(d).',\n",
       " 'Hence, Logistic Regression is very good for low latency applications, i.e, for applications where the dimension of the data is small.',\n",
       " '16. Why is Logistic Regression termed as Regression and not classification?',\n",
       " 'The major difference between Regression and classification problem statements is that the target variable in the Regression is numerical (or continuous) whereas in classification it is categorical (or discrete).',\n",
       " 'Logistic Regression is basically a supervised classification algorithm. However, the Logistic Regression builds a model just like linear regression in order to predict the probability that a given data point belongs to the category numbered as “1”.',\n",
       " 'For Example, Let’s have a binary classification problem, and ‘x’ be some feature and ‘y’ be the target outcome which can be either 0 or 1.',\n",
       " 'The probability that the target outcome is 1 given its input can be represented as:',\n",
       " '',\n",
       " 'If we predict the probability by using linear Regression, we can describe it as:',\n",
       " '',\n",
       " 'where, p(x) = p(y=1|x)',\n",
       " 'Logistic regression models generate predicted probabilities as any number ranging from neg to pos infinity while the probability of an outcome can only lie between 0< P(x)<1.',\n",
       " 'However, to solve the problem of outliers, a sigmoid function is used in Logistic Regression. The Linear equation is put in the sigmoid function.',\n",
       " '',\n",
       " '17. Discuss the Train complexity of Logistic Regression.',\n",
       " 'In order to train a Logistic Regression model, we just need w and b to find a line(in 2-D), plane(in 3-D), or hyperplane(in more than 3-D dimension) that can separate both the classes point as perfect as possible so that when it encounters with any new point, it can easily classify, from which class the unseen data point belongs to.',\n",
       " 'The value of w and b should be such that it maximizes the sum yi*wT*xi > 0.',\n",
       " 'Now, let’s calculate its time complexity in terms of Big O notation:',\n",
       " '',\n",
       " 'Therefore, the overall time complexity of the Logistic Regression during training is n(O(d))=O(nd).',\n",
       " '18. Why can’t we use Mean Square Error (MSE) as a cost function for Logistic Regression?',\n",
       " 'In Logistic Regression, we use the sigmoid function to perform a non-linear transformation to obtain the probabilities. If we square this nonlinear transformation, then it will lead to the problem of non-convexity with local minimums and by using gradient descent in such cases, it is not possible to find the global minimum. As a result, MSE is not suitable for Logistic Regression.',\n",
       " 'So, in the Logistic Regression algorithm, we used Cross-entropy or log loss as a cost function. The property of the cost function for Logistic Regression is that:',\n",
       " 'By optimizing this cost function, convergence is achieved.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new13 = lst13[8:109]\n",
    "lst_new13 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "25\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What do you mean by the Logistic Regression?</td>\n",
       "      <td>It’s a classification algorithm that is used where the target variable is of categorical nature. The main objective behind Logistic Regression is to determine the relationship between features and the probability of a particular outcome.For Example, when we need to predict whether a student passes or fails in an exam given the number of hours spent studying as a feature, the target variable comprises two values i.e. pass and fail.Therefore, we can solve classification problem statements which is a supervised machine learning technique using Logistic Regression.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the different types of Logistic Regression?</td>\n",
       "      <td>Three different types of Logistic Regression are as follows:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Binary Logistic Regression: In this, the target variable has only two 2 possible outcomes.</td>\n",
       "      <td>For Example, 0 and 1, or pass and fail or true and false.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Multinomial Logistic Regression: In this, the target variable can have three or more possible values without any order.</td>\n",
       "      <td>For Example, Predicting preference of food i.e. Veg, Non-Veg, Vegan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ordinal Logistic Regression: In this, the target variable can have three or more values with ordering.</td>\n",
       "      <td>For Example, Movie rating from 1 to 5.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   Questions  \\\n",
       "0                                                                               What do you mean by the Logistic Regression?   \n",
       "1                                                                       What are the different types of Logistic Regression?   \n",
       "2                                 Binary Logistic Regression: In this, the target variable has only two 2 possible outcomes.   \n",
       "3    Multinomial Logistic Regression: In this, the target variable can have three or more possible values without any order.   \n",
       "4                     Ordinal Logistic Regression: In this, the target variable can have three or more values with ordering.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Answer  \n",
       "0  It’s a classification algorithm that is used where the target variable is of categorical nature. The main objective behind Logistic Regression is to determine the relationship between features and the probability of a particular outcome.For Example, when we need to predict whether a student passes or fails in an exam given the number of hours spent studying as a feature, the target variable comprises two values i.e. pass and fail.Therefore, we can solve classification problem statements which is a supervised machine learning technique using Logistic Regression.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Three different types of Logistic Regression are as follows:  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                For Example, 0 and 1, or pass and fail or true and false.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     For Example, Predicting preference of food i.e. Veg, Non-Veg, Vegan.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   For Example, Movie rating from 1 to 5.  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\.[\\w\\d\\s]+\\?*\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new13:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)\n",
    "        \n",
    "        \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df13=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df13.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df13[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df13=df13.drop(df13.index[[1,2,3,4,12,13,14,15,16,17]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df13[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign in',\n",
       " 'Monika Sharma',\n",
       " 'Oct 30, 2019·9 min read',\n",
       " 'In the interest of not failing twice in the same spot and making yourself useful to others, this post of mine is dedicated to people who want to follow their passion of becoming/improving as a Data Scientist. Data Science is a field that requires constant improvement in your skills set, while developing basic concepts in Machine Learning algorithms on a daily basis. So without further ado, let us dive straight into some questions and answers that you might useful in your next interview.',\n",
       " 'Question 1: Can you explain cost function of decision trees?',\n",
       " 'Answer: Before we answer this question, it is important to note that Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. Hence their cost functions are also different.',\n",
       " 'Cost function for classification type problems:',\n",
       " 'Gini impurity is an important concept before we can understand cost function, so let me first explain that.',\n",
       " 'where “p” is the ratio of class k instances among the training instances at the ith node. What does that mean? Let’s understand from an example below. Figure I displays a simple visualization of Iris Decision Tree of depth 2. Top level is the root node. The concept of dividing a training set into a set of decisions is fairly simple in the algorithm. Here, for instance, Iris data set is divided into two subsets based on a single feature called “petal width” at the root node. If petal width is less than or equal to 0.8, then the algorithm goes to depth 1, left. If not, it goes to depth 1, right. Where it further divides instances based on an additional feature of “petal width”. Depth 1, right node has a sample of 100 instances and applies 0 instances to Iris-Setosa, 50 instances to Iris-Versicolor and remaining 50 to Iris-Virginica.',\n",
       " 'So this node’s gini impurity is 0.5:',\n",
       " 'Similarly, at depth 1, left node, Gini impurity is zero because all training instances apply to the same class. The node is essentially “pure”.',\n",
       " 'Now that understand what is Gini impurity, let’s get into the meat of the answer. Decision Trees use Classification and Regression Tree (CART) algorithm for training purposes based on a simple concept that the data set will be split into two subsets using a single feature (k) and threshold (t) . In Iris data set the feature was “petal width” and threshold was 0.8. How does it choose k and t? It searches for the pair (k, t) that produces the purest subsets. So the cost function that the algorithm tries to minimize is given by below equation:',\n",
       " 'where G left or right represent gini impurity of subsets while m represents instances of the subsets.',\n",
       " 'Cost function for regression type problems:',\n",
       " 'For regression trees, cost function is fairly intuitive. We use Residual Sum of Squares (RSS). Equation III displays cost function of regression type trees, where “y” is ground truth and “y-hat” is the predicted value.',\n",
       " 'Question II: How does collinearity affect your models?',\n",
       " 'Answer: Collinearity refers to a situation where two or more predictor variables are closely related to one another. Figure 2 below shows as example of collinear variables. Variable 2 strictly follows variable 1 with a Pearson correlation coefficient of 1. So obviously one of these variables will behave like noise when fed into machine learning models.',\n",
       " 'The presence of collinearity can become problematic in regression type questions, since it becomes difficult to separate out the individual effects of collinear variables on the response. Or in other words collinearity reduces the accuracy of the estimates of the regression coefficients and results in increase in errors. This will ultimately lead to decline in the t-statistic, as a result, in the presence of collinearity we may fail to reject the null hypothesis.',\n",
       " 'A simple way to detect collinearity is to look at the correlation matrix of the predictor variables. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore collinearity problem with the data. Unfortunately not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for the collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. Such a situation is called multi-collinearity. For such cases, instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF for each variable can be computed using the formula:',\n",
       " 'Where R-square term is the regression of variable X, onto all of the other predictors. If VIF is close to or more than one then collinearity is present. When faced with the problem of collinearity there are two possible solutions. One is to drop the redundant variable. This can be done without compromising the regression fit. The second solution is to combine the collinear variables together into a single predictor.',\n",
       " 'Question III: How will you explain deep neural network to a layman?',\n",
       " 'Answer: The idea of Neural Network (NN) originally stemmed from human brain that is designed to identify patterns. NN is a set of algorithms that interpret sensory data through machine perception, labeling and clustering raw input data. Any type of real world data, be it images, text, sound or even time series data must be converted into a vector space containing numbers.',\n",
       " 'The word “deep” in deep neural network means that the neural network consists of multiple layers. These layers are made of nodes where computation takes place. An analogy of node is a neuron in human brain which fires when it encounters sufficient stimuli. A node combines data from the raw input with their coefficients or weights that either dampen or amplify that input based on the weight. The product of input and weight is then summed at the summation node shown below in Fig. 3, which is then passed on to the activation function which determines whether and to what extend that signal should progress further through the network to affect the ultimate outcome. A node layer is a row of such neuron-like switches that turn on or off as the input is fed through the network.',\n",
       " 'Deep neural networks differs from earlier version of neural networks such as perceptrons because they were shallow and simply consisted of input and output layer along with one hidden layer.',\n",
       " 'Question IV: What would be a 3-minute pitch for your data science take home project?',\n",
       " 'Answer: A typical data science interview process starts with a take home data analysis project. I have taken two of those and time span may vary depending on the complexity of the take home project. First time, I was given two days to solve a problem using machine learning along with an executive summary. While the second time I was given two weeks to solve a problem. It is needless to point out that second time it was a much harder problem where I was dealing with class imbalanced data set. So 3-minutes sales pitch type interview question allows you to showcase your grasp of the problem at hand. Please be sure to start with what is your interpretation of the problem; your brief approach to solving the problem; what types of machine learning models did you use in your approach and why? And close this by boasting about the accuracy of your models.',\n",
       " 'I believe that this is a very important question during your interview that enables you to prove that you are a leader in Data Science field and can solve a complex problem with the latest and greatest tools at your disposal.',\n",
       " 'Question V: What do you mean by model regularization and how will you achieve regularization in linear models?',\n",
       " 'Answer: Regularization is a term used for constraining your machine learning model. One good way to constrain or reduce overfitting in machine learning models is to have fewer degrees of freedom. With fewer degrees of freedom, it gets harder for the model to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees of freedom. However, for linear models, regularization is typically achieved by constraining the weights of the model. So, instead of linear regression, Ridge regression, Lasso Regression and Elastic Net models have three different ways to constraint the weights. For the sake of completeness, lets start with the definition of linear regression first:',\n",
       " 'Mean Square Error cost function for a Linear Regression model is defined as:',\n",
       " 'Where thetaT is the transpose of theta (a row vector instead of column vector).',\n",
       " 'Ridge Regression: Is a regularized version of Linear Regression, i.e., an additional regularization term is added to the cost function. This forces the learning algorithms to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.',\n",
       " 'The hyperparameter alpha controls how much you want to regularize the model. If alpha is zero, then ridge regression is just linear regression.',\n",
       " 'Lasso Regression: Least Absolute Shrinkage and Selection Operator Regression (simple called Lasso Regression) is another regularized version of Linear Regression: Just like the Ridge Regression, it adds a regularization term to the cost function, but it uses the L1 norm of the weight vector instead of half the square of the L2 norm .',\n",
       " 'An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with a few non-zero feature weights).',\n",
       " 'Elastic Net Regression: This is a middle ground between Ridge and Lasso regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization term and can be controlled with “r”. When r=0, Elastic Net is equivalent to Ridge Regression, and when r=1, it is equivalent to Lasso Regression.',\n",
       " 'It is always preferable to have at least a little bit of regularization and generally plain linear regression should always be avoided. Ridge is a good default, but if only a few features are useful in a particular data set, then Lasso should be used. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of instances or when several features are strongly correlated.',\n",
       " 'In this article I have discussed five questions that I personally faced during a technical data science interview, which I thought could have been better. I highly recommend the following resources to read to hone your basic concepts on a daily basis. Believe me, I must have read these concepts over and over again, and yet I fumbled upon them during my interview.',\n",
       " 'If you enjoyed reading the article please don’t forget to upvote it!',\n",
       " 'Happy learning!',\n",
       " 'Data Scientist https://www.linkedin.com/in/monikaasharma/',\n",
       " '2.2K ',\n",
       " '9',\n",
       " '2.2K\\xa0',\n",
       " '2.2K ',\n",
       " '9',\n",
       " 'Your home for data science. A Medium publication sharing concepts, ideas and codes.']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lasso regresion\n",
    "lst14= []\n",
    "url = \"https://towardsdatascience.com/five-data-science-interview-questions-that-you-must-be-able-to-answer-8f2ec53b409a\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([{'class':'ju gs'},'p'])\n",
    "for answer in answers:\n",
    "    lst14.append(answer.text)\n",
    "lst14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question 1: Can you explain cost function of decision trees?',\n",
       " 'Answer: Before we answer this question, it is important to note that Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. Hence their cost functions are also different.',\n",
       " 'Cost function for classification type problems:',\n",
       " 'Gini impurity is an important concept before we can understand cost function, so let me first explain that.',\n",
       " 'where “p” is the ratio of class k instances among the training instances at the ith node. What does that mean? Let’s understand from an example below. Figure I displays a simple visualization of Iris Decision Tree of depth 2. Top level is the root node. The concept of dividing a training set into a set of decisions is fairly simple in the algorithm. Here, for instance, Iris data set is divided into two subsets based on a single feature called “petal width” at the root node. If petal width is less than or equal to 0.8, then the algorithm goes to depth 1, left. If not, it goes to depth 1, right. Where it further divides instances based on an additional feature of “petal width”. Depth 1, right node has a sample of 100 instances and applies 0 instances to Iris-Setosa, 50 instances to Iris-Versicolor and remaining 50 to Iris-Virginica.',\n",
       " 'So this node’s gini impurity is 0.5:',\n",
       " 'Similarly, at depth 1, left node, Gini impurity is zero because all training instances apply to the same class. The node is essentially “pure”.',\n",
       " 'Now that understand what is Gini impurity, let’s get into the meat of the answer. Decision Trees use Classification and Regression Tree (CART) algorithm for training purposes based on a simple concept that the data set will be split into two subsets using a single feature (k) and threshold (t) . In Iris data set the feature was “petal width” and threshold was 0.8. How does it choose k and t? It searches for the pair (k, t) that produces the purest subsets. So the cost function that the algorithm tries to minimize is given by below equation:',\n",
       " 'where G left or right represent gini impurity of subsets while m represents instances of the subsets.',\n",
       " 'Cost function for regression type problems:',\n",
       " 'For regression trees, cost function is fairly intuitive. We use Residual Sum of Squares (RSS). Equation III displays cost function of regression type trees, where “y” is ground truth and “y-hat” is the predicted value.',\n",
       " 'Question II: How does collinearity affect your models?',\n",
       " 'Answer: Collinearity refers to a situation where two or more predictor variables are closely related to one another. Figure 2 below shows as example of collinear variables. Variable 2 strictly follows variable 1 with a Pearson correlation coefficient of 1. So obviously one of these variables will behave like noise when fed into machine learning models.',\n",
       " 'The presence of collinearity can become problematic in regression type questions, since it becomes difficult to separate out the individual effects of collinear variables on the response. Or in other words collinearity reduces the accuracy of the estimates of the regression coefficients and results in increase in errors. This will ultimately lead to decline in the t-statistic, as a result, in the presence of collinearity we may fail to reject the null hypothesis.',\n",
       " 'A simple way to detect collinearity is to look at the correlation matrix of the predictor variables. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore collinearity problem with the data. Unfortunately not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for the collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. Such a situation is called multi-collinearity. For such cases, instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF for each variable can be computed using the formula:',\n",
       " 'Where R-square term is the regression of variable X, onto all of the other predictors. If VIF is close to or more than one then collinearity is present. When faced with the problem of collinearity there are two possible solutions. One is to drop the redundant variable. This can be done without compromising the regression fit. The second solution is to combine the collinear variables together into a single predictor.',\n",
       " 'Question III: How will you explain deep neural network to a layman?',\n",
       " 'Answer: The idea of Neural Network (NN) originally stemmed from human brain that is designed to identify patterns. NN is a set of algorithms that interpret sensory data through machine perception, labeling and clustering raw input data. Any type of real world data, be it images, text, sound or even time series data must be converted into a vector space containing numbers.',\n",
       " 'The word “deep” in deep neural network means that the neural network consists of multiple layers. These layers are made of nodes where computation takes place. An analogy of node is a neuron in human brain which fires when it encounters sufficient stimuli. A node combines data from the raw input with their coefficients or weights that either dampen or amplify that input based on the weight. The product of input and weight is then summed at the summation node shown below in Fig. 3, which is then passed on to the activation function which determines whether and to what extend that signal should progress further through the network to affect the ultimate outcome. A node layer is a row of such neuron-like switches that turn on or off as the input is fed through the network.',\n",
       " 'Deep neural networks differs from earlier version of neural networks such as perceptrons because they were shallow and simply consisted of input and output layer along with one hidden layer.',\n",
       " 'Question IV: What would be a 3-minute pitch for your data science take home project?',\n",
       " 'Answer: A typical data science interview process starts with a take home data analysis project. I have taken two of those and time span may vary depending on the complexity of the take home project. First time, I was given two days to solve a problem using machine learning along with an executive summary. While the second time I was given two weeks to solve a problem. It is needless to point out that second time it was a much harder problem where I was dealing with class imbalanced data set. So 3-minutes sales pitch type interview question allows you to showcase your grasp of the problem at hand. Please be sure to start with what is your interpretation of the problem; your brief approach to solving the problem; what types of machine learning models did you use in your approach and why? And close this by boasting about the accuracy of your models.',\n",
       " 'I believe that this is a very important question during your interview that enables you to prove that you are a leader in Data Science field and can solve a complex problem with the latest and greatest tools at your disposal.',\n",
       " 'Question V: What do you mean by model regularization and how will you achieve regularization in linear models?',\n",
       " 'Answer: Regularization is a term used for constraining your machine learning model. One good way to constrain or reduce overfitting in machine learning models is to have fewer degrees of freedom. With fewer degrees of freedom, it gets harder for the model to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees of freedom. However, for linear models, regularization is typically achieved by constraining the weights of the model. So, instead of linear regression, Ridge regression, Lasso Regression and Elastic Net models have three different ways to constraint the weights. For the sake of completeness, lets start with the definition of linear regression first:',\n",
       " 'Mean Square Error cost function for a Linear Regression model is defined as:',\n",
       " 'Where thetaT is the transpose of theta (a row vector instead of column vector).',\n",
       " 'Ridge Regression: Is a regularized version of Linear Regression, i.e., an additional regularization term is added to the cost function. This forces the learning algorithms to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.',\n",
       " 'The hyperparameter alpha controls how much you want to regularize the model. If alpha is zero, then ridge regression is just linear regression.',\n",
       " 'Lasso Regression: Least Absolute Shrinkage and Selection Operator Regression (simple called Lasso Regression) is another regularized version of Linear Regression: Just like the Ridge Regression, it adds a regularization term to the cost function, but it uses the L1 norm of the weight vector instead of half the square of the L2 norm .',\n",
       " 'An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with a few non-zero feature weights).',\n",
       " 'Elastic Net Regression: This is a middle ground between Ridge and Lasso regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization term and can be controlled with “r”. When r=0, Elastic Net is equivalent to Ridge Regression, and when r=1, it is equivalent to Lasso Regression.',\n",
       " 'It is always preferable to have at least a little bit of regularization and generally plain linear regression should always be avoided. Ridge is a good default, but if only a few features are useful in a particular data set, then Lasso should be used. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of instances or when several features are strongly correlated.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new14 = lst14[4:37]\n",
    "lst_new14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you explain cost function of decision trees?</td>\n",
       "      <td>Answer: Before we answer this question, it is important to note that Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. Hence their cost functions are also different.Cost function for classification type problems:Gini impurity is an important concept before we can understand cost function, so let me first explain that.where “p” is the ratio of class k instances among the training instances at the ith node. What does that mean? Let’s understand from an example below. Figure I displays a simple visualization of Iris Decision Tree of depth 2. Top level is the root node. The concept of dividing a training set into a set of decisions is fairly simple in the algorithm. Here, for instance, Iris data set is divided into two subsets based on a single feature called “petal width” at the root node. If petal width is less than or equal to 0.8, then the algorithm goes to depth 1, left. If not, it goes to depth 1, right. Where it further divides instances based on an additional feature of “petal width”. Depth 1, right node has a sample of 100 instances and applies 0 instances to Iris-Setosa, 50 instances to Iris-Versicolor and remaining 50 to Iris-Virginica.So this node’s gini impurity is 0.5:Similarly, at depth 1, left node, Gini impurity is zero because all training instances apply to the same class. The node is essentially “pure”.Now that understand what is Gini impurity, let’s get into the meat of the answer. Decision Trees use Classification and Regression Tree (CART) algorithm for training purposes based on a simple concept that the data set will be split into two subsets using a single feature (k) and threshold (t) . In Iris data set the feature was “petal width” and threshold was 0.8. How does it choose k and t? It searches for the pair (k, t) that produces the purest subsets. So the cost function that the algorithm tries to minimize is given by below equation:where G left or right represent gini impurity of subsets while m represents instances of the subsets.Cost function for regression type problems:For regression trees, cost function is fairly intuitive. We use Residual Sum of Squares (RSS). Equation III displays cost function of regression type trees, where “y” is ground truth and “y-hat” is the predicted value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does collinearity affect your models?</td>\n",
       "      <td>Answer: Collinearity refers to a situation where two or more predictor variables are closely related to one another. Figure 2 below shows as example of collinear variables. Variable 2 strictly follows variable 1 with a Pearson correlation coefficient of 1. So obviously one of these variables will behave like noise when fed into machine learning models.The presence of collinearity can become problematic in regression type questions, since it becomes difficult to separate out the individual effects of collinear variables on the response. Or in other words collinearity reduces the accuracy of the estimates of the regression coefficients and results in increase in errors. This will ultimately lead to decline in the t-statistic, as a result, in the presence of collinearity we may fail to reject the null hypothesis.A simple way to detect collinearity is to look at the correlation matrix of the predictor variables. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore collinearity problem with the data. Unfortunately not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for the collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. Such a situation is called multi-collinearity. For such cases, instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF for each variable can be computed using the formula:Where R-square term is the regression of variable X, onto all of the other predictors. If VIF is close to or more than one then collinearity is present. When faced with the problem of collinearity there are two possible solutions. One is to drop the redundant variable. This can be done without compromising the regression fit. The second solution is to combine the collinear variables together into a single predictor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How will you explain deep neural network to a layman?</td>\n",
       "      <td>Answer: The idea of Neural Network (NN) originally stemmed from human brain that is designed to identify patterns. NN is a set of algorithms that interpret sensory data through machine perception, labeling and clustering raw input data. Any type of real world data, be it images, text, sound or even time series data must be converted into a vector space containing numbers.The word “deep” in deep neural network means that the neural network consists of multiple layers. These layers are made of nodes where computation takes place. An analogy of node is a neuron in human brain which fires when it encounters sufficient stimuli. A node combines data from the raw input with their coefficients or weights that either dampen or amplify that input based on the weight. The product of input and weight is then summed at the summation node shown below in Fig. 3, which is then passed on to the activation function which determines whether and to what extend that signal should progress further through the network to affect the ultimate outcome. A node layer is a row of such neuron-like switches that turn on or off as the input is fed through the network.Deep neural networks differs from earlier version of neural networks such as perceptrons because they were shallow and simply consisted of input and output layer along with one hidden layer.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What would be a 3-minute pitch for your data science take home project?</td>\n",
       "      <td>Answer: A typical data science interview process starts with a take home data analysis project. I have taken two of those and time span may vary depending on the complexity of the take home project. First time, I was given two days to solve a problem using machine learning along with an executive summary. While the second time I was given two weeks to solve a problem. It is needless to point out that second time it was a much harder problem where I was dealing with class imbalanced data set. So 3-minutes sales pitch type interview question allows you to showcase your grasp of the problem at hand. Please be sure to start with what is your interpretation of the problem; your brief approach to solving the problem; what types of machine learning models did you use in your approach and why? And close this by boasting about the accuracy of your models.I believe that this is a very important question during your interview that enables you to prove that you are a leader in Data Science field and can solve a complex problem with the latest and greatest tools at your disposal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do you mean by model regularization and how will you achieve regularization in linear models?</td>\n",
       "      <td>Answer: Regularization is a term used for constraining your machine learning model. One good way to constrain or reduce overfitting in machine learning models is to have fewer degrees of freedom. With fewer degrees of freedom, it gets harder for the model to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees of freedom. However, for linear models, regularization is typically achieved by constraining the weights of the model. So, instead of linear regression, Ridge regression, Lasso Regression and Elastic Net models have three different ways to constraint the weights. For the sake of completeness, lets start with the definition of linear regression first:Mean Square Error cost function for a Linear Regression model is defined as:Where thetaT is the transpose of theta (a row vector instead of column vector).Ridge Regression: Is a regularized version of Linear Regression, i.e., an additional regularization term is added to the cost function. This forces the learning algorithms to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.The hyperparameter alpha controls how much you want to regularize the model. If alpha is zero, then ridge regression is just linear regression.Lasso Regression: Least Absolute Shrinkage and Selection Operator Regression (simple called Lasso Regression) is another regularized version of Linear Regression: Just like the Ridge Regression, it adds a regularization term to the cost function, but it uses the L1 norm of the weight vector instead of half the square of the L2 norm .An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with a few non-zero feature weights).Elastic Net Regression: This is a middle ground between Ridge and Lasso regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization term and can be controlled with “r”. When r=0, Elastic Net is equivalent to Ridge Regression, and when r=1, it is equivalent to Lasso Regression.It is always preferable to have at least a little bit of regularization and generally plain linear regression should always be avoided. Ridge is a good default, but if only a few features are useful in a particular data set, then Lasso should be used. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of instances or when several features are strongly correlated.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              Questions  \\\n",
       "0                                                      Can you explain cost function of decision trees?   \n",
       "1                                                             How does collinearity affect your models?   \n",
       "2                                                 How will you explain deep neural network to a layman?   \n",
       "3                               What would be a 3-minute pitch for your data science take home project?   \n",
       "4    What do you mean by model regularization and how will you achieve regularization in linear models?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Answer: Before we answer this question, it is important to note that Decision Trees are versatile Machine Learning algorithms that can perform both classification and regression tasks. Hence their cost functions are also different.Cost function for classification type problems:Gini impurity is an important concept before we can understand cost function, so let me first explain that.where “p” is the ratio of class k instances among the training instances at the ith node. What does that mean? Let’s understand from an example below. Figure I displays a simple visualization of Iris Decision Tree of depth 2. Top level is the root node. The concept of dividing a training set into a set of decisions is fairly simple in the algorithm. Here, for instance, Iris data set is divided into two subsets based on a single feature called “petal width” at the root node. If petal width is less than or equal to 0.8, then the algorithm goes to depth 1, left. If not, it goes to depth 1, right. Where it further divides instances based on an additional feature of “petal width”. Depth 1, right node has a sample of 100 instances and applies 0 instances to Iris-Setosa, 50 instances to Iris-Versicolor and remaining 50 to Iris-Virginica.So this node’s gini impurity is 0.5:Similarly, at depth 1, left node, Gini impurity is zero because all training instances apply to the same class. The node is essentially “pure”.Now that understand what is Gini impurity, let’s get into the meat of the answer. Decision Trees use Classification and Regression Tree (CART) algorithm for training purposes based on a simple concept that the data set will be split into two subsets using a single feature (k) and threshold (t) . In Iris data set the feature was “petal width” and threshold was 0.8. How does it choose k and t? It searches for the pair (k, t) that produces the purest subsets. So the cost function that the algorithm tries to minimize is given by below equation:where G left or right represent gini impurity of subsets while m represents instances of the subsets.Cost function for regression type problems:For regression trees, cost function is fairly intuitive. We use Residual Sum of Squares (RSS). Equation III displays cost function of regression type trees, where “y” is ground truth and “y-hat” is the predicted value.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Answer: Collinearity refers to a situation where two or more predictor variables are closely related to one another. Figure 2 below shows as example of collinear variables. Variable 2 strictly follows variable 1 with a Pearson correlation coefficient of 1. So obviously one of these variables will behave like noise when fed into machine learning models.The presence of collinearity can become problematic in regression type questions, since it becomes difficult to separate out the individual effects of collinear variables on the response. Or in other words collinearity reduces the accuracy of the estimates of the regression coefficients and results in increase in errors. This will ultimately lead to decline in the t-statistic, as a result, in the presence of collinearity we may fail to reject the null hypothesis.A simple way to detect collinearity is to look at the correlation matrix of the predictor variables. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore collinearity problem with the data. Unfortunately not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for the collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation. Such a situation is called multi-collinearity. For such cases, instead of inspecting the correlation matrix, a better way to assess multicollinearity is to compute the variance inflation factor (VIF). The VIF for each variable can be computed using the formula:Where R-square term is the regression of variable X, onto all of the other predictors. If VIF is close to or more than one then collinearity is present. When faced with the problem of collinearity there are two possible solutions. One is to drop the redundant variable. This can be done without compromising the regression fit. The second solution is to combine the collinear variables together into a single predictor.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Answer: The idea of Neural Network (NN) originally stemmed from human brain that is designed to identify patterns. NN is a set of algorithms that interpret sensory data through machine perception, labeling and clustering raw input data. Any type of real world data, be it images, text, sound or even time series data must be converted into a vector space containing numbers.The word “deep” in deep neural network means that the neural network consists of multiple layers. These layers are made of nodes where computation takes place. An analogy of node is a neuron in human brain which fires when it encounters sufficient stimuli. A node combines data from the raw input with their coefficients or weights that either dampen or amplify that input based on the weight. The product of input and weight is then summed at the summation node shown below in Fig. 3, which is then passed on to the activation function which determines whether and to what extend that signal should progress further through the network to affect the ultimate outcome. A node layer is a row of such neuron-like switches that turn on or off as the input is fed through the network.Deep neural networks differs from earlier version of neural networks such as perceptrons because they were shallow and simply consisted of input and output layer along with one hidden layer.  \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Answer: A typical data science interview process starts with a take home data analysis project. I have taken two of those and time span may vary depending on the complexity of the take home project. First time, I was given two days to solve a problem using machine learning along with an executive summary. While the second time I was given two weeks to solve a problem. It is needless to point out that second time it was a much harder problem where I was dealing with class imbalanced data set. So 3-minutes sales pitch type interview question allows you to showcase your grasp of the problem at hand. Please be sure to start with what is your interpretation of the problem; your brief approach to solving the problem; what types of machine learning models did you use in your approach and why? And close this by boasting about the accuracy of your models.I believe that this is a very important question during your interview that enables you to prove that you are a leader in Data Science field and can solve a complex problem with the latest and greatest tools at your disposal.  \n",
       "4  Answer: Regularization is a term used for constraining your machine learning model. One good way to constrain or reduce overfitting in machine learning models is to have fewer degrees of freedom. With fewer degrees of freedom, it gets harder for the model to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees of freedom. However, for linear models, regularization is typically achieved by constraining the weights of the model. So, instead of linear regression, Ridge regression, Lasso Regression and Elastic Net models have three different ways to constraint the weights. For the sake of completeness, lets start with the definition of linear regression first:Mean Square Error cost function for a Linear Regression model is defined as:Where thetaT is the transpose of theta (a row vector instead of column vector).Ridge Regression: Is a regularized version of Linear Regression, i.e., an additional regularization term is added to the cost function. This forces the learning algorithms to not only fit the data but also keep the model weights as small as possible. Note that the regularization term should only be added to the cost function during training. Once the model is trained, you want to evaluate the model’s performance using the unregularized performance measure.The hyperparameter alpha controls how much you want to regularize the model. If alpha is zero, then ridge regression is just linear regression.Lasso Regression: Least Absolute Shrinkage and Selection Operator Regression (simple called Lasso Regression) is another regularized version of Linear Regression: Just like the Ridge Regression, it adds a regularization term to the cost function, but it uses the L1 norm of the weight vector instead of half the square of the L2 norm .An important characteristic of Lasso Regression is that it tends to completely eliminate the weights of the least important features (i.e., set them to zero). In other words, Lasso Regression automatically performs feature selection and outputs a sparse model (i.e., with a few non-zero feature weights).Elastic Net Regression: This is a middle ground between Ridge and Lasso regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization term and can be controlled with “r”. When r=0, Elastic Net is equivalent to Ridge Regression, and when r=1, it is equivalent to Lasso Regression.It is always preferable to have at least a little bit of regularization and generally plain linear regression should always be avoided. Ridge is a good default, but if only a few features are useful in a particular data set, then Lasso should be used. In general, Elastic Net is preferred over Lasso since Lasso may behave erratically when the number of features is greater than the number of instances or when several features are strongly correlated.  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^Question\\s[1|I|V]+:.[\\w\\d\\s]+\\?*\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new14:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^Question\\s[1|I|V]+:\",\" \",ques[i])\n",
    "    \n",
    "df14=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign in',\n",
       " 'Decision Tree Questions To Ace Your Next Data Science Interview',\n",
       " 'Abhishek Verma',\n",
       " 'Nov 20, 2020·2 min read',\n",
       " 'What is a Decision Tree?',\n",
       " 'A decision tree is a tree in which every node specifies a test of some attribute of the data and each branch descending from that node corresponds to one of the possible values for this attribute.',\n",
       " 'To which kind of problems are decision trees most suitable?',\n",
       " 'On what basis is an attribute selected in the decision tree for choosing it as a node?',\n",
       " 'Attribute selection is done using Information Gain in decision trees. The attribute with maximum information gain is selected.',\n",
       " 'What is Information Gain? What are its disadvantages?',\n",
       " 'Information gain is the reduction in entropy due to the selection of an attribute. Information gain ratio biases the decision tree against considering attributes with a large number of distinct values which might lead to overfitting. In order to solve this problem, information gain ratio is used.',\n",
       " 'What is the inductive bias of decision trees?',\n",
       " 'Shorter trees are preferred over longer trees. Trees that place high information gain attributes close to the root are preferred over those that do not.',\n",
       " 'How does a decision tree handle continuous attributes?',\n",
       " 'By converting continuous attributes to a threshold-based boolean attribute. The threshold is decided by maximizing the information gain.',\n",
       " 'How does a decision tree handle missing attribute values?',\n",
       " 'One way to assign the most common value of that attribute to the missing attribute value. The other way is to assign a probability to each of the possible values of the attribute based on other samples.',\n",
       " 'Name some algorithms used for deriving decision trees?',\n",
       " 'ID3 and C4.5',\n",
       " 'Conclusion',\n",
       " 'As much as deep learning seems to be in vogue, decision trees still stand in the shadows and support the machine learning presence in the technological landscape. They remain an integral part of the ML stack and Data Science interviews.',\n",
       " 'I am a Data Scientist. I like to write about concepts related to deep learning.',\n",
       " '8 ',\n",
       " '8\\xa0',\n",
       " '8 ',\n",
       " 'Your home for data science. A Medium publication sharing concepts, ideas and codes.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deecision tree\n",
    "lst15= []\n",
    "url = \"https://towardsdatascience.com/decision-tree-questions-to-ace-your-next-data-science-interview-692ee246b6ae\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['h1','p'])\n",
    "for answer in answers:\n",
    "    lst15.append(answer.text)\n",
    "lst15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is a Decision Tree?',\n",
       " 'A decision tree is a tree in which every node specifies a test of some attribute of the data and each branch descending from that node corresponds to one of the possible values for this attribute.',\n",
       " 'To which kind of problems are decision trees most suitable?',\n",
       " 'On what basis is an attribute selected in the decision tree for choosing it as a node?',\n",
       " 'Attribute selection is done using Information Gain in decision trees. The attribute with maximum information gain is selected.',\n",
       " 'What is Information Gain? What are its disadvantages?',\n",
       " 'Information gain is the reduction in entropy due to the selection of an attribute. Information gain ratio biases the decision tree against considering attributes with a large number of distinct values which might lead to overfitting. In order to solve this problem, information gain ratio is used.',\n",
       " 'What is the inductive bias of decision trees?',\n",
       " 'Shorter trees are preferred over longer trees. Trees that place high information gain attributes close to the root are preferred over those that do not.',\n",
       " 'How does a decision tree handle continuous attributes?',\n",
       " 'By converting continuous attributes to a threshold-based boolean attribute. The threshold is decided by maximizing the information gain.',\n",
       " 'How does a decision tree handle missing attribute values?',\n",
       " 'One way to assign the most common value of that attribute to the missing attribute value. The other way is to assign a probability to each of the possible values of the attribute based on other samples.',\n",
       " 'Name some algorithms used for deriving decision trees?',\n",
       " 'ID3 and C4.5']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new15 = lst15[4:19]\n",
    "lst_new15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is a Decision Tree?</td>\n",
       "      <td>A decision tree is a tree in which every node specifies a test of some attribute of the data and each branch descending from that node corresponds to one of the possible values for this attribute.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To which kind of problems are decision trees most suitable?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>On what basis is an attribute selected in the decision tree for choosing it as a node?</td>\n",
       "      <td>Attribute selection is done using Information Gain in decision trees. The attribute with maximum information gain is selected.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is Information Gain? What are its disadvantages?</td>\n",
       "      <td>Information gain is the reduction in entropy due to the selection of an attribute. Information gain ratio biases the decision tree against considering attributes with a large number of distinct values which might lead to overfitting. In order to solve this problem, information gain ratio is used.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the inductive bias of decision trees?</td>\n",
       "      <td>Shorter trees are preferred over longer trees. Trees that place high information gain attributes close to the root are preferred over those that do not.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                Questions  \\\n",
       "0                                                                What is a Decision Tree?   \n",
       "1                             To which kind of problems are decision trees most suitable?   \n",
       "2  On what basis is an attribute selected in the decision tree for choosing it as a node?   \n",
       "3                                   What is Information Gain? What are its disadvantages?   \n",
       "4                                           What is the inductive bias of decision trees?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                      Answer  \n",
       "0                                                                                                       A decision tree is a tree in which every node specifies a test of some attribute of the data and each branch descending from that node corresponds to one of the possible values for this attribute.  \n",
       "1                                                                                                                                                                                                                                                                                                             \n",
       "2                                                                                                                                                                             Attribute selection is done using Information Gain in decision trees. The attribute with maximum information gain is selected.  \n",
       "3  Information gain is the reduction in entropy due to the selection of an attribute. Information gain ratio biases the decision tree against considering attributes with a large number of distinct values which might lead to overfitting. In order to solve this problem, information gain ratio is used.  \n",
       "4                                                                                                                                                   Shorter trees are preferred over longer trees. Trees that place high information gain attributes close to the root are preferred over those that do not.  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\".\\?$\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new15:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^Question\\s[1|I|V]+:\",\" \",ques[i])\n",
    "    \n",
    "df15=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df15[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df15=df15.drop(df15.index[[1]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df15[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This article was published as a part of the\\xa0Data Science Blogathon',\n",
       " 'Introduction',\n",
       " 'K nearest neighbour (KNN) is one of the most widely used and simplest algorithms for classification problems under supervised Machine Learning.\\xa0\\n',\n",
       " 'Therefore it becomes necessary for every aspiring Data Scientist and Machine Learning Engineer to have a good knowledge of this algorithm.',\n",
       " 'In this article, we will discuss the most important questions on the K Nearest Neighbor (KNN) Algorithm which is helpful to get you a clear understanding of the algorithm, and also for Data Science Interviews, which covers its very fundamental level to complex concepts.',\n",
       " 'Let’s get started,',\n",
       " '1. What is the KNN Algorithm?',\n",
       " 'KNN(K-nearest neighbours) is a supervised learning and non-parametric algorithm that can be used to solve both classification and regression problem statements.',\n",
       " 'It uses data in which there is a target column present i.e, labelled data to model a function to produce an output for the unseen data. It uses the euclidean distance formula to compute the distance between the data points for classification or prediction.',\n",
       " 'The main objective of this algorithm is that similar data points must be close to each other so it uses the distance to calculate the similar points that are close to each other.',\n",
       " '',\n",
       " 'Image Source: Google Images',\n",
       " '2. Why is KNN a non-parametric Algorithm?',\n",
       " 'The term “non-parametric” refers to not making any assumptions on the underlying data distribution. These methods do not have any fixed numbers of parameters in the model.',\n",
       " 'Similarly in KNN, the model parameters grow with the training data by considering each training case as a parameter of the model. So, KNN is a non-parametric algorithm.',\n",
       " '3. What is “K” in the KNN Algorithm?',\n",
       " 'K represents the number of nearest neighbours you want to select to predict the class of a given item, which is coming as an unseen dataset for the model.',\n",
       " '\\xa0',\n",
       " '4. Why is the odd value of “K” preferred over even values in the KNN Algorithm?',\n",
       " 'The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting. If the square root of a number of data points is even, then add or subtract 1 to it to make it odd.',\n",
       " '\\xa0',\n",
       " '5. How does the KNN algorithm make the predictions on the unseen dataset?',\n",
       " 'The following operations have happened during each iteration of the algorithm. For each of the unseen or test data point, the kNN classifier must:',\n",
       " 'Step-1: Calculate the distances of test point to all points in the training set and store them',\n",
       " 'Step-2: Sort the calculated distances in increasing order',\n",
       " 'Step-3: Store the K nearest points from our training dataset',\n",
       " 'Step-4: Calculate the proportions of each class',\n",
       " 'Step-5: Assign the class with the highest proportion',\n",
       " '',\n",
       " 'Image Source: Google Images',\n",
       " '6. Is Feature Scaling required for the KNN Algorithm? Explain with proper justification.',\n",
       " 'Yes, feature scaling is required to get the better performance of the KNN algorithm.',\n",
       " 'For Example, Imagine a dataset having n number of instances and N number of features. There is one feature having values ranging between 0 and 1. Meanwhile, there is also a feature that varies from -999 to 999. When these values are substituted in the formula of Euclidean Distance, this will affect the performance by giving higher weightage to variables having a higher magnitude.',\n",
       " '\\xa0',\n",
       " '7. What is space and time complexity of the KNN Algorithm?',\n",
       " 'Time complexity:',\n",
       " 'The distance calculation step requires quadratic time complexity, and the sorting of the calculated distances requires an O(N log N) time. Together, we can say that the process is an O(N3 log N) process, which is a monstrously long process.',\n",
       " 'Space complexity:',\n",
       " 'Since it stores all the pairwise distances and is sorted in memory on a machine, memory is also the problem. Usually, local machines will crash, if we have very large datasets.',\n",
       " '8. Can the KNN algorithm be used for regression problem statements?',\n",
       " 'Yes, KNN can be used for regression problem statements.',\n",
       " 'In other words, the KNN algorithm can be applied \\u200awhen the dependent variable is continuous. For regression problem statements, the predicted value is given by the average of the values of its k nearest neighbours.',\n",
       " '9. Why is the KNN Algorithm known as Lazy Learner?',\n",
       " 'When the KNN algorithm gets the training data, it does not learn and make a model, it just stores the data. Instead of finding any discriminative function with the help of the training data, it follows instance-based learning and also uses the training data when it actually needs to do some prediction on the unseen datasets.',\n",
       " 'As a result, KNN does not immediately learn a model rather delays the learning thereby being referred to as Lazy Learner.',\n",
       " '10. Why is it recommended not to use the KNN Algorithm for large datasets?',\n",
       " 'The Problem in processing the data:',\n",
       " 'KNN works well with smaller datasets because it is a lazy learner. It needs to store all the data and then make a decision only at run time. It includes the computation of distances for a given point with all other points. So if the dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm.',\n",
       " 'Sensitive to noise:',\n",
       " 'Another thing in the context of large datasets is that there is more likely a chance of noise in the dataset which adversely affects the performance of the KNN algorithm since the KNN algorithm is sensitive to the noise present in the dataset.',\n",
       " '11. How to handle categorical variables in the KNN Algorithm?',\n",
       " 'To handle the categorical variables we have to create dummy variables out of a categorical variable and include them instead of the original categorical variable. Unlike regression, create k dummies instead of (k-1).',\n",
       " 'For example, a categorical variable named “Degree” has 5 unique levels or categories. So we will create 5 dummy variables. Each dummy variable has 1 against its degree and else 0.',\n",
       " '12. How to choose the optimal value of K in the KNN Algorithm?',\n",
       " 'There is no straightforward method to find the optimal value of K in the KNN algorithm.',\n",
       " 'You have to play around with different values to choose which value of K should be optimal for my problem statement. Choosing the right value of K is done through a process known as Hyperparameter Tuning.',\n",
       " 'The optimum value of K for KNN is highly dependent on the data itself. In different scenarios, the optimum K may vary. It is more or less a hit and trial method.',\n",
       " 'There is no one proper method of finding the K value in the KNN algorithm. No method is the rule of thumb but you should try the following suggestions:',\n",
       " '1. Square Root Method: Take the square root of the number of samples in the training dataset and assign it to the K value.',\n",
       " '2. Cross-Validation Method: We should also take the help of cross-validation to find out the optimal value of K in KNN. Start with the minimum value of k i.e, K=1, and run cross-validation, measure the accuracy, and keep repeating till the results become consistent.',\n",
       " 'As the value of K increases, the error usually goes down after each one-step increase in K, then stabilizes, and then raises again. Finally, pick the optimum K at the beginning of the stable zone. This technique is also known as the Elbow Method.',\n",
       " '\\n',\n",
       " 'Image Source: Google Images',\n",
       " '3. Domain Knowledge: Sometimes with the help of domain knowledge for a particular use case we are able to find the optimum value of K (K should be an odd number).',\n",
       " 'I would therefore suggest trying a mix of all the above points to reach any conclusion.',\n",
       " '13. How can you relate KNN Algorithm to the Bias-Variance tradeoff?',\n",
       " 'Problem with having too small K:',\n",
       " 'The major concern associated with small values of K lies behind the fact that the smaller value causes noise to have a higher influence on the result which will also lead to a large variance in the predictions.',\n",
       " 'Problem with having too large K:',\n",
       " 'The larger the value of K, the higher is the accuracy. If K is too large, then our model is under-fitted. As a result, the error will go up again. So, to prevent your model from under-fitting it should retain the generalization capabilities otherwise there are fair chances that your model may perform well in the training data but drastically fail in the real data. The computational expense of the algorithm also increases if we choose the k very large.',\n",
       " 'So, choosing k to a large value may lead to a model with a large bias(error).',\n",
       " 'The effects of k values on the bias and variance is explained below :',\n",
       " 'So, there is a tradeoff between overfitting and underfitting and you have to maintain a balance while choosing the value of K in KNN. Therefore, K should not be too small or too large.',\n",
       " '',\n",
       " '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Image Source: Google Images',\n",
       " '14. Which algorithm can be used for value imputation in both categorical and continuous categories of data?',\n",
       " 'KNN is the only algorithm that can be used for the imputation of both categorical and continuous variables. It can be used as one of many techniques when it comes to handling missing values.',\n",
       " 'To impute a new sample, we determine the samples in the training set “nearest” to the new sample and averages the nearby points to impute. A Scikit learn library of Python\\xa0provides a quick and convenient way to use this technique.',\n",
       " 'Note: NaNs are omitted while distances are calculated. Hence we replace the missing values with the average value of the neighbours. The missing values will then be replaced by the average value of their “neighbours”.',\n",
       " '\\n15. Explain the statement- “The KNN algorithm does more computation on test time rather than train time”.',\n",
       " 'The above-given statement is absolutely true.',\n",
       " 'The basic idea behind the kNN algorithm is to determine a k-long list of samples that are close to a sample that we want to classify. Therefore, the training phase is basically storing a training set, whereas during the prediction stage the algorithm looks for k-neighbours using that stored data. Moreover, KNN does not learn anything from the training dataset as well.',\n",
       " '16. What are the things which should be kept in our mind while choosing the value of k in the KNN Algorithm?',\n",
       " 'If K is small, then results might not be reliable because the noise will have a higher influence on the result. If K is large, then there will be a lot of processing to be done which may adversely impact the performance of the algorithm.',\n",
       " 'So, the following things must be considered while choosing the value of K:',\n",
       " '17. What are the advantages of the KNN Algorithm?',\n",
       " 'Some of the advantages of the KNN algorithm are as follows:',\n",
       " '1. No Training Period: It does not learn anything during the training period since it does not find any discriminative function with the help of the training data. In simple words, actually, there is no training period for the KNN algorithm. It stores the training dataset and learns from it only when we use the algorithm for making the real-time predictions on the test dataset.',\n",
       " 'As a result, the KNN algorithm is much faster than other algorithms which require training.\\xa0For Example, SupportVector Machines(SVMs), Linear Regression, etc.',\n",
       " 'Moreover, since the KNN algorithm does not require any training before making predictions as a result new data can be added seamlessly without impacting the accuracy of the algorithm.',\n",
       " '2. Easy to implement and understand: To implement the KNN algorithm, we need only two parameters i.e. the value of K and the distance metric(e.g. Euclidean or Manhattan, etc.). Since both the parameters are easily interpretable therefore they are easy to understand.',\n",
       " '18. What are the disadvantages of the KNN Algorithm?',\n",
       " 'Some of the disadvantages of the KNN algorithm are as follows:',\n",
       " '1. Does not work well with large datasets: In large datasets, the cost of calculating the distance between the new point and each existing point is huge which decreases the performance of the algorithm.',\n",
       " '2. Does not work well with high dimensions: KNN algorithms generally do not work well with high dimensional data since, with the increasing number of dimensions, it becomes difficult to calculate the distance for each dimension.',\n",
       " '3. Need feature scaling: We need to do feature scaling (standardization and normalization) on the dataset before feeding it to the KNN algorithm otherwise it may generate wrong predictions.',\n",
       " '4. Sensitive to Noise and Outliers: KNN is highly sensitive to the noise present in the dataset and requires manual imputation of the missing values along with outliers removal.',\n",
       " '19. Is it possible to use the KNN algorithm for Image processing? ',\n",
       " 'Yes, KNN can be used for image processing by converting a 3-dimensional image into a single-dimensional vector and then using it as the input to the KNN algorithm.',\n",
       " '',\n",
       " '20. What are the real-life applications of KNN Algorithms?',\n",
       " 'The various real-life applications of the KNN Algorithm includes:',\n",
       " '1.\\xa0KNN allows the calculation of the credit rating. By collecting the financial characteristics vs. comparing people having similar financial features to a database we can calculate the same. Moreover, the very nature of a credit rating where people who have similar financial details would be given similar credit ratings also plays an important role. Hence the existing database can then be used to predict a new customer’s credit rating, without having to perform all the calculations.',\n",
       " '2. In political science: KNN can also be used to predict whether a potential voter “will vote” or “will not vote”, or to “vote Democrat” or “vote Republican” in an election.',\n",
       " 'Apart from the above-mentioned use cases, KNN algorithms are also used for handwriting detection (like OCR), Image recognition, and video recognition.',\n",
       " 'End Notes',\n",
       " 'Thanks for reading!',\n",
       " 'I hope you enjoyed the questions and were able to test your knowledge about K Nearest Neighbor (KNN) Algorithm.\\n',\n",
       " 'If you liked this and want to know more, go visit my other articles on Data Science and Machine Learning by clicking on the Link',\n",
       " 'Please feel free to contact me on Linkedin, Email.',\n",
       " 'Something not mentioned or want to share your thoughts? Feel free to comment below And I’ll get back to you.',\n",
       " 'Chirag Goyal',\n",
       " 'Currently, I pursuing my Bachelor of Technology (B.Tech) in Computer Science and Engineering from the Indian Institute of Technology Jodhpur(IITJ). I am very enthusiastic about Machine learning, Deep Learning, and Artificial Intelligence.',\n",
       " 'The media shown in this article are not owned by Analytics Vidhya and is used at the Author’s discretion.',\n",
       " '',\n",
       " ' Notify me of follow-up comments by email.',\n",
       " ' Notify me of new posts by email.',\n",
       " '',\n",
       " '',\n",
       " 'Top Resources',\n",
       " 'Basic Concepts of Object-Oriented Programming in Python',\n",
       " 'Python Tutorial: Working with CSV file for Data Science',\n",
       " 'Commonly used Machine Learning Algorithms (with Python and R Codes)',\n",
       " '3 Interesting Python Projects With Code for Beginners!',\n",
       " '\\n\\n\\n×\\n\\n',\n",
       " '© Copyright 2013-2021 Analytics Vidhya.']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn\n",
    "lst16= []\n",
    "url = \"https://www.analyticsvidhya.com/blog/2021/05/20-questions-to-test-your-skills-on-k-nearest-neighbour/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all(['p','h2'])\n",
    "for answer in answers:\n",
    "    lst16.append(answer.text)\n",
    "lst16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What is the KNN Algorithm?',\n",
       " 'KNN(K-nearest neighbours) is a supervised learning and non-parametric algorithm that can be used to solve both classification and regression problem statements.',\n",
       " 'It uses data in which there is a target column present i.e, labelled data to model a function to produce an output for the unseen data. It uses the euclidean distance formula to compute the distance between the data points for classification or prediction.',\n",
       " 'The main objective of this algorithm is that similar data points must be close to each other so it uses the distance to calculate the similar points that are close to each other.',\n",
       " '',\n",
       " 'Image Source: Google Images',\n",
       " '2. Why is KNN a non-parametric Algorithm?',\n",
       " 'The term “non-parametric” refers to not making any assumptions on the underlying data distribution. These methods do not have any fixed numbers of parameters in the model.',\n",
       " 'Similarly in KNN, the model parameters grow with the training data by considering each training case as a parameter of the model. So, KNN is a non-parametric algorithm.',\n",
       " '3. What is “K” in the KNN Algorithm?',\n",
       " 'K represents the number of nearest neighbours you want to select to predict the class of a given item, which is coming as an unseen dataset for the model.',\n",
       " '\\xa0',\n",
       " '4. Why is the odd value of “K” preferred over even values in the KNN Algorithm?',\n",
       " 'The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting. If the square root of a number of data points is even, then add or subtract 1 to it to make it odd.',\n",
       " '\\xa0',\n",
       " '5. How does the KNN algorithm make the predictions on the unseen dataset?',\n",
       " 'The following operations have happened during each iteration of the algorithm. For each of the unseen or test data point, the kNN classifier must:',\n",
       " 'Step-1: Calculate the distances of test point to all points in the training set and store them',\n",
       " 'Step-2: Sort the calculated distances in increasing order',\n",
       " 'Step-3: Store the K nearest points from our training dataset',\n",
       " 'Step-4: Calculate the proportions of each class',\n",
       " 'Step-5: Assign the class with the highest proportion',\n",
       " '',\n",
       " 'Image Source: Google Images',\n",
       " '6. Is Feature Scaling required for the KNN Algorithm? Explain with proper justification.',\n",
       " 'Yes, feature scaling is required to get the better performance of the KNN algorithm.',\n",
       " 'For Example, Imagine a dataset having n number of instances and N number of features. There is one feature having values ranging between 0 and 1. Meanwhile, there is also a feature that varies from -999 to 999. When these values are substituted in the formula of Euclidean Distance, this will affect the performance by giving higher weightage to variables having a higher magnitude.',\n",
       " '\\xa0',\n",
       " '7. What is space and time complexity of the KNN Algorithm?',\n",
       " 'Time complexity:',\n",
       " 'The distance calculation step requires quadratic time complexity, and the sorting of the calculated distances requires an O(N log N) time. Together, we can say that the process is an O(N3 log N) process, which is a monstrously long process.',\n",
       " 'Space complexity:',\n",
       " 'Since it stores all the pairwise distances and is sorted in memory on a machine, memory is also the problem. Usually, local machines will crash, if we have very large datasets.',\n",
       " '8. Can the KNN algorithm be used for regression problem statements?',\n",
       " 'Yes, KNN can be used for regression problem statements.',\n",
       " 'In other words, the KNN algorithm can be applied \\u200awhen the dependent variable is continuous. For regression problem statements, the predicted value is given by the average of the values of its k nearest neighbours.',\n",
       " '9. Why is the KNN Algorithm known as Lazy Learner?',\n",
       " 'When the KNN algorithm gets the training data, it does not learn and make a model, it just stores the data. Instead of finding any discriminative function with the help of the training data, it follows instance-based learning and also uses the training data when it actually needs to do some prediction on the unseen datasets.',\n",
       " 'As a result, KNN does not immediately learn a model rather delays the learning thereby being referred to as Lazy Learner.',\n",
       " '10. Why is it recommended not to use the KNN Algorithm for large datasets?',\n",
       " 'The Problem in processing the data:',\n",
       " 'KNN works well with smaller datasets because it is a lazy learner. It needs to store all the data and then make a decision only at run time. It includes the computation of distances for a given point with all other points. So if the dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm.',\n",
       " 'Sensitive to noise:',\n",
       " 'Another thing in the context of large datasets is that there is more likely a chance of noise in the dataset which adversely affects the performance of the KNN algorithm since the KNN algorithm is sensitive to the noise present in the dataset.',\n",
       " '11. How to handle categorical variables in the KNN Algorithm?',\n",
       " 'To handle the categorical variables we have to create dummy variables out of a categorical variable and include them instead of the original categorical variable. Unlike regression, create k dummies instead of (k-1).',\n",
       " 'For example, a categorical variable named “Degree” has 5 unique levels or categories. So we will create 5 dummy variables. Each dummy variable has 1 against its degree and else 0.',\n",
       " '12. How to choose the optimal value of K in the KNN Algorithm?',\n",
       " 'There is no straightforward method to find the optimal value of K in the KNN algorithm.',\n",
       " 'You have to play around with different values to choose which value of K should be optimal for my problem statement. Choosing the right value of K is done through a process known as Hyperparameter Tuning.',\n",
       " 'The optimum value of K for KNN is highly dependent on the data itself. In different scenarios, the optimum K may vary. It is more or less a hit and trial method.',\n",
       " 'There is no one proper method of finding the K value in the KNN algorithm. No method is the rule of thumb but you should try the following suggestions:',\n",
       " '1. Square Root Method: Take the square root of the number of samples in the training dataset and assign it to the K value.',\n",
       " '2. Cross-Validation Method: We should also take the help of cross-validation to find out the optimal value of K in KNN. Start with the minimum value of k i.e, K=1, and run cross-validation, measure the accuracy, and keep repeating till the results become consistent.',\n",
       " 'As the value of K increases, the error usually goes down after each one-step increase in K, then stabilizes, and then raises again. Finally, pick the optimum K at the beginning of the stable zone. This technique is also known as the Elbow Method.',\n",
       " '\\n',\n",
       " 'Image Source: Google Images',\n",
       " '3. Domain Knowledge: Sometimes with the help of domain knowledge for a particular use case we are able to find the optimum value of K (K should be an odd number).',\n",
       " 'I would therefore suggest trying a mix of all the above points to reach any conclusion.',\n",
       " '13. How can you relate KNN Algorithm to the Bias-Variance tradeoff?',\n",
       " 'Problem with having too small K:',\n",
       " 'The major concern associated with small values of K lies behind the fact that the smaller value causes noise to have a higher influence on the result which will also lead to a large variance in the predictions.',\n",
       " 'Problem with having too large K:',\n",
       " 'The larger the value of K, the higher is the accuracy. If K is too large, then our model is under-fitted. As a result, the error will go up again. So, to prevent your model from under-fitting it should retain the generalization capabilities otherwise there are fair chances that your model may perform well in the training data but drastically fail in the real data. The computational expense of the algorithm also increases if we choose the k very large.',\n",
       " 'So, choosing k to a large value may lead to a model with a large bias(error).',\n",
       " 'The effects of k values on the bias and variance is explained below :',\n",
       " 'So, there is a tradeoff between overfitting and underfitting and you have to maintain a balance while choosing the value of K in KNN. Therefore, K should not be too small or too large.',\n",
       " '',\n",
       " '\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 Image Source: Google Images',\n",
       " '14. Which algorithm can be used for value imputation in both categorical and continuous categories of data?',\n",
       " 'KNN is the only algorithm that can be used for the imputation of both categorical and continuous variables. It can be used as one of many techniques when it comes to handling missing values.',\n",
       " 'To impute a new sample, we determine the samples in the training set “nearest” to the new sample and averages the nearby points to impute. A Scikit learn library of Python\\xa0provides a quick and convenient way to use this technique.',\n",
       " 'Note: NaNs are omitted while distances are calculated. Hence we replace the missing values with the average value of the neighbours. The missing values will then be replaced by the average value of their “neighbours”.',\n",
       " '\\n15. Explain the statement- “The KNN algorithm does more computation on test time rather than train time”.',\n",
       " 'The above-given statement is absolutely true.',\n",
       " 'The basic idea behind the kNN algorithm is to determine a k-long list of samples that are close to a sample that we want to classify. Therefore, the training phase is basically storing a training set, whereas during the prediction stage the algorithm looks for k-neighbours using that stored data. Moreover, KNN does not learn anything from the training dataset as well.',\n",
       " '16. What are the things which should be kept in our mind while choosing the value of k in the KNN Algorithm?',\n",
       " 'If K is small, then results might not be reliable because the noise will have a higher influence on the result. If K is large, then there will be a lot of processing to be done which may adversely impact the performance of the algorithm.',\n",
       " 'So, the following things must be considered while choosing the value of K:',\n",
       " '17. What are the advantages of the KNN Algorithm?',\n",
       " 'Some of the advantages of the KNN algorithm are as follows:',\n",
       " '1. No Training Period: It does not learn anything during the training period since it does not find any discriminative function with the help of the training data. In simple words, actually, there is no training period for the KNN algorithm. It stores the training dataset and learns from it only when we use the algorithm for making the real-time predictions on the test dataset.',\n",
       " 'As a result, the KNN algorithm is much faster than other algorithms which require training.\\xa0For Example, SupportVector Machines(SVMs), Linear Regression, etc.',\n",
       " 'Moreover, since the KNN algorithm does not require any training before making predictions as a result new data can be added seamlessly without impacting the accuracy of the algorithm.',\n",
       " '2. Easy to implement and understand: To implement the KNN algorithm, we need only two parameters i.e. the value of K and the distance metric(e.g. Euclidean or Manhattan, etc.). Since both the parameters are easily interpretable therefore they are easy to understand.',\n",
       " '18. What are the disadvantages of the KNN Algorithm?',\n",
       " 'Some of the disadvantages of the KNN algorithm are as follows:',\n",
       " '1. Does not work well with large datasets: In large datasets, the cost of calculating the distance between the new point and each existing point is huge which decreases the performance of the algorithm.',\n",
       " '2. Does not work well with high dimensions: KNN algorithms generally do not work well with high dimensional data since, with the increasing number of dimensions, it becomes difficult to calculate the distance for each dimension.',\n",
       " '3. Need feature scaling: We need to do feature scaling (standardization and normalization) on the dataset before feeding it to the KNN algorithm otherwise it may generate wrong predictions.',\n",
       " '4. Sensitive to Noise and Outliers: KNN is highly sensitive to the noise present in the dataset and requires manual imputation of the missing values along with outliers removal.',\n",
       " '19. Is it possible to use the KNN algorithm for Image processing? ',\n",
       " 'Yes, KNN can be used for image processing by converting a 3-dimensional image into a single-dimensional vector and then using it as the input to the KNN algorithm.',\n",
       " '',\n",
       " '20. What are the real-life applications of KNN Algorithms?',\n",
       " 'The various real-life applications of the KNN Algorithm includes:',\n",
       " '1.\\xa0KNN allows the calculation of the credit rating. By collecting the financial characteristics vs. comparing people having similar financial features to a database we can calculate the same. Moreover, the very nature of a credit rating where people who have similar financial details would be given similar credit ratings also plays an important role. Hence the existing database can then be used to predict a new customer’s credit rating, without having to perform all the calculations.',\n",
       " '2. In political science: KNN can also be used to predict whether a potential voter “will vote” or “will not vote”, or to “vote Democrat” or “vote Republican” in an election.']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new16 = lst16[7:105]\n",
    "lst_new16 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "19\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the KNN Algorithm?</td>\n",
       "      <td>KNN(K-nearest neighbours) is a supervised learning and non-parametric algorithm that can be used to solve both classification and regression problem statements.It uses data in which there is a target column present i.e, labelled data to model a function to produce an output for the unseen data. It uses the euclidean distance formula to compute the distance between the data points for classification or prediction.The main objective of this algorithm is that similar data points must be close to each other so it uses the distance to calculate the similar points that are close to each other.Image Source: Google Images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is KNN a non-parametric Algorithm?</td>\n",
       "      <td>The term “non-parametric” refers to not making any assumptions on the underlying data distribution. These methods do not have any fixed numbers of parameters in the model.Similarly in KNN, the model parameters grow with the training data by considering each training case as a parameter of the model. So, KNN is a non-parametric algorithm.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is “K” in the KNN Algorithm?</td>\n",
       "      <td>K represents the number of nearest neighbours you want to select to predict the class of a given item, which is coming as an unseen dataset for the model.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why is the odd value of “K” preferred over even values in the KNN Algorithm?</td>\n",
       "      <td>The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting. If the square root of a number of data points is even, then add or subtract 1 to it to make it odd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does the KNN algorithm make the predictions on the unseen dataset?</td>\n",
       "      <td>The following operations have happened during each iteration of the algorithm. For each of the unseen or test data point, the kNN classifier must:Step-1: Calculate the distances of test point to all points in the training set and store themStep-2: Sort the calculated distances in increasing orderStep-3: Store the K nearest points from our training datasetStep-4: Calculate the proportions of each classStep-5: Assign the class with the highest proportionImage Source: Google Images</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        Questions  \\\n",
       "0                                                      What is the KNN Algorithm?   \n",
       "1                                          Why is KNN a non-parametric Algorithm?   \n",
       "2                                               What is “K” in the KNN Algorithm?   \n",
       "3    Why is the odd value of “K” preferred over even values in the KNN Algorithm?   \n",
       "4          How does the KNN algorithm make the predictions on the unseen dataset?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Answer  \n",
       "0  KNN(K-nearest neighbours) is a supervised learning and non-parametric algorithm that can be used to solve both classification and regression problem statements.It uses data in which there is a target column present i.e, labelled data to model a function to produce an output for the unseen data. It uses the euclidean distance formula to compute the distance between the data points for classification or prediction.The main objective of this algorithm is that similar data points must be close to each other so it uses the distance to calculate the similar points that are close to each other.Image Source: Google Images  \n",
       "1                                                                                                                                                                                                                                                                                            The term “non-parametric” refers to not making any assumptions on the underlying data distribution. These methods do not have any fixed numbers of parameters in the model.Similarly in KNN, the model parameters grow with the training data by considering each training case as a parameter of the model. So, KNN is a non-parametric algorithm.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    K represents the number of nearest neighbours you want to select to predict the class of a given item, which is coming as an unseen dataset for the model.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                          The odd value of K should be preferred over even values in order to ensure that there are no ties in the voting. If the square root of a number of data points is even, then add or subtract 1 to it to make it odd.   \n",
       "4                                                                                                                                            The following operations have happened during each iteration of the algorithm. For each of the unseen or test data point, the kNN classifier must:Step-1: Calculate the distances of test point to all points in the training set and store themStep-2: Sort the calculated distances in increasing orderStep-3: Store the K nearest points from our training datasetStep-4: Calculate the proportions of each classStep-5: Assign the class with the highest proportionImage Source: Google Images  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\..+\\?\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new16:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df16=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df16.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df16[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'This article was published as a part of the\\xa0Data Science Blogathon',\n",
       " 'Before the sudden rise of neural networks, Support Vector Machines (SVMs) was considered the most powerful Machine Learning Algorithm. Still, it is more computation friendly as compared to Neural Networks and used extensively in industries. In this article, we will discuss the most important questions on SVM that are helpful to get you a clear understanding of the SVMs and also for\\xa0Data Science Interviews, which covers its very fundamental level to complex concepts.',\n",
       " '1. What are Support Vector Machines (SVMs)?',\n",
       " '👉 SVM is a supervised machine learning algorithm that works on both classification and regression problem statements.',\n",
       " '👉 For classification problem statements, it tries to differentiate data points of different classes by finding a hyperplane that maximizes the margin between the classes in the training data.',\n",
       " '👉 In simple words, SVM tries to choose the hyperplane which separates the data points as widely as possible since this margin maximization improves the model’s accuracy on the test or the unseen data.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " '2. What are Support Vectors in SVMs?',\n",
       " '👉 Support vectors are those instances that are located on the margin itself. For SVMS, the decision boundary is entirely determined by using only the support vectors.',\n",
       " '👉 Any instance that is not a support vector (not on the margin boundaries) has no influence whatsoever; you could remove them or add more instances, or move them around, and as long as they stay off the margin they won’t affect the decision boundary.',\n",
       " '👉 For computing the predictions, only the support vectors are involved, not the whole training set.',\n",
       " '3. What is the basic principle of a Support Vector Machine?',\n",
       " 'It’s aimed at finding an optimal hyperplane that is linearly separable, and for the dataset which is not directly linearly separable, it extends its formulation by transforming the original data to map into a new space, which is also called kernel trick.',\n",
       " '4. What are hard margin and soft Margin SVMs?',\n",
       " '👉 \\xa0Hard margin SVMs work only if the data is linearly separable and these types of SVMs are quite sensitive to the outliers.',\n",
       " '👉 \\xa0But our main objective is to find a good balance between keeping the margins as large as possible and limiting the margin violation i.e. instances that end up in the middle of margin or even on the wrong side, and this method is called soft margin SVM.',\n",
       " '5. What do you mean by Hinge loss?',\n",
       " 'The function defined by max(0, 1 – t) is called the hinge loss function.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " 'Properties of Hinge loss function:',\n",
       " '👉 It is equal to 0 when the value of t is greater than or equal to 1 i.e, t>=1.',\n",
       " '👉 Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1.',\n",
       " '👉 It is not differentiable at t = 1.',\n",
       " '👉 It penalizes the model for wrongly classifying the instances and increases as far the instance is classified from the correct region of classification.',\n",
       " '6. What is the “Kernel trick”?',\n",
       " '👉 A Kernel is a function capable of computing the dot product of instances mapped in higher dimension space without actually transforming all the instances into the higher feature space and calculating the dot product.',\n",
       " '👉 This trick makes the whole process much less computationally expensive than that actual transformation to calculate the dot product and this is the essence of the kernel trick.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " '',\n",
       " '7. What is the role of the C hyper-parameter in SVM? Does it affect the bias/variance trade-off?',\n",
       " '👉 The balance between keeping the margins as large as possible and limiting the margin violation is controlled by the C parameter: a small value leads to a wider street but more margin violation and a higher value of C makes fewer margin violations but ends up with a smaller margin and overfitting.',\n",
       " '👉 Here thing becomes a little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make W as small as possible to increase the margin. This is where the role of the C hyperparameter comes in which allows us to define the trade-off between these two objectives.',\n",
       " '8. Explain different types of kernel functions.',\n",
       " 'A function is called kernel if there exist a function ϕ that maps a and b into another space such that K(a, b) = ϕ(a)T · ϕ(b). So you can use K as a kernel since you just know that a mapping ϕ exists, even if you don’t know what ϕ function is. These are the very good things about kernels.',\n",
       " 'Some of the kernel functions are as follows:',\n",
       " '👉 Polynomial Kernel:\\xa0These are the kernel functions that represent the similarity of vectors in a feature space over polynomials of original variables.',\n",
       " '👉 Gaussian Radial Basis Function (RBF) kernel:\\xa0 Gaussian RBF kernel maps each training instance to an infinite-dimensional space, therefore it’s a good thing that you don’t need to perform the mapping.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " '9. How you formulate SVM for a regression problem statement?',\n",
       " 'For formulating SVM as a regression problem statement we have to reverse the objective: instead of trying to fit the largest possible street between two classes which we will do for classification problem statements while limiting margin violations, now for SVM Regression, it tries to fit as many instances as possible between the margin while limiting the margin violations.',\n",
       " '',\n",
       " '10. What affects the decision boundary in SVM?',\n",
       " 'Adding more instances off the margin of the hyperplane does not affect the decision boundary, it is fully determined (or supported ) by the instances located at the edge of the street called support vectors',\n",
       " '11. What is a slack variable?',\n",
       " '👉 To meet the soft margin objective, we need to introduce a slack variable ε>=0 for each sample; it measures how much any particular instance is allowed to violate the margin.',\n",
       " '👉 Here thing becomes little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make w (weight matrix) as small as possible to increase the margin. This is where the role of the\\xa0C hyperparameter comes which allows us to define the trade-off between these two objectives.',\n",
       " '',\n",
       " 'Fig. Picture Showing the slack variables',\n",
       " 'Image Source: link',\n",
       " '12. What is a dual and primal problem and how is it relevant to SVMs?',\n",
       " '👉 Given a constrained optimization problem, known as the Primal problem, it is possible to express a different but closely related problem, which is known as its Dual problem.',\n",
       " '👉 The solution to the dual problem typically provides a lower bound to the solution of the primal problem, but under some conditions, it can be possible that it has even the same solutions as the primal problem.',\n",
       " '👉 Fortunately, the SVM problem completes these conditions, so that you can choose to solve the primal problem or the dual problem; and they both will have the same solution.',\n",
       " '\\xa0',\n",
       " '13. Can an SVM classifier outputs a confidence score when it classifies an instance? What about a probability?',\n",
       " '👉 An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities.',\n",
       " '👉 But if you set probability=True when building a model of SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s scores. By using this techniques, we can add the predict_proba() and predict_log_proba() methods to the SVM model.',\n",
       " '14. If you train an SVM classifier with an RBF kernel. It seems to underfit the training dataset: should you increase or decrease the\\xa0hyper-parameter\\xa0γ (gamma)? What about the C\\xa0hyper-parameter?',\n",
       " 'If we trained an SVM classifier using a Radial Basis Function (RBF) kernel, then it underfits the training set, so there might be too much regularization. To decrease it, you need to increase the gamma or C hyper-parameter.',\n",
       " '15. Is SVM sensitive to the Feature Scaling?',\n",
       " 'Yes, SVMs are sensitive to feature scaling as it takes input data to find the margins around hyperplanes and gets biased for the variance in high values.',\n",
       " 'Thanks for reading!',\n",
       " 'I hope you enjoyed the questions and were able to test your knowledge about Support Vector Machines (SVM).\\n',\n",
       " 'If you liked this and want to know more, go visit my other articles on Data Science and Machine Learning by clicking on the Link',\n",
       " 'Please feel free to contact me on Linkedin, Email.',\n",
       " 'Something not mentioned or want to share your thoughts? Feel free to comment below And I’ll get back to you.',\n",
       " 'About the author',\n",
       " 'Currently, I pursuing my Bachelor of Technology (B.Tech) in Computer Science and Engineering from the Indian Institute of Technology Jodhpur(IITJ). I am very enthusiastic about Machine learning, Deep Learning, and Artificial Intelligence.',\n",
       " 'The media shown in this article are not owned by Analytics Vidhya and is used at the Author’s discretion.\\xa0',\n",
       " 'About the Author',\n",
       " '',\n",
       " 'Our Top Authors',\n",
       " '\\nDownload\\nAnalytics Vidhya App for the Latest blog/Article\\n',\n",
       " 'Leave a Reply Your email address will not be published. Required fields are marked *',\n",
       " ' Notify me of follow-up comments by email.',\n",
       " ' Notify me of new posts by email.',\n",
       " '',\n",
       " '',\n",
       " 'Basic Concepts of Object-Oriented Programming in Python',\n",
       " 'Python Tutorial: Working with CSV file for Data Science',\n",
       " 'Commonly used Machine Learning Algorithms (with Python and R Codes)',\n",
       " '3 Interesting Python Projects With Code for Beginners!',\n",
       " '© Copyright 2013-2021 Analytics Vidhya.']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SVM\n",
    "lst17= []\n",
    "url = \"https://www.analyticsvidhya.com/blog/2021/05/top-15-questions-to-test-your-data-science-skills-on-svm/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([\"h3\", \"p\"])\n",
    "for answer in answers:\n",
    "    lst17.append(answer.text)\n",
    "lst17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. What are Support Vector Machines (SVMs)?',\n",
       " '👉 SVM is a supervised machine learning algorithm that works on both classification and regression problem statements.',\n",
       " '👉 For classification problem statements, it tries to differentiate data points of different classes by finding a hyperplane that maximizes the margin between the classes in the training data.',\n",
       " '👉 In simple words, SVM tries to choose the hyperplane which separates the data points as widely as possible since this margin maximization improves the model’s accuracy on the test or the unseen data.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " '2. What are Support Vectors in SVMs?',\n",
       " '👉 Support vectors are those instances that are located on the margin itself. For SVMS, the decision boundary is entirely determined by using only the support vectors.',\n",
       " '👉 Any instance that is not a support vector (not on the margin boundaries) has no influence whatsoever; you could remove them or add more instances, or move them around, and as long as they stay off the margin they won’t affect the decision boundary.',\n",
       " '👉 For computing the predictions, only the support vectors are involved, not the whole training set.',\n",
       " '3. What is the basic principle of a Support Vector Machine?',\n",
       " 'It’s aimed at finding an optimal hyperplane that is linearly separable, and for the dataset which is not directly linearly separable, it extends its formulation by transforming the original data to map into a new space, which is also called kernel trick.',\n",
       " '4. What are hard margin and soft Margin SVMs?',\n",
       " '👉 \\xa0Hard margin SVMs work only if the data is linearly separable and these types of SVMs are quite sensitive to the outliers.',\n",
       " '👉 \\xa0But our main objective is to find a good balance between keeping the margins as large as possible and limiting the margin violation i.e. instances that end up in the middle of margin or even on the wrong side, and this method is called soft margin SVM.',\n",
       " '5. What do you mean by Hinge loss?',\n",
       " 'The function defined by max(0, 1 – t) is called the hinge loss function.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " 'Properties of Hinge loss function:',\n",
       " '👉 It is equal to 0 when the value of t is greater than or equal to 1 i.e, t>=1.',\n",
       " '👉 Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1.',\n",
       " '👉 It is not differentiable at t = 1.',\n",
       " '👉 It penalizes the model for wrongly classifying the instances and increases as far the instance is classified from the correct region of classification.',\n",
       " '6. What is the “Kernel trick”?',\n",
       " '👉 A Kernel is a function capable of computing the dot product of instances mapped in higher dimension space without actually transforming all the instances into the higher feature space and calculating the dot product.',\n",
       " '👉 This trick makes the whole process much less computationally expensive than that actual transformation to calculate the dot product and this is the essence of the kernel trick.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " '',\n",
       " '7. What is the role of the C hyper-parameter in SVM? Does it affect the bias/variance trade-off?',\n",
       " '👉 The balance between keeping the margins as large as possible and limiting the margin violation is controlled by the C parameter: a small value leads to a wider street but more margin violation and a higher value of C makes fewer margin violations but ends up with a smaller margin and overfitting.',\n",
       " '👉 Here thing becomes a little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make W as small as possible to increase the margin. This is where the role of the C hyperparameter comes in which allows us to define the trade-off between these two objectives.',\n",
       " '8. Explain different types of kernel functions.',\n",
       " 'A function is called kernel if there exist a function ϕ that maps a and b into another space such that K(a, b) = ϕ(a)T · ϕ(b). So you can use K as a kernel since you just know that a mapping ϕ exists, even if you don’t know what ϕ function is. These are the very good things about kernels.',\n",
       " 'Some of the kernel functions are as follows:',\n",
       " '👉 Polynomial Kernel:\\xa0These are the kernel functions that represent the similarity of vectors in a feature space over polynomials of original variables.',\n",
       " '👉 Gaussian Radial Basis Function (RBF) kernel:\\xa0 Gaussian RBF kernel maps each training instance to an infinite-dimensional space, therefore it’s a good thing that you don’t need to perform the mapping.',\n",
       " '',\n",
       " 'Image Source: link',\n",
       " '9. How you formulate SVM for a regression problem statement?',\n",
       " 'For formulating SVM as a regression problem statement we have to reverse the objective: instead of trying to fit the largest possible street between two classes which we will do for classification problem statements while limiting margin violations, now for SVM Regression, it tries to fit as many instances as possible between the margin while limiting the margin violations.',\n",
       " '',\n",
       " '10. What affects the decision boundary in SVM?',\n",
       " 'Adding more instances off the margin of the hyperplane does not affect the decision boundary, it is fully determined (or supported ) by the instances located at the edge of the street called support vectors',\n",
       " '11. What is a slack variable?',\n",
       " '👉 To meet the soft margin objective, we need to introduce a slack variable ε>=0 for each sample; it measures how much any particular instance is allowed to violate the margin.',\n",
       " '👉 Here thing becomes little complex as we have conflicting objectives of making the slack variables as small as possible to reduce margin violation and make w (weight matrix) as small as possible to increase the margin. This is where the role of the\\xa0C hyperparameter comes which allows us to define the trade-off between these two objectives.',\n",
       " '',\n",
       " 'Fig. Picture Showing the slack variables',\n",
       " 'Image Source: link',\n",
       " '12. What is a dual and primal problem and how is it relevant to SVMs?',\n",
       " '👉 Given a constrained optimization problem, known as the Primal problem, it is possible to express a different but closely related problem, which is known as its Dual problem.',\n",
       " '👉 The solution to the dual problem typically provides a lower bound to the solution of the primal problem, but under some conditions, it can be possible that it has even the same solutions as the primal problem.',\n",
       " '👉 Fortunately, the SVM problem completes these conditions, so that you can choose to solve the primal problem or the dual problem; and they both will have the same solution.',\n",
       " '\\xa0',\n",
       " '13. Can an SVM classifier outputs a confidence score when it classifies an instance? What about a probability?',\n",
       " '👉 An SVM classifier can give the distance between the test instance and the decision boundary as output, so we can use that as a confidence score, but we cannot use this score to directly converted it into class probabilities.',\n",
       " '👉 But if you set probability=True when building a model of SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM’s scores. By using this techniques, we can add the predict_proba() and predict_log_proba() methods to the SVM model.',\n",
       " '14. If you train an SVM classifier with an RBF kernel. It seems to underfit the training dataset: should you increase or decrease the\\xa0hyper-parameter\\xa0γ (gamma)? What about the C\\xa0hyper-parameter?',\n",
       " 'If we trained an SVM classifier using a Radial Basis Function (RBF) kernel, then it underfits the training set, so there might be too much regularization. To decrease it, you need to increase the gamma or C hyper-parameter.',\n",
       " '15. Is SVM sensitive to the Feature Scaling?',\n",
       " 'Yes, SVMs are sensitive to feature scaling as it takes input data to find the margins around hyperplanes and gets biased for the variance in high values.']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new17 = lst17[3:66]\n",
    "lst_new17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are Support Vector Machines (SVMs)?</td>\n",
       "      <td>👉 SVM is a supervised machine learning algorithm that works on both classification and regression problem statements.👉 For classification problem statements, it tries to differentiate data points of different classes by finding a hyperplane that maximizes the margin between the classes in the training data.👉 In simple words, SVM tries to choose the hyperplane which separates the data points as widely as possible since this margin maximization improves the model’s accuracy on the test or the unseen data.Image Source: link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are Support Vectors in SVMs?</td>\n",
       "      <td>👉 Support vectors are those instances that are located on the margin itself. For SVMS, the decision boundary is entirely determined by using only the support vectors.👉 Any instance that is not a support vector (not on the margin boundaries) has no influence whatsoever; you could remove them or add more instances, or move them around, and as long as they stay off the margin they won’t affect the decision boundary.👉 For computing the predictions, only the support vectors are involved, not the whole training set.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the basic principle of a Support Vector Machine?</td>\n",
       "      <td>It’s aimed at finding an optimal hyperplane that is linearly separable, and for the dataset which is not directly linearly separable, it extends its formulation by transforming the original data to map into a new space, which is also called kernel trick.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are hard margin and soft Margin SVMs?</td>\n",
       "      <td>👉  Hard margin SVMs work only if the data is linearly separable and these types of SVMs are quite sensitive to the outliers.👉  But our main objective is to find a good balance between keeping the margins as large as possible and limiting the margin violation i.e. instances that end up in the middle of margin or even on the wrong side, and this method is called soft margin SVM.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What do you mean by Hinge loss?</td>\n",
       "      <td>The function defined by max(0, 1 – t) is called the hinge loss function.Image Source: linkProperties of Hinge loss function:👉 It is equal to 0 when the value of t is greater than or equal to 1 i.e, t&gt;=1.👉 Its derivative (slope) is equal to –1 if t &lt; 1 and 0 if t &gt; 1.👉 It is not differentiable at t = 1.👉 It penalizes the model for wrongly classifying the instances and increases as far the instance is classified from the correct region of classification.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Questions  \\\n",
       "0                    What are Support Vector Machines (SVMs)?   \n",
       "1                           What are Support Vectors in SVMs?   \n",
       "2    What is the basic principle of a Support Vector Machine?   \n",
       "3                  What are hard margin and soft Margin SVMs?   \n",
       "4                             What do you mean by Hinge loss?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Answer  \n",
       "0  👉 SVM is a supervised machine learning algorithm that works on both classification and regression problem statements.👉 For classification problem statements, it tries to differentiate data points of different classes by finding a hyperplane that maximizes the margin between the classes in the training data.👉 In simple words, SVM tries to choose the hyperplane which separates the data points as widely as possible since this margin maximization improves the model’s accuracy on the test or the unseen data.Image Source: link  \n",
       "1             👉 Support vectors are those instances that are located on the margin itself. For SVMS, the decision boundary is entirely determined by using only the support vectors.👉 Any instance that is not a support vector (not on the margin boundaries) has no influence whatsoever; you could remove them or add more instances, or move them around, and as long as they stay off the margin they won’t affect the decision boundary.👉 For computing the predictions, only the support vectors are involved, not the whole training set.  \n",
       "2                                                                                                                                                                                                                                                                                  It’s aimed at finding an optimal hyperplane that is linearly separable, and for the dataset which is not directly linearly separable, it extends its formulation by transforming the original data to map into a new space, which is also called kernel trick.  \n",
       "3                                                                                                                                                     👉  Hard margin SVMs work only if the data is linearly separable and these types of SVMs are quite sensitive to the outliers.👉  But our main objective is to find a good balance between keeping the margins as large as possible and limiting the margin violation i.e. instances that end up in the middle of margin or even on the wrong side, and this method is called soft margin SVM.  \n",
       "4                                                                        The function defined by max(0, 1 – t) is called the hinge loss function.Image Source: linkProperties of Hinge loss function:👉 It is equal to 0 when the value of t is greater than or equal to 1 i.e, t>=1.👉 Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1.👉 It is not differentiable at t = 1.👉 It penalizes the model for wrongly classifying the instances and increases as far the instance is classified from the correct region of classification.  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\..+\\?\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new17:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df17=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df17.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df17[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign in',\n",
       " 'Alekhyo Banerjee',\n",
       " 'Aug 31, 2020·8 min read',\n",
       " 'Support Vector Machine” (SVM) is a supervised machine learning algorithm which can be used for both classification or regression problems.',\n",
       " 'SVMs are particularly well suited for classification of complex but small- or medium-sized datasets.',\n",
       " 'We’ll talk about some Interview questions related to SVMs.',\n",
       " 'Explanation: Suppose you have to construct a bidirectional road. Now you have to make a dividing line. The optimal approach would be to make margins on the sides and draw an equidistant line from both the margins.',\n",
       " 'This is exactly how SVM tries to classify points by finding an optimal centre line (technically called as hyperplane).',\n",
       " '2. Can you explain SVM?',\n",
       " 'Explanation: Support vector machines is a supervised machine learning algorithm which works both on classification and regression problems. It tries to classify data by finding a hyperplane that maximizes the margin between the classes in the training data. Hence, SVM is an example of a large margin classifier.',\n",
       " 'The basic idea of support vector machines:',\n",
       " '3. What is the geometric intuition behind SVM?',\n",
       " 'Explanation: If you are asked to classify two different classes. There can be multiple hyperplanes which can be drawn.',\n",
       " 'SVM chooses the hyperplane which separates the data points as widely as possible. SVM draws a hyperplane parallel to the actual hyperplane intersecting with the first point of class A (also known as Support Vectors) and another hyperplane parallel to the actual hyperplane intersecting with the first point of class B. SVM tries to maximize these margins. Eventually, this margin maximization improves the model’s accuracy on unseen data.',\n",
       " '4. How would explain Convex Hull in light of SVMs?',\n",
       " 'Explanation: We simply build a convex hull for class A and class B and draw a perpendicular on the shortest distance between the closest points of both these hulls.',\n",
       " '5. What do know about Hard Margin SVM and Soft Margin SVM?',\n",
       " 'Explanation: If a point Xi satisfies the equation Yi(WT*Xi +b) ≥ 1, then Xi is correctly classified else incorrectly classified. So we can see that if the points are linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called hard margin SVM (since we have very strict constraints to correctly classify each and every data point).',\n",
       " 'To overcome this, we introduce a term ( ξ ) (pronounced as Zeta)',\n",
       " 'if ξi= 0, the points can be considered as correctly classified.',\n",
       " 'if ξi> 0 , Incorrectly classified points.',\n",
       " '6. What is Hinge Loss?',\n",
       " 'Explanation: Hinge Loss is a loss function which penalises the SVM model for inaccurate predictions.',\n",
       " 'If Yi(WT*Xi +b) ≥ 1, hinge loss is ‘0’ i.e the points are correctly classified. When',\n",
       " 'Yi(WT*Xi +b) < 1, then hinge loss increases massively.',\n",
       " 'As Yi(WT*Xi +b) increases with every misclassified point, the upper bound of hinge loss {1- Yi(WT*Xi +b)} also increases exponentially.',\n",
       " 'Hence, the points that are farther away from the decision margins have a greater loss value, thus penalising those points.',\n",
       " 'We can formulate hinge loss as max[0, 1- Yi(WT*Xi +b)]',\n",
       " '7. Explain the Dual form of SVM formulation?',\n",
       " 'Explanation: The aim of the Soft Margin formulation is to minimize',\n",
       " 'subject to',\n",
       " 'This is also known as the primal form of SVM.',\n",
       " 'The duality theory provides a convenient way to deal with the constraints. The dual optimization problem can be written in terms of dot products, thereby making it possible to use kernel functions.',\n",
       " 'It is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions, it can even have the same solutions as the primal problem. Luckily, the SVM problem happens to meet these conditions, so you can choose to solve the primal problem or the dual problem; both will have the same solution.',\n",
       " '8. What’s the “kernel trick” and how is it useful?',\n",
       " 'Explanation: Earlier we have discussed applying SVM on linearly separable data but it is very rare to get such data. Here, kernel trick plays a huge role. The idea is to map the non-linear separable data-set into a higher dimensional space where we can find a hyperplane that can separate the samples.',\n",
       " 'It reduces the complexity of finding the mapping function. So, Kernel function defines the inner product in the transformed space. Application of the kernel trick is not limited to the SVM algorithm. Any computations involving the dot products (x, y) can utilize the kernel trick.',\n",
       " '9. What is Polynomial kernel?',\n",
       " 'Explanation: Polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.',\n",
       " 'For d-degree polynomials, the polynomial kernel is defined as:',\n",
       " '10. What is RBF-Kernel?',\n",
       " 'Explanation:',\n",
       " 'The RBF kernel on two samples x and x’, represented as feature vectors in some input space, is defined as',\n",
       " '||x-x’||² recognized as the squared Euclidean distance between the two feature vectors. sigma is a free parameter.',\n",
       " '11. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?',\n",
       " 'Explanation: This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m² and m³. So, if there are millions of instances, you should use the primal form, because the dual form will be much too slow.',\n",
       " '12. Explain about SVM Regression?',\n",
       " 'Explanation: The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because the output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM',\n",
       " '13. Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm.',\n",
       " 'Explanation:',\n",
       " '14. What is the role of C in SVM? How does it affect the bias/variance trade-off?',\n",
       " 'Explanation:',\n",
       " 'In the given Soft Margin Formulation of SVM, C is a hyperparameter.',\n",
       " 'C hyperparameter adds a penalty for each misclassified data point.',\n",
       " 'Large Value of parameter C implies a small margin, there is a tendency to overfit the training model.',\n",
       " 'Small Value of parameter C implies a large margin which might lead to underfitting of the model.',\n",
       " '15. SVM being a large margin classifier, is it influenced by outliers?',\n",
       " 'Explanation: Yes, if C is large, otherwise not.',\n",
       " '16. In SVM, what is the angle between the decision boundary and theta?',\n",
       " 'Explanation: Decision boundary is a plane having equation Theta1*x1+Theta2*x2+……+c = 0, so as per the property of a plane, it’s coefficients vector is normal to the plane. Hence, the Theta vector is perpendicular to the decision boundary.',\n",
       " '17. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Explanation:',\n",
       " '18. What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'Explanation: They differ only in the implementation . SVM is much more efficient and has good optimization packages.',\n",
       " '19. Can any similarity function be used for SVM?',\n",
       " 'Explanation: No. It has to have to satisfy Mercer’s theorem.',\n",
       " '20. Does SVM give any probabilistic output?',\n",
       " 'Explanation: SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation',\n",
       " 'Reference:',\n",
       " 'Data Science| Data Analysis| Data Visualisation| OOP|Python|C Second-Year Undergraduate in Computer Science and Engineering at RCCIIT,Kolkata',\n",
       " '102 ',\n",
       " '1',\n",
       " '102\\xa0',\n",
       " '102 ',\n",
       " '1',\n",
       " 'Data Science| Data Analysis| Data Visualisation| OOP|Python|C Second-Year Undergraduate in Computer Science and Engineering at RCCIIT,Kolkata']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst18= []\n",
    "url = \"https://alekhyo.medium.com/interview-questions-on-svm-bf13e5fbcca8\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([{\"class\":\"hv da\"},\"p\"])\n",
    "for answer in answers:\n",
    "    lst18.append(answer.text)\n",
    "lst18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2. Can you explain SVM?',\n",
       " 'Explanation: Support vector machines is a supervised machine learning algorithm which works both on classification and regression problems. It tries to classify data by finding a hyperplane that maximizes the margin between the classes in the training data. Hence, SVM is an example of a large margin classifier.',\n",
       " 'The basic idea of support vector machines:',\n",
       " '3. What is the geometric intuition behind SVM?',\n",
       " 'Explanation: If you are asked to classify two different classes. There can be multiple hyperplanes which can be drawn.',\n",
       " 'SVM chooses the hyperplane which separates the data points as widely as possible. SVM draws a hyperplane parallel to the actual hyperplane intersecting with the first point of class A (also known as Support Vectors) and another hyperplane parallel to the actual hyperplane intersecting with the first point of class B. SVM tries to maximize these margins. Eventually, this margin maximization improves the model’s accuracy on unseen data.',\n",
       " '4. How would explain Convex Hull in light of SVMs?',\n",
       " 'Explanation: We simply build a convex hull for class A and class B and draw a perpendicular on the shortest distance between the closest points of both these hulls.',\n",
       " '5. What do know about Hard Margin SVM and Soft Margin SVM?',\n",
       " 'Explanation: If a point Xi satisfies the equation Yi(WT*Xi +b) ≥ 1, then Xi is correctly classified else incorrectly classified. So we can see that if the points are linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called hard margin SVM (since we have very strict constraints to correctly classify each and every data point).',\n",
       " 'To overcome this, we introduce a term ( ξ ) (pronounced as Zeta)',\n",
       " 'if ξi= 0, the points can be considered as correctly classified.',\n",
       " 'if ξi> 0 , Incorrectly classified points.',\n",
       " '6. What is Hinge Loss?',\n",
       " 'Explanation: Hinge Loss is a loss function which penalises the SVM model for inaccurate predictions.',\n",
       " 'If Yi(WT*Xi +b) ≥ 1, hinge loss is ‘0’ i.e the points are correctly classified. When',\n",
       " 'Yi(WT*Xi +b) < 1, then hinge loss increases massively.',\n",
       " 'As Yi(WT*Xi +b) increases with every misclassified point, the upper bound of hinge loss {1- Yi(WT*Xi +b)} also increases exponentially.',\n",
       " 'Hence, the points that are farther away from the decision margins have a greater loss value, thus penalising those points.',\n",
       " 'We can formulate hinge loss as max[0, 1- Yi(WT*Xi +b)]',\n",
       " '7. Explain the Dual form of SVM formulation?',\n",
       " 'Explanation: The aim of the Soft Margin formulation is to minimize',\n",
       " 'subject to',\n",
       " 'This is also known as the primal form of SVM.',\n",
       " 'The duality theory provides a convenient way to deal with the constraints. The dual optimization problem can be written in terms of dot products, thereby making it possible to use kernel functions.',\n",
       " 'It is possible to express a different but closely related problem, called its dual problem. The solution to the dual problem typically gives a lower bound to the solution of the primal problem, but under some conditions, it can even have the same solutions as the primal problem. Luckily, the SVM problem happens to meet these conditions, so you can choose to solve the primal problem or the dual problem; both will have the same solution.',\n",
       " '8. What’s the “kernel trick” and how is it useful?',\n",
       " 'Explanation: Earlier we have discussed applying SVM on linearly separable data but it is very rare to get such data. Here, kernel trick plays a huge role. The idea is to map the non-linear separable data-set into a higher dimensional space where we can find a hyperplane that can separate the samples.',\n",
       " 'It reduces the complexity of finding the mapping function. So, Kernel function defines the inner product in the transformed space. Application of the kernel trick is not limited to the SVM algorithm. Any computations involving the dot products (x, y) can utilize the kernel trick.',\n",
       " '9. What is Polynomial kernel?',\n",
       " 'Explanation: Polynomial kernel is a kernel function commonly used with support vector machines (SVMs) and other kernelized models, that represents the similarity of vectors (training samples) in a feature space over polynomials of the original variables, allowing learning of non-linear models.',\n",
       " 'For d-degree polynomials, the polynomial kernel is defined as:',\n",
       " '10. What is RBF-Kernel?',\n",
       " 'Explanation:',\n",
       " 'The RBF kernel on two samples x and x’, represented as feature vectors in some input space, is defined as',\n",
       " '||x-x’||² recognized as the squared Euclidean distance between the two feature vectors. sigma is a free parameter.',\n",
       " '11. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?',\n",
       " 'Explanation: This question applies only to linear SVMs since kernelized can only use the dual form. The computational complexity of the primal form of the SVM problem is proportional to the number of training instances m, while the computational complexity of the dual form is proportional to a number between m² and m³. So, if there are millions of instances, you should use the primal form, because the dual form will be much too slow.',\n",
       " '12. Explain about SVM Regression?',\n",
       " 'Explanation: The Support Vector Regression (SVR) uses the same principles as the SVM for classification, with only a few minor differences. First of all, because the output is a real number it becomes very difficult to predict the information at hand, which has infinite possibilities. In the case of regression, a margin of tolerance (epsilon) is set in approximation to the SVM',\n",
       " '13. Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm.',\n",
       " 'Explanation:',\n",
       " '14. What is the role of C in SVM? How does it affect the bias/variance trade-off?',\n",
       " 'Explanation:',\n",
       " 'In the given Soft Margin Formulation of SVM, C is a hyperparameter.',\n",
       " 'C hyperparameter adds a penalty for each misclassified data point.',\n",
       " 'Large Value of parameter C implies a small margin, there is a tendency to overfit the training model.',\n",
       " 'Small Value of parameter C implies a large margin which might lead to underfitting of the model.',\n",
       " '15. SVM being a large margin classifier, is it influenced by outliers?',\n",
       " 'Explanation: Yes, if C is large, otherwise not.',\n",
       " '16. In SVM, what is the angle between the decision boundary and theta?',\n",
       " 'Explanation: Decision boundary is a plane having equation Theta1*x1+Theta2*x2+……+c = 0, so as per the property of a plane, it’s coefficients vector is normal to the plane. Hence, the Theta vector is perpendicular to the decision boundary.',\n",
       " '17. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Explanation:',\n",
       " '18. What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'Explanation: They differ only in the implementation . SVM is much more efficient and has good optimization packages.',\n",
       " '19. Can any similarity function be used for SVM?',\n",
       " 'Explanation: No. It has to have to satisfy Mercer’s theorem.',\n",
       " '20. Does SVM give any probabilistic output?',\n",
       " 'Explanation: SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new18 = lst18[8:68]\n",
    "lst_new18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you explain SVM?</td>\n",
       "      <td>Explanation: Support vector machines is a supervised machine learning algorithm which works both on classification and regression problems. It tries to classify data by finding a hyperplane that maximizes the margin between the classes in the training data. Hence, SVM is an example of a large margin classifier.The basic idea of support vector machines:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the geometric intuition behind SVM?</td>\n",
       "      <td>Explanation: If you are asked to classify two different classes. There can be multiple hyperplanes which can be drawn.SVM chooses the hyperplane which separates the data points as widely as possible. SVM draws a hyperplane parallel to the actual hyperplane intersecting with the first point of class A (also known as Support Vectors) and another hyperplane parallel to the actual hyperplane intersecting with the first point of class B. SVM tries to maximize these margins. Eventually, this margin maximization improves the model’s accuracy on unseen data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How would explain Convex Hull in light of SVMs?</td>\n",
       "      <td>Explanation: We simply build a convex hull for class A and class B and draw a perpendicular on the shortest distance between the closest points of both these hulls.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What do know about Hard Margin SVM and Soft Margin SVM?</td>\n",
       "      <td>Explanation: If a point Xi satisfies the equation Yi(WT*Xi +b) ≥ 1, then Xi is correctly classified else incorrectly classified. So we can see that if the points are linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called hard margin SVM (since we have very strict constraints to correctly classify each and every data point).To overcome this, we introduce a term ( ξ ) (pronounced as Zeta)if ξi= 0, the points can be considered as correctly classified.if ξi&gt; 0 , Incorrectly classified points.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Hinge Loss?</td>\n",
       "      <td>Explanation: Hinge Loss is a loss function which penalises the SVM model for inaccurate predictions.If Yi(WT*Xi +b) ≥ 1, hinge loss is ‘0’ i.e the points are correctly classified. WhenYi(WT*Xi +b) &lt; 1, then hinge loss increases massively.As Yi(WT*Xi +b) increases with every misclassified point, the upper bound of hinge loss {1- Yi(WT*Xi +b)} also increases exponentially.Hence, the points that are farther away from the decision margins have a greater loss value, thus penalising those points.We can formulate hinge loss as max[0, 1- Yi(WT*Xi +b)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Questions  \\\n",
       "0                                       Can you explain SVM?   \n",
       "1                What is the geometric intuition behind SVM?   \n",
       "2            How would explain Convex Hull in light of SVMs?   \n",
       "3    What do know about Hard Margin SVM and Soft Margin SVM?   \n",
       "4                                        What is Hinge Loss?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Answer  \n",
       "0                                                                                                                                                                                                                                                                            Explanation: Support vector machines is a supervised machine learning algorithm which works both on classification and regression problems. It tries to classify data by finding a hyperplane that maximizes the margin between the classes in the training data. Hence, SVM is an example of a large margin classifier.The basic idea of support vector machines:  \n",
       "1                                                                  Explanation: If you are asked to classify two different classes. There can be multiple hyperplanes which can be drawn.SVM chooses the hyperplane which separates the data points as widely as possible. SVM draws a hyperplane parallel to the actual hyperplane intersecting with the first point of class A (also known as Support Vectors) and another hyperplane parallel to the actual hyperplane intersecting with the first point of class B. SVM tries to maximize these margins. Eventually, this margin maximization improves the model’s accuracy on unseen data.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Explanation: We simply build a convex hull for class A and class B and draw a perpendicular on the shortest distance between the closest points of both these hulls.  \n",
       "3  Explanation: If a point Xi satisfies the equation Yi(WT*Xi +b) ≥ 1, then Xi is correctly classified else incorrectly classified. So we can see that if the points are linearly separable then only our hyperplane is able to distinguish between them and if any outlier is introduced then it is not able to separate them. So these type of SVM is called hard margin SVM (since we have very strict constraints to correctly classify each and every data point).To overcome this, we introduce a term ( ξ ) (pronounced as Zeta)if ξi= 0, the points can be considered as correctly classified.if ξi> 0 , Incorrectly classified points.  \n",
       "4                                                                         Explanation: Hinge Loss is a loss function which penalises the SVM model for inaccurate predictions.If Yi(WT*Xi +b) ≥ 1, hinge loss is ‘0’ i.e the points are correctly classified. WhenYi(WT*Xi +b) < 1, then hinge loss increases massively.As Yi(WT*Xi +b) increases with every misclassified point, the upper bound of hinge loss {1- Yi(WT*Xi +b)} also increases exponentially.Hence, the points that are farther away from the decision margins have a greater loss value, thus penalising those points.We can formulate hinge loss as max[0, 1- Yi(WT*Xi +b)]  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\..+\\?\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new18:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df18=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df18.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df18[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df18=df18.drop(df18.index[[14]],axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df18[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sign in',\n",
       " 'Manish Sharma',\n",
       " 'May 16·7 min read',\n",
       " 'Recently I got an opportunity to prepare a lecture on SVM for a group that had a mix of freshers and experienced people in data science. I prepared a lecture that involves basics and even an intuition for the derivation of SVM (vector method) and also some questions along with answers in the discussion section. I hope the lecture can benefit a larger audience. Do let me know in the comment section what more would you expect in a lecture on an ML algorithm in the future.',\n",
       " 'SVM is',\n",
       " 'Supervised Machine Learning AlgorithmUsed for Classification and Regression',\n",
       " 'The real strength of SVM lies in the classification and to be the specific classification of vectors with high dimensionality (data points with a large number of features). Although the support vector regressor (SVR) also uses the same intuition and concept as of support vector classifier but for regression linear or random forest regressor are also strong competitors to SVR. We would focus on classification using a support vector classifier and not on regression.',\n",
       " 'Optimization depends upon the dot product of the pairs of vectors',\n",
       " 'Derivation (proof of the fact that optimization in SVM classifier depends only on the product of pairs of vectors).',\n",
       " 'Assume a binary classification problem as shown in figure 1, in order to separate the two classes in 2D a decision boundary needs to be decided. If the boundary separating the classes is linear the classes are said to be linearly separable. Figure 1 shows the vectors (data points) for two different classes + and negative.',\n",
       " 'Multiple lines can act as the decision boundary which separates the two classes (look at figure 2). Any of the blue, yellow or green line may act as a decision boundary as all three of them separates the data. How shall we decide which of these boundaries separates the data best? This decision would give us a condition using which we can reject all but one decision boundary among all the possibilities.',\n",
       " 'To define the best decision boundary we use the principle of maximum margin. The boundary which is at the maximum distance from the nearest members of both the class is selected as the best boundary. In figure 3, shown below the hashed lines are the lines passing through the nearest points of the + and — class. Distance from the yellow line to the hashed line is defined as the margin. Given that among all the possible lines separating the two classes if the yellow line has the widest separation between the margins then the yellow line is the best decision boundary. The points of both the classes through which the hashed yellow line passes is/are known as support vector(s). There can be a single support vector (as for + class) or there can be multiple support vectors (as for — class), the slope of the hashed line would be the same as the decision boundary.',\n",
       " 'In order to maximize the margins or distance between two hashed lines, we need to create an expression and take a derivative of it to find the optimum (extremum) value. Assume a vector ‘w’ which is normal to the decision boundary and an unknown vector ‘u’, as shown below in figure 4. To decide whether the unknown vector u lies on the side of — ive or + ive class we measure the length of the projection of u on w if it is greater than a certain length it is in +ive class or else it belongs to -ive class (or lies on the margin).',\n",
       " 'To differentiate between + and — class we assume that if the unknown vector is a vector from + class (x+ ) it should give a value greater than one or if it is a vector from — class (x− ) it should give a value less than -1. +1 and -1 value is decided to bring in the clear separation between both the classes. So mathematically the condition is:',\n",
       " 'For mathematical convenience let’s introduce a variable yi such that yi = 1 for + samples and yi = -1 for — samples. Multiplying yi with the equations above we get:',\n",
       " 'The support vector from + class (x+sv) and — class (x−sv) lies on the margin itself. So the projection of the vector (x+sv−x−sv) ⋅w/|w| on normal w is the distance between the margins. This distance should be maximized with the constraints given above.',\n",
       " 'Maximize:',\n",
       " 'From the above equations we can decipher:',\n",
       " 'thus the maximization condition becomes maximum of 2/|w| or we can say minimum of:',\n",
       " 'minimize:',\n",
       " 'constraint:',\n",
       " 'To find extremum along with a constraint we use Langrangian,',\n",
       " 'The minimum of Lagrangian is:',\n",
       " 'Here in the above expression, you can observe that the minimum value (our optimization) depends upon the dot product of pair of two vectors (data points).',\n",
       " 'In such cases adding extra features and transforming the vectors to higher dimensions might help in separating the vectors by a plane (hyper plane). For example, assume two classes c1(blue) and c2(orange) shown in figure 5 below. There seems to be no linear decision boundary for the two classes. What if a new dimension is added to the data x_2 = (x1)².',\n",
       " 'Non linearly separable data:',\n",
       " 'Transformed data which is now linearly separable (shown in figure 6):',\n",
       " 'Similarly, with 2-dimensional data containing x1 and x2 dimension new dimensions like (x1)², (x2)², (x1)3 e.t.c can be created and data can be transformed to a higher dimension to separate the classes.',\n",
       " 'We have some functions which can help to transform the data to higher dimensions. These functions are known as kernels.',\n",
       " 'LinearPolynomialGaussianRadial Basis Function (RBF)HyperbolicSigmoid',\n",
       " 'Generic question for any algorithm: ADA? (Advantages, Disadvantages and Assumptions)',\n",
       " 'Advantages:',\n",
       " 'Turnkey algorithm, very few parameters to optimize.Optimization is in convex space so a global minimum is reached unlike neural network it doesn’t suffer with problem of stucking at local minima.Memory efficientLinear SVM is more effective than other algorithm if the number of features are highe (>1000)',\n",
       " 'Disadvantages:',\n",
       " 'Feature scalingTends to get slower as number of examples increasesHard margin leads to overfitting',\n",
       " 'Assumptions:',\n",
       " 'None',\n",
       " '2. Explain hard and soft margin in context with SVM.',\n",
       " 'Hard Margin Does not allow any errorSoft Margin Allow error, it reduces the chance of overfitting',\n",
       " '3. Why does SVM gets slower when the number of examples increases?',\n",
       " 'The computation cost depends upon the selection of pair of vectors (nC2, as derived in the optimization section).',\n",
       " '4. What are the hyperparameters associated with SVM?',\n",
       " 'KernelC: Number of permitted errors. Large C gives a narrow margin and vice versa.',\n",
       " '5. Why the SVM algorithm is known to be memory efficient?',\n",
       " 'Not all the data points are required to make the decision, once the support vectors are decided only the support vectors and equation of the hyperplane is required to make the decision.',\n",
       " '6. State the statement is True or False: ‘SVM decision boundary is perpendicular bisector to the line joining the closest point of convex hull of the two classes’.',\n",
       " 'True statement. Geometrical explanation: make the convex hull of each of the classes, try to join the convex hulls using the closest point and the classification boundary will be the perpendicular bisector.',\n",
       " '7. Give a typical example where SVM will be used over RandomForest.',\n",
       " 'Text classification: SVM is an algorithm of choice for higher-dimensional space when the number of features is large.',\n",
       " '8. What is regularization? How is an SVM algorithm regularized? Is it L1 or L2 regularization in nature?',\n",
       " '‘C parameter’ is actually the regularization parameter. The C parameter is multiplied by the sum of errors thus the regularization, in general, is L1 in nature. But if the cost function is modified in such a way that the cost of SVM is C multiplied by the sum of squares of the error the regularization becomes L2 in nature.',\n",
       " 'Do look for other methods of derivation (linear algebra method)Mercer’s Theorem',\n",
       " 'U Waterloo | Unige Switzerland| IIT Bombay: A ML/AI enthusiast.',\n",
       " '36 ',\n",
       " '1',\n",
       " '36\\xa0',\n",
       " '36 ',\n",
       " '1',\n",
       " 'SvmSupport Vector MachinesKernelLinear Svm',\n",
       " 'Your home for data science. A Medium publication sharing concepts, ideas and codes.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst19= []\n",
    "url = \"https://towardsdatascience.com/support-vector-machine-svm-719e530a725f\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([\"ol\", \"ul\", \"p\"])\n",
    "for answer in answers:\n",
    "    lst19.append(answer.text)\n",
    "lst19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2. Explain hard and soft margin in context with SVM.',\n",
       " 'Hard Margin Does not allow any errorSoft Margin Allow error, it reduces the chance of overfitting',\n",
       " '3. Why does SVM gets slower when the number of examples increases?',\n",
       " 'The computation cost depends upon the selection of pair of vectors (nC2, as derived in the optimization section).',\n",
       " '4. What are the hyperparameters associated with SVM?',\n",
       " 'KernelC: Number of permitted errors. Large C gives a narrow margin and vice versa.',\n",
       " '5. Why the SVM algorithm is known to be memory efficient?',\n",
       " 'Not all the data points are required to make the decision, once the support vectors are decided only the support vectors and equation of the hyperplane is required to make the decision.',\n",
       " '6. State the statement is True or False: ‘SVM decision boundary is perpendicular bisector to the line joining the closest point of convex hull of the two classes’.',\n",
       " 'True statement. Geometrical explanation: make the convex hull of each of the classes, try to join the convex hulls using the closest point and the classification boundary will be the perpendicular bisector.',\n",
       " '7. Give a typical example where SVM will be used over RandomForest.',\n",
       " 'Text classification: SVM is an algorithm of choice for higher-dimensional space when the number of features is large.',\n",
       " '8. What is regularization? How is an SVM algorithm regularized? Is it L1 or L2 regularization in nature?',\n",
       " '‘C parameter’ is actually the regularization parameter. The C parameter is multiplied by the sum of errors thus the regularization, in general, is L1 in nature. But if the cost function is modified in such a way that the cost of SVM is C multiplied by the sum of squares of the error the regularization becomes L2 in nature.']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new19 = lst19[37:51]\n",
    "lst_new19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explain hard and soft margin in context with SVM.</td>\n",
       "      <td>Hard Margin Does not allow any errorSoft Margin Allow error, it reduces the chance of overfitting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why does SVM gets slower when the number of examples increases?</td>\n",
       "      <td>The computation cost depends upon the selection of pair of vectors (nC2, as derived in the optimization section).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the hyperparameters associated with SVM?</td>\n",
       "      <td>KernelC: Number of permitted errors. Large C gives a narrow margin and vice versa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why the SVM algorithm is known to be memory efficient?</td>\n",
       "      <td>Not all the data points are required to make the decision, once the support vectors are decided only the support vectors and equation of the hyperplane is required to make the decision.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>State the statement is True or False: ‘SVM decision boundary is perpendicular bisector to the line joining the closest point of convex hull of the two classes’.</td>\n",
       "      <td>True statement. Geometrical explanation: make the convex hull of each of the classes, try to join the convex hulls using the closest point and the classification boundary will be the perpendicular bisector.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                            Questions  \\\n",
       "0                                                                                                                   Explain hard and soft margin in context with SVM.   \n",
       "1                                                                                                     Why does SVM gets slower when the number of examples increases?   \n",
       "2                                                                                                                   What are the hyperparameters associated with SVM?   \n",
       "3                                                                                                              Why the SVM algorithm is known to be memory efficient?   \n",
       "4    State the statement is True or False: ‘SVM decision boundary is perpendicular bisector to the line joining the closest point of convex hull of the two classes’.   \n",
       "\n",
       "                                                                                                                                                                                                           Answer  \n",
       "0                                                                                                               Hard Margin Does not allow any errorSoft Margin Allow error, it reduces the chance of overfitting  \n",
       "1                                                                                               The computation cost depends upon the selection of pair of vectors (nC2, as derived in the optimization section).  \n",
       "2                                                                                                                              KernelC: Number of permitted errors. Large C gives a narrow margin and vice versa.  \n",
       "3                       Not all the data points are required to make the decision, once the support vectors are decided only the support vectors and equation of the hyperplane is required to make the decision.  \n",
       "4  True statement. Geometrical explanation: make the convex hull of each of the classes, try to join the convex hulls using the closest point and the classification boundary will be the perpendicular bisector.  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+\\..+[\\?|\\.]+\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new19:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df19=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df19.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df19[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AboutAITechBlockchainFinanceEconomicsStartupStart Here',\n",
       " 'Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?',\n",
       " 'Ans. The performance depends on many factors',\n",
       " 'the number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your model',\n",
       " 'In general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.',\n",
       " 'Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.',\n",
       " 'So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)',\n",
       " '2. Why SVM is an example of a large margin classifier?',\n",
       " 'Why SVM is an example of a large margin classifier?',\n",
       " 'SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.',\n",
       " '3. What is the role of C in SVM?',\n",
       " 'What is the role of C in SVM?',\n",
       " 'Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.',\n",
       " '4. What is the intuition of a large margin classifier?',\n",
       " 'What is the intuition of a large margin classifier?',\n",
       " 'Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.',\n",
       " 'So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.',\n",
       " 'If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.',\n",
       " '5. What is a kernel in SVM? Why do we use kernels in SVM?',\n",
       " 'What is a kernel in SVM? Why do we use kernels in SVM?',\n",
       " 'Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.',\n",
       " '6. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Ans.',\n",
       " 'Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy',\n",
       " '7. What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'Ans. Only in implementation, One is much more efficient and has good optimization packages',\n",
       " '8. What is the difference between logistic regression and SVM',\n",
       " 'What is the difference between logistic regression and SVM',\n",
       " 'Ans. Logistic regression assumes that the predictors aren’t sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If there’s a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.',\n",
       " 'On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.',\n",
       " 'Non-regularized logistic regression techniques don’t work well (in fact, the fitted coefficients diverge) when there’s a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and there’s no guarantee that you’ll get the best one. What you get is an extremely confident model with poor predictive power near the margin.',\n",
       " 'SVMs get you the best separating hyperplane, and they’re efficient in high dimensional spaces. They’re similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there’s noise in the data.',\n",
       " 'Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.',\n",
       " 'Logistic regression is great in a low number of dimensions and when the predictors don’t suffice to give more than a probabilistic estimate of the response. SVMs do better when there’s a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.',\n",
       " '9. Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?',\n",
       " 'Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?',\n",
       " 'Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.',\n",
       " 'For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.',\n",
       " 'For a higher gamma, the model will capture the shape of the dataset well.',\n",
       " '10. What is generalization error in terms of the SVM?',\n",
       " 'What is generalization error in terms of the SVM?',\n",
       " 'Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.',\n",
       " 'External Links',\n",
       " 'https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance',\n",
       " 'https://www.quora.com/Why-do-we-call-an-SVM-a-large-margin-classifier#',\n",
       " 'https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel',\n",
       " 'https://www.quora.com/What-is-the-intuition-behind-margin-in-SVM',\n",
       " 'https://www.quora.com/What-is-the-difference-between-Linear-SVMs-and-Logistic-Regression',\n",
       " 'https://www.analyticsvidhya.com/blog/2017/10/svm-skilltest/',\n",
       " 'https://stats.stackexchange.com/questions/43996/kernel-logistic-regression-vs-svm',\n",
       " 'I will add more in Future…….',\n",
       " 'empowerment through data, knowledge, and expertise.',\n",
       " '68 ',\n",
       " 'Machine LearningData ScienceArtificial IntelligenceSvmAlgorithms',\n",
       " '68\\xa0claps',\n",
       " '68 ',\n",
       " 'empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com',\n",
       " 'Written by',\n",
       " 'Artificial Intelligence Research & Development Engineer',\n",
       " 'empowerment through data, knowledge, and expertise. subscribe to DDIntel at https://ddintel.datadriveninvestor.com']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst20= []\n",
    "url = \"https://medium.datadriveninvestor.com/support-vector-machines-important-questions-a47224692495\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([\"strong\", {\"class\":\"ho in\"}, \"ul\", \"p\"])\n",
    "for answer in answers:\n",
    "    lst20.append(answer.text)\n",
    "lst20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?',\n",
       " 'Ans. The performance depends on many factors',\n",
       " 'the number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your model',\n",
       " 'In general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.',\n",
       " 'Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.',\n",
       " 'So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)',\n",
       " '2. Why SVM is an example of a large margin classifier?',\n",
       " 'Why SVM is an example of a large margin classifier?',\n",
       " 'SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.',\n",
       " '3. What is the role of C in SVM?',\n",
       " 'What is the role of C in SVM?',\n",
       " 'Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.',\n",
       " '4. What is the intuition of a large margin classifier?',\n",
       " 'What is the intuition of a large margin classifier?',\n",
       " 'Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.',\n",
       " 'So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.',\n",
       " 'If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.',\n",
       " '5. What is a kernel in SVM? Why do we use kernels in SVM?',\n",
       " 'What is a kernel in SVM? Why do we use kernels in SVM?',\n",
       " 'Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.',\n",
       " '6. Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Can we apply the kernel trick to logistic regression? Why is it not used in practice then?',\n",
       " 'Ans.',\n",
       " 'Looking at the above it almost feels like kernel logistic regression is what you should be using. However, there are certain advantages that SVMs enjoy',\n",
       " '7. What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'What is the difference between logistic regression and SVM without a kernel?',\n",
       " 'Ans. Only in implementation, One is much more efficient and has good optimization packages',\n",
       " '8. What is the difference between logistic regression and SVM',\n",
       " 'What is the difference between logistic regression and SVM',\n",
       " 'Ans. Logistic regression assumes that the predictors aren’t sufficient to determine the response variable, but determine a probability that is a logistic function of a linear combination of them. If there’s a lot of noise, logistic regression (usually fit with maximum-likelihood techniques) is a great technique.',\n",
       " 'On the other hand, there are problems where you have thousands of dimensions and the predictors do nearly-certainly determine the response, but in some hard-to-explicitly-program way. An example would be image recognition. If you have a grayscale image, 100 by 100 pixels, you have 10,000 dimensions already. With various basis transforms (kernel trick) you will be able to get a linear separator of the data.',\n",
       " 'Non-regularized logistic regression techniques don’t work well (in fact, the fitted coefficients diverge) when there’s a separating hyperplane, because the maximum likelihood is achieved by any separating plane, and there’s no guarantee that you’ll get the best one. What you get is an extremely confident model with poor predictive power near the margin.',\n",
       " 'SVMs get you the best separating hyperplane, and they’re efficient in high dimensional spaces. They’re similar to regularization in terms of trying to find the lowest-normed vector that separates the data, but with a margin condition that favors choosing a good hyperplane. A hard-margin SVM will find a hyperplane that separates all the data (if one exists) and fail if there is none; soft-margin SVMs (generally preferred) do better when there’s noise in the data.',\n",
       " 'Additionally, SVMs only consider points near the margin (support vectors). Logistic regression considers all the points in the data set. Which you prefer depends on your problem.',\n",
       " 'Logistic regression is great in a low number of dimensions and when the predictors don’t suffice to give more than a probabilistic estimate of the response. SVMs do better when there’s a higher number of dimensions, and especially on problems where the predictors do certainly (or near-certainly) determine the responses.',\n",
       " '9. Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?',\n",
       " 'Suppose you are using RBF kernel in SVM with high Gamma value. What does this signify?',\n",
       " 'Ans. The gamma parameter in SVM tuning signifies the influence of points either near or far away from the hyperplane.',\n",
       " 'For a low gamma, the model will be too constrained and include all points of the training dataset, without really capturing the shape.',\n",
       " 'For a higher gamma, the model will capture the shape of the dataset well.',\n",
       " '10. What is generalization error in terms of the SVM?',\n",
       " 'What is generalization error in terms of the SVM?',\n",
       " 'Ans. Generalisation error in statistics is generally the out-of-sample error which is the measure of how accurately a model can predict values for previously unseen data.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new20 = lst20[1:44]\n",
    "lst_new20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?</td>\n",
       "      <td>Ans. The performance depends on many factorsthe number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your modelIn general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why SVM is an example of a large margin classifier?</td>\n",
       "      <td>Why SVM is an example of a large margin classifier?SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the role of C in SVM?</td>\n",
       "      <td>What is the role of C in SVM?Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the intuition of a large margin classifier?</td>\n",
       "      <td>What is the intuition of a large margin classifier?Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is a kernel in SVM? Why do we use kernels in SVM?</td>\n",
       "      <td>What is a kernel in SVM? Why do we use kernels in SVM?Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       Questions  \\\n",
       "0  Give some situations where you will use an SVM over a RandomForest Machine Learning algorithm and vice-versa?   \n",
       "1                                                            Why SVM is an example of a large margin classifier?   \n",
       "2                                                                                  What is the role of C in SVM?   \n",
       "3                                                            What is the intuition of a large margin classifier?   \n",
       "4                                                         What is a kernel in SVM? Why do we use kernels in SVM?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Answer  \n",
       "0  Ans. The performance depends on many factorsthe number of training instancesthe distribution of the datalinear vs. non-linear problemsinput scale of the featuresthe chosen hyperparametershow you validate/evaluate your modelIn general, It is easier to train a well-performing Random Forest classifier since you have to worry less about hyperparameter optimization. Due to the nature Random Forests, you are less likely to overfit. You simply grow ntrees on n bootstrap samples of the training set on feature subspaces — using the majority vote, the estimate will be pretty robust.Using Support Vector Machines, you have “more things” to “worry” about such as choosing an appropriate kernel (poly, RBF, linear …), the regularization penalty, the regularization strength, kernel parameters such as the poly degree or gamma, and so forth.So, in sum, We can say that Random Forests are much more automated and thus “easier” to train compared to SVMs, but there are many examples in literature where SVMs outperform Random Forests and vice versa on different datasets. So, if you like to compare these two, make sure that you run a large enough grid search for the SVM and use nested cross-validation to reduce the performance estimation bias. (https://www.quora.com/What-makes-Random-Forest-outperform-the-support-vector-machine-SVM-and-the-euclidean-distance)  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Why SVM is an example of a large margin classifier?SVM is a type of classifier which classifies positive and negative examples, here blue and red data pointsAs shown in the image, the largest margin is found in order to avoid overfitting ie,.. the optimal hyperplane is at the maximum distance from the positive and negative examples(Equal distant from the boundary lines).To satisfy this constraint, and also to classify the data points accurately, the margin is maximised, that is why this is called the large margin classifier.  \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     What is the role of C in SVM?Ans. The C parameter tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable.  \n",
       "3                                                                                                                                                     What is the intuition of a large margin classifier?Ans. Let’s say you’ve found a hyperplane that completely separates the two classes in your training set. We expect that when new data comes along (i.e. your test set), the new data will look like your training data. Points that should be classified as one class or the other should lie near the points in your training data with the corresponding class. Now, if your hyperplane is oriented such that it is close to some of the points in your training set, there’s a good chance that the new data will lie on the wrong side of the hyperplane, even if the new points lie close to training examples of the correct class.So we say that we want to find the hyperplane with the maximum margin. That is, find a hyperplane that divides your data properly, but is also as far as possible from your data points. That way, when new data comes in, even if it is a little closer to the wrong class than the training points, it will still lie on the right side of the hyperplane.If your data is separable, then there are infinitely many hyperplanes that will separate it. SVM (and some other classifiers) optimizes for the one with the maximum margin, as described above.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                What is a kernel in SVM? Why do we use kernels in SVM?Ans. SVM algorithms use a set of mathematical functions that are defined as the kernel. The function of kernel is to take data as input and transform it into the required form. Different SVM algorithms use different types of kernel functions. These functions can be different types. For example linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. Introduce Kernel functions for sequence data, graphs, text, images, as well as vectors. The most used type of kernel function is RBF. Because it has localized and finite response along the entire x-axis. The kernel functions return the inner product between two points in a suitable feature space. Thus by defining a notion of similarity, with little computational cost even in very high-dimensional spaces.  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^G|\\d+\\.[\\w\\d\\s]+\\?*\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new20:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^\\d+\\.\",\" \",ques[i])\n",
    "    \n",
    "df20=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df20[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Introduced a few years ago by Tianqi Chen and his team of researchers at the University of Washington, eXtreme Gradient Boosting or XGBoost is a popular and efficient gradient boosting method. XGBoost is an optimised distributed gradient boosting library, which is highly efficient, flexible and portable.\\xa0',\n",
       " 'The method is used for supervised learning problems and has been widely applied by data scientists to get optimised results for various machine learning challenges. It implements ML algorithms under the Gradient Boosting framework and helps in solving data science problems in a fast and accurate manner.\\xa0',\n",
       " 'Register for our upcoming Masterclass>>',\n",
       " 'Here are the top ten interview questions on XGBoost that Data Scientists must know.',\n",
       " '1| Is XGBoost faster than random forest?',\n",
       " 'Solution: XGBoost is usually used to train gradient-boosted decision trees (GBDT) and other gradient boosted models. Random forests also use the same model representation and inference as gradient-boosted decision trees, but it is a different training algorithm. XGBoost can be used to train a standalone random forest. Also, random forest can be used as a base model for gradient boosting techniques.',\n",
       " 'Solution:',\n",
       " 'Further, random forest is an improvement over bagging that helps in reducing the variance. Random forest builds trees in parallel, while in boosting, trees are built sequentially. Meaning, each of the trees is grown using information from previously grown trees, unlike bagging, where multiple copies of original training data are created and fit separate decision tree on each. This is the reason why XGBoost generally performs better than random forest.\\xa0',\n",
       " 'Know more here.',\n",
       " '2| What are the advantages and disadvantages of XGBoost?',\n",
       " 'Advantages:',\n",
       " 'Advantages:',\n",
       " 'Disadvantages:',\n",
       " 'Disadvantages:',\n",
       " 'Know more here.',\n",
       " '3| How XGBoost Works?',\n",
       " 'Solution: When using gradient boosting for regression, where the weak learners are considered to be regression trees, each of the regression trees maps an input data point to one of its leaves that includes a continuous score. XGB minimises a regularised objective function that merges a convex loss function, which is based on the variation between the target outputs and the predicted outputs. The training then proceeds iteratively, adding new trees with the capability to predict the residuals as well as errors of prior trees that are then coupled with the previous trees to make the final prediction.\\xa0',\n",
       " 'Solution: ',\n",
       " 'Click here to learn the step by step process of how XGB works.',\n",
       " '4| What does the weight of XGB leaf nodes mean? How to calculate it?',\n",
       " 'Solution: The “leaf weight” can be said as the model’s predicted output associated with each leaf (exit) node. Here is an instance of how to calculate the weights of the leaf nodes in XGB-',\n",
       " 'Solution: ',\n",
       " 'Consider a test data point, where age=10 and gender=female.To get the prediction for the data point, the tree is traversed from the top to bottom, performing a series of tests. At each of the intermediate nodes, a feature is needed to compare against a threshold.\\xa0',\n",
       " 'Now, depending on the result of the comparison, one must proceed to either the left or right child node of the tree. In case of (10, female), the test “age < 15” is to be performed first and then proceed to the left branch, because “age < 15” is true. Then, the second test “gender = male?” is performed, which evaluates to false, so we proceed to the right branch. We end up at the Leaf 2, whose output (leaf weight) is 0.1.',\n",
       " 'Click here to know more in detail.',\n",
       " '5| What are the data pre-processing steps for XGB?',\n",
       " 'Solution: The data pre-processing steps for XGB include the following-',\n",
       " 'Solution:',\n",
       " 'Know more here.',\n",
       " '6| How does XGB calculate features?',\n",
       " 'Solution: XGB automatically provides the estimations of feature importance from a trained predictive model. After a boosting tree is constructed, it retrieves feature importance scores for each attribute. The feature importance contributes a score which indicates how much valuable each feature was in the construction of the boosted decision trees within the model.\\xa0\\xa0',\n",
       " 'Solution:',\n",
       " 'Also, in terms of accuracy, XGB models show better performance for the training phase and comparable performance for the testing phase when compared to SVM models. Besides accuracy, XGB has higher computation speed than SVM.\\xa0\\xa0',\n",
       " 'Know more here.',\n",
       " '7| Why does XGBoost perform better than SVM?',\n",
       " 'Solution: In case of missing values, XGB is internally designed to handle missing values. The missing values are interpreted in such a way that if there endures any trend in the missing values, it is captured by the model. Users are required to supply a different value than other observations and pass that as a parameter.\\xa0',\n",
       " 'Solution: ',\n",
       " 'XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future. On the other hand, Support Vector Machine (SVM) does not perform well with the missing data and it is always a better option to impute the missing values before running SVM.\\xa0',\n",
       " 'Know more here.',\n",
       " '8| Differences between XGBoost and LightGBM.',\n",
       " 'Solution: XGBoost and LightGBM are the packages belonging to the family of gradient boosting decision trees (GBDTs).\\xa0',\n",
       " 'Solution:',\n",
       " 'Know more here.',\n",
       " '9| How does XGB handle missing values?',\n",
       " 'Solution: XGBoost supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. It is important to note that the gblinear booster treats missing values as zeros. During the training time XGB decides whether the missing values should fall into the right node or left node. This decision is taken to minimise the loss. If there are no missing values during the training time, the tree made a default\\xa0 decision to send any new missings to the right node.',\n",
       " 'Solution:',\n",
       " 'Know more here.',\n",
       " '10| What is the difference between AdaBoost and XGBoost?',\n",
       " 'Solution: XGBoost is flexible compared to AdaBoost as XGB is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function.',\n",
       " 'Solution: ',\n",
       " 'A Technical Journalist who loves writing about Machine Learning and Artificial Intelligence. A lover of music, writing and learning something out of the box.',\n",
       " 'Masterclass',\n",
       " 'Accelerating Data Science Workloads with GPUs',\n",
       " 'Accelerating Data Science Workloads with GPUs',\n",
       " '6th Oct 2021',\n",
       " 'Register>>',\n",
       " '\\xa0',\n",
       " 'Hands-on Workshop',\n",
       " 'Data Engineering & Databases',\n",
       " 'Data Engineering & Databases',\n",
       " '9th Oct 2021',\n",
       " 'Register>>',\n",
       " 'Copyright Analytics India Magazine Pvt Ltd']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst21= []\n",
    "url = \"https://analyticsindiamag.com/top-xgboost-interview-questions-for-data-scientists/\"\n",
    "page_source = requests.get(url)\n",
    "soup = BeautifulSoup(page_source.content, \"html.parser\")\n",
    "answers = soup.find_all([\"strong\", \"p\"])\n",
    "for answer in answers:\n",
    "    lst21.append(answer.text)\n",
    "lst21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1| Is XGBoost faster than random forest?',\n",
       " 'Solution: XGBoost is usually used to train gradient-boosted decision trees (GBDT) and other gradient boosted models. Random forests also use the same model representation and inference as gradient-boosted decision trees, but it is a different training algorithm. XGBoost can be used to train a standalone random forest. Also, random forest can be used as a base model for gradient boosting techniques.',\n",
       " 'Solution:',\n",
       " 'Further, random forest is an improvement over bagging that helps in reducing the variance. Random forest builds trees in parallel, while in boosting, trees are built sequentially. Meaning, each of the trees is grown using information from previously grown trees, unlike bagging, where multiple copies of original training data are created and fit separate decision tree on each. This is the reason why XGBoost generally performs better than random forest.\\xa0',\n",
       " 'Know more here.',\n",
       " '2| What are the advantages and disadvantages of XGBoost?',\n",
       " 'Advantages:',\n",
       " 'Advantages:',\n",
       " 'Disadvantages:',\n",
       " 'Disadvantages:',\n",
       " 'Know more here.',\n",
       " '3| How XGBoost Works?',\n",
       " 'Solution: When using gradient boosting for regression, where the weak learners are considered to be regression trees, each of the regression trees maps an input data point to one of its leaves that includes a continuous score. XGB minimises a regularised objective function that merges a convex loss function, which is based on the variation between the target outputs and the predicted outputs. The training then proceeds iteratively, adding new trees with the capability to predict the residuals as well as errors of prior trees that are then coupled with the previous trees to make the final prediction.\\xa0',\n",
       " 'Solution: ',\n",
       " 'Click here to learn the step by step process of how XGB works.',\n",
       " '4| What does the weight of XGB leaf nodes mean? How to calculate it?',\n",
       " 'Solution: The “leaf weight” can be said as the model’s predicted output associated with each leaf (exit) node. Here is an instance of how to calculate the weights of the leaf nodes in XGB-',\n",
       " 'Solution: ',\n",
       " 'Consider a test data point, where age=10 and gender=female.To get the prediction for the data point, the tree is traversed from the top to bottom, performing a series of tests. At each of the intermediate nodes, a feature is needed to compare against a threshold.\\xa0',\n",
       " 'Now, depending on the result of the comparison, one must proceed to either the left or right child node of the tree. In case of (10, female), the test “age < 15” is to be performed first and then proceed to the left branch, because “age < 15” is true. Then, the second test “gender = male?” is performed, which evaluates to false, so we proceed to the right branch. We end up at the Leaf 2, whose output (leaf weight) is 0.1.',\n",
       " 'Click here to know more in detail.',\n",
       " '5| What are the data pre-processing steps for XGB?',\n",
       " 'Solution: The data pre-processing steps for XGB include the following-',\n",
       " 'Solution:',\n",
       " 'Know more here.',\n",
       " '6| How does XGB calculate features?',\n",
       " 'Solution: XGB automatically provides the estimations of feature importance from a trained predictive model. After a boosting tree is constructed, it retrieves feature importance scores for each attribute. The feature importance contributes a score which indicates how much valuable each feature was in the construction of the boosted decision trees within the model.\\xa0\\xa0',\n",
       " 'Solution:',\n",
       " 'Also, in terms of accuracy, XGB models show better performance for the training phase and comparable performance for the testing phase when compared to SVM models. Besides accuracy, XGB has higher computation speed than SVM.\\xa0\\xa0',\n",
       " 'Know more here.',\n",
       " '7| Why does XGBoost perform better than SVM?',\n",
       " 'Solution: In case of missing values, XGB is internally designed to handle missing values. The missing values are interpreted in such a way that if there endures any trend in the missing values, it is captured by the model. Users are required to supply a different value than other observations and pass that as a parameter.\\xa0',\n",
       " 'Solution: ',\n",
       " 'XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future. On the other hand, Support Vector Machine (SVM) does not perform well with the missing data and it is always a better option to impute the missing values before running SVM.\\xa0',\n",
       " 'Know more here.',\n",
       " '8| Differences between XGBoost and LightGBM.',\n",
       " 'Solution: XGBoost and LightGBM are the packages belonging to the family of gradient boosting decision trees (GBDTs).\\xa0',\n",
       " 'Solution:',\n",
       " 'Know more here.',\n",
       " '9| How does XGB handle missing values?',\n",
       " 'Solution: XGBoost supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. It is important to note that the gblinear booster treats missing values as zeros. During the training time XGB decides whether the missing values should fall into the right node or left node. This decision is taken to minimise the loss. If there are no missing values during the training time, the tree made a default\\xa0 decision to send any new missings to the right node.',\n",
       " 'Solution:',\n",
       " 'Know more here.',\n",
       " '10| What is the difference between AdaBoost and XGBoost?',\n",
       " 'Solution: XGBoost is flexible compared to AdaBoost as XGB is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_new21 = lst21[4:49]\n",
    "lst_new21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is XGBoost faster than random forest?</td>\n",
       "      <td>Solution: XGBoost is usually used to train gradient-boosted decision trees (GBDT) and other gradient boosted models. Random forests also use the same model representation and inference as gradient-boosted decision trees, but it is a different training algorithm. XGBoost can be used to train a standalone random forest. Also, random forest can be used as a base model for gradient boosting techniques.Solution:Further, random forest is an improvement over bagging that helps in reducing the variance. Random forest builds trees in parallel, while in boosting, trees are built sequentially. Meaning, each of the trees is grown using information from previously grown trees, unlike bagging, where multiple copies of original training data are created and fit separate decision tree on each. This is the reason why XGBoost generally performs better than random forest. Know more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the advantages and disadvantages of XGBoost?</td>\n",
       "      <td>Advantages:Advantages:Disadvantages:Disadvantages:Know more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How XGBoost Works?</td>\n",
       "      <td>Solution: When using gradient boosting for regression, where the weak learners are considered to be regression trees, each of the regression trees maps an input data point to one of its leaves that includes a continuous score. XGB minimises a regularised objective function that merges a convex loss function, which is based on the variation between the target outputs and the predicted outputs. The training then proceeds iteratively, adding new trees with the capability to predict the residuals as well as errors of prior trees that are then coupled with the previous trees to make the final prediction. Solution: Click here to learn the step by step process of how XGB works.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the weight of XGB leaf nodes mean? How to calculate it?</td>\n",
       "      <td>Solution: The “leaf weight” can be said as the model’s predicted output associated with each leaf (exit) node. Here is an instance of how to calculate the weights of the leaf nodes in XGB-Solution: Consider a test data point, where age=10 and gender=female.To get the prediction for the data point, the tree is traversed from the top to bottom, performing a series of tests. At each of the intermediate nodes, a feature is needed to compare against a threshold. Now, depending on the result of the comparison, one must proceed to either the left or right child node of the tree. In case of (10, female), the test “age &lt; 15” is to be performed first and then proceed to the left branch, because “age &lt; 15” is true. Then, the second test “gender = male?” is performed, which evaluates to false, so we proceed to the right branch. We end up at the Leaf 2, whose output (leaf weight) is 0.1.Click here to know more in detail.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the data pre-processing steps for XGB?</td>\n",
       "      <td>Solution: The data pre-processing steps for XGB include the following-Solution:Know more here.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            Questions  \\\n",
       "0                               Is XGBoost faster than random forest?   \n",
       "1               What are the advantages and disadvantages of XGBoost?   \n",
       "2                                                  How XGBoost Works?   \n",
       "3   What does the weight of XGB leaf nodes mean? How to calculate it?   \n",
       "4                     What are the data pre-processing steps for XGB?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Answer  \n",
       "0                                          Solution: XGBoost is usually used to train gradient-boosted decision trees (GBDT) and other gradient boosted models. Random forests also use the same model representation and inference as gradient-boosted decision trees, but it is a different training algorithm. XGBoost can be used to train a standalone random forest. Also, random forest can be used as a base model for gradient boosting techniques.Solution:Further, random forest is an improvement over bagging that helps in reducing the variance. Random forest builds trees in parallel, while in boosting, trees are built sequentially. Meaning, each of the trees is grown using information from previously grown trees, unlike bagging, where multiple copies of original training data are created and fit separate decision tree on each. This is the reason why XGBoost generally performs better than random forest. Know more here.  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Advantages:Advantages:Disadvantages:Disadvantages:Know more here.  \n",
       "2                                                                                                                                                                                                                                                    Solution: When using gradient boosting for regression, where the weak learners are considered to be regression trees, each of the regression trees maps an input data point to one of its leaves that includes a continuous score. XGB minimises a regularised objective function that merges a convex loss function, which is based on the variation between the target outputs and the predicted outputs. The training then proceeds iteratively, adding new trees with the capability to predict the residuals as well as errors of prior trees that are then coupled with the previous trees to make the final prediction. Solution: Click here to learn the step by step process of how XGB works.  \n",
       "3  Solution: The “leaf weight” can be said as the model’s predicted output associated with each leaf (exit) node. Here is an instance of how to calculate the weights of the leaf nodes in XGB-Solution: Consider a test data point, where age=10 and gender=female.To get the prediction for the data point, the tree is traversed from the top to bottom, performing a series of tests. At each of the intermediate nodes, a feature is needed to compare against a threshold. Now, depending on the result of the comparison, one must proceed to either the left or right child node of the tree. In case of (10, female), the test “age < 15” is to be performed first and then proceed to the left branch, because “age < 15” is true. Then, the second test “gender = male?” is performed, which evaluates to false, so we proceed to the right branch. We end up at the Leaf 2, whose output (leaf weight) is 0.1.Click here to know more in detail.  \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Solution: The data pre-processing steps for XGB include the following-Solution:Know more here.  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern=r\"^\\d+.+\\?*\"    \n",
    "flag=2\n",
    "str1=\"\"\n",
    "ques=[]\n",
    "ans=[]\n",
    "for i in lst_new21:\n",
    "    w=re.findall(pattern,i)\n",
    "    if len(w)>0:\n",
    "        ques.append(str(i))\n",
    "        if len(ques)==flag:\n",
    "            ans.append(str1)\n",
    "            str1=\"\"\n",
    "            flag=flag+1\n",
    "    else:\n",
    "        str1=str1+str(i)       \n",
    "ans.append(str1)\n",
    "print(len(ques))\n",
    "print(len(ans))\n",
    "for i in range(len(ans)):\n",
    "    ans[i]=ans[i].replace('\\n',\" \")\n",
    "for i in range(len(ques)):\n",
    "    ques[i]=re.sub(r\"^[\\d\\|]+\",\"\",ques[i])\n",
    "    \n",
    "df21=pd.DataFrame({\"Questions\":ques,\"Answer\":ans})\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df21[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.                                                       Image Source: Google Images</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is of the form:The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).Therefore, a linear regression model is quite easy to interpret.For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the basic assumptions of the Linear Regression Algorithm?</td>\n",
       "      <td>The basic assumptions of the Linear regression algorithm are as follows: Linearity: The relationship between the features and target. Homoscedasticity: The error term has a constant variance. Multicollinearity: There is no multicollinearity between the features. Independence: Observations are independent of each other. Normality: The error(residuals) follows a normal distribution. Now, let’s break these assumptions into different categories:It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’. Normality assumption: The error terms, ε(i), are normally distributed. Zero mean assumption: The residuals have a mean value of zero. Constant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity. Independent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.  The independent variables are measured without error. There does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the difference between Correlation and Regression.</td>\n",
       "      <td>Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.Regression: It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the Gradient Descent algorithm with respect to linear regression.</td>\n",
       "      <td>Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,y is the target column or output, and m is the number of data points in the training set.Step-1: Gradient Descent starts with a random solution,Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.The updated value for the parameter is given by the formulae:Repeat until convergence(upto minimum loss function)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>How does XGB calculate features?</td>\n",
       "      <td>Solution: XGB automatically provides the estimations of feature importance from a trained predictive model. After a boosting tree is constructed, it retrieves feature importance scores for each attribute. The feature importance contributes a score which indicates how much valuable each feature was in the construction of the boosted decision trees within the model.  Solution:Also, in terms of accuracy, XGB models show better performance for the training phase and comparable performance for the testing phase when compared to SVM models. Besides accuracy, XGB has higher computation speed than SVM.  Know more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>Why does XGBoost perform better than SVM?</td>\n",
       "      <td>Solution: In case of missing values, XGB is internally designed to handle missing values. The missing values are interpreted in such a way that if there endures any trend in the missing values, it is captured by the model. Users are required to supply a different value than other observations and pass that as a parameter. Solution: XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future. On the other hand, Support Vector Machine (SVM) does not perform well with the missing data and it is always a better option to impute the missing values before running SVM. Know more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Differences between XGBoost and LightGBM.</td>\n",
       "      <td>Solution: XGBoost and LightGBM are the packages belonging to the family of gradient boosting decision trees (GBDTs). Solution:Know more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>How does XGB handle missing values?</td>\n",
       "      <td>Solution: XGBoost supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. It is important to note that the gblinear booster treats missing values as zeros. During the training time XGB decides whether the missing values should fall into the right node or left node. This decision is taken to minimise the loss. If there are no missing values during the training time, the tree made a default  decision to send any new missings to the right node.Solution:Know more here.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>What is the difference between AdaBoost and XGBoost?</td>\n",
       "      <td>Solution: XGBoost is flexible compared to AdaBoost as XGB is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       Questions  \\\n",
       "0                                           What is Linear Regression Algorithm?   \n",
       "1                                How do you interpret a linear regression model?   \n",
       "2             What are the basic assumptions of the Linear Regression Algorithm?   \n",
       "3                     Explain the difference between Correlation and Regression.   \n",
       "4      Explain the Gradient Descent algorithm with respect to linear regression.   \n",
       "..                                                                           ...   \n",
       "581                                             How does XGB calculate features?   \n",
       "582                                    Why does XGBoost perform better than SVM?   \n",
       "583                                    Differences between XGBoost and LightGBM.   \n",
       "584                                          How does XGB handle missing values?   \n",
       "585                         What is the difference between AdaBoost and XGBoost?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Answer  \n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                In simple terms: It is a method of finding the best straight line fitting to the given dataset, i.e. tries to find the best linear relationship between the independent and dependent variables.In technical terms: It is a supervised machine learning algorithm that finds the best linear-fit relationship on the given dataset, between independent and dependent variables. It is mostly done with the help of the Sum of Squared Residuals Method, known as the Ordinary least squares (OLS) method.                                                       Image Source: Google Images   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              As we know that the linear regression model is of the form:The significance of the linear regression model lies in the fact that we can easily interpret and understand the marginal changes in the independent variables(predictors) and observed their consequences on the dependent variable(response).Therefore, a linear regression model is quite easy to interpret.For Example, if we increase the value of x1 increases by 1 unit, keeping other variables constant, then the total increase in the value of y will be βi and the intercept term (β0) is the response when all the predictor’s terms are set to zero or not considered.  \n",
       "2    The basic assumptions of the Linear regression algorithm are as follows: Linearity: The relationship between the features and target. Homoscedasticity: The error term has a constant variance. Multicollinearity: There is no multicollinearity between the features. Independence: Observations are independent of each other. Normality: The error(residuals) follows a normal distribution. Now, let’s break these assumptions into different categories:It is assumed that there exists a linear relationship between the dependent and the independent variables. Sometimes, this assumption is known as the ‘linearity assumption’. Normality assumption: The error terms, ε(i), are normally distributed. Zero mean assumption: The residuals have a mean value of zero. Constant variance assumption: The residual terms have the same (but unknown) value of variance, σ2. This assumption is also called the assumption of homogeneity or homoscedasticity. Independent error assumption: The residual terms are independent of each other, i.e. their pair-wise covariance value is zero.  The independent variables are measured without error. There does not exist a linear dependency between the independent variables, i.e. there is no multicollinearity in the data.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Correlation: It measures the strength or degree of relationship between two variables. It doesn’t capture causality. It is visualized by a single point.Regression: It measures how one variable affects another variable. Regression is all about model fitting. It tries to capture the causality and describes the cause and the effect. It is visualized by a regression line.  \n",
       "4                                                                                     Gradient descent is a first-order optimization algorithm. In linear regression, this algorithm is used to optimize the cost function to find the values of the βs (estimators) corresponding to the optimized value of the cost function.The working of Gradient descent is similar to a ball that rolls down a graph (ignoring the inertia). In that case, the ball moves along the direction of the maximum gradient and comes to rest at the flat surface i.e, corresponds to minima.Mathematically, the main objective of the gradient descent for linear regression is to find the solution of the following expression,ArgMin J(θ0, θ1), where J(θ0, θ1) represents the cost function of the linear regression. It is given by :Here, h is the linear hypothesis model, defined as h=θ0 + θ1x,y is the target column or output, and m is the number of data points in the training set.Step-1: Gradient Descent starts with a random solution,Step-2: Based on the direction of the gradient, the solution is updated to the new value where the cost function has a lower value.The updated value for the parameter is given by the formulae:Repeat until convergence(upto minimum loss function)  \n",
       "..                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...  \n",
       "581                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Solution: XGB automatically provides the estimations of feature importance from a trained predictive model. After a boosting tree is constructed, it retrieves feature importance scores for each attribute. The feature importance contributes a score which indicates how much valuable each feature was in the construction of the boosted decision trees within the model.  Solution:Also, in terms of accuracy, XGB models show better performance for the training phase and comparable performance for the testing phase when compared to SVM models. Besides accuracy, XGB has higher computation speed than SVM.  Know more here.  \n",
       "582                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Solution: In case of missing values, XGB is internally designed to handle missing values. The missing values are interpreted in such a way that if there endures any trend in the missing values, it is captured by the model. Users are required to supply a different value than other observations and pass that as a parameter. Solution: XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future. On the other hand, Support Vector Machine (SVM) does not perform well with the missing data and it is always a better option to impute the missing values before running SVM. Know more here.  \n",
       "583                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Solution: XGBoost and LightGBM are the packages belonging to the family of gradient boosting decision trees (GBDTs). Solution:Know more here.  \n",
       "584                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Solution: XGBoost supports missing values by default. In tree algorithms, branch directions for missing values are learned during training. It is important to note that the gblinear booster treats missing values as zeros. During the training time XGB decides whether the missing values should fall into the right node or left node. This decision is taken to minimise the loss. If there are no missing values during the training time, the tree made a default  decision to send any new missings to the right node.Solution:Know more here.  \n",
       "585                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Solution: XGBoost is flexible compared to AdaBoost as XGB is a generic algorithm to find approximate solutions to the additive modeling problem, while AdaBoost can be seen as a special case with a particular loss function.  \n",
       "\n",
       "[586 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Questions    0\n",
       "Answer       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"ChatBotData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What are the basic assumptions of the Linear...</td>\n",
       "      <td>The basic assumptions of the Linear regression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Explain the difference between Correlation a...</td>\n",
       "      <td>Correlation: It measures the strength or degre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Explain the Gradient Descent algorithm with ...</td>\n",
       "      <td>Gradient descent is a first-order optimization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>581</td>\n",
       "      <td>How does XGB calculate features?</td>\n",
       "      <td>Solution: XGB automatically provides the estim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>582</td>\n",
       "      <td>Why does XGBoost perform better than SVM?</td>\n",
       "      <td>Solution: In case of missing values, XGB is in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>583</td>\n",
       "      <td>Differences between XGBoost and LightGBM.</td>\n",
       "      <td>Solution: XGBoost and LightGBM are the package...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>584</td>\n",
       "      <td>How does XGB handle missing values?</td>\n",
       "      <td>Solution: XGBoost supports missing values by d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>585</td>\n",
       "      <td>What is the difference between AdaBoost and X...</td>\n",
       "      <td>Solution: XGBoost is flexible compared to AdaB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                          Questions  \\\n",
       "0             0               What is Linear Regression Algorithm?   \n",
       "1             1    How do you interpret a linear regression model?   \n",
       "2             2    What are the basic assumptions of the Linear...   \n",
       "3             3    Explain the difference between Correlation a...   \n",
       "4             4    Explain the Gradient Descent algorithm with ...   \n",
       "..          ...                                                ...   \n",
       "581         581                   How does XGB calculate features?   \n",
       "582         582          Why does XGBoost perform better than SVM?   \n",
       "583         583          Differences between XGBoost and LightGBM.   \n",
       "584         584                How does XGB handle missing values?   \n",
       "585         585   What is the difference between AdaBoost and X...   \n",
       "\n",
       "                                                Answer  \n",
       "0    In simple terms: It is a method of finding the...  \n",
       "1    As we know that the linear regression model is...  \n",
       "2    The basic assumptions of the Linear regression...  \n",
       "3    Correlation: It measures the strength or degre...  \n",
       "4    Gradient descent is a first-order optimization...  \n",
       "..                                                 ...  \n",
       "581  Solution: XGB automatically provides the estim...  \n",
       "582  Solution: In case of missing values, XGB is in...  \n",
       "583  Solution: XGBoost and LightGBM are the package...  \n",
       "584  Solution: XGBoost supports missing values by d...  \n",
       "585  Solution: XGBoost is flexible compared to AdaB...  \n",
       "\n",
       "[586 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"ChatBotData.csv\", encoding='utf-8')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1254', 'confidence': 0.5617028932314014, 'language': 'Turkish'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"ChatBotData.csv\", 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "\n",
    "# check what the character encoding might be\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the basic assumptions of the Linear...</td>\n",
       "      <td>The basic assumptions of the Linear regression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the difference between Correlation a...</td>\n",
       "      <td>Correlation: It measures the strength or degre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the Gradient Descent algorithm with ...</td>\n",
       "      <td>Gradient descent is a first-order optimization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>What are dimensionality reduction and its bene...</td>\n",
       "      <td>Dimensionality reduction refers to the process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>How can you select k for k-means?</td>\n",
       "      <td>We use the elbow method to select k for k-mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>What are the feature vectors?</td>\n",
       "      <td>A feature vector is an n-dimensional vector of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>What is root cause analysis?</td>\n",
       "      <td>Root cause analysis was initially developed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>What is collaborative filtering?</td>\n",
       "      <td>Most recommender systems use this filtering pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>377 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Questions  \\\n",
       "0                 What is Linear Regression Algorithm?   \n",
       "1      How do you interpret a linear regression model?   \n",
       "2      What are the basic assumptions of the Linear...   \n",
       "3      Explain the difference between Correlation a...   \n",
       "4      Explain the Gradient Descent algorithm with ...   \n",
       "..                                                 ...   \n",
       "372  What are dimensionality reduction and its bene...   \n",
       "373                  How can you select k for k-means?   \n",
       "374                      What are the feature vectors?   \n",
       "375                       What is root cause analysis?   \n",
       "376                   What is collaborative filtering?   \n",
       "\n",
       "                                                Answer  \n",
       "0    In simple terms: It is a method of finding the...  \n",
       "1    As we know that the linear regression model is...  \n",
       "2    The basic assumptions of the Linear regression...  \n",
       "3    Correlation: It measures the strength or degre...  \n",
       "4    Gradient descent is a first-order optimization...  \n",
       "..                                                 ...  \n",
       "372  Dimensionality reduction refers to the process...  \n",
       "373  We use the elbow method to select k for k-mean...  \n",
       "374  A feature vector is an n-dimensional vector of...  \n",
       "375  Root cause analysis was initially developed to...  \n",
       "376  Most recommender systems use this filtering pr...  \n",
       "\n",
       "[377 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv(\"Dataset.csv\", usecols=[1,2])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>What are the basic assumptions of the Linear...</td>\n",
       "      <td>The basic assumptions of the Linear regression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Explain the difference between Correlation a...</td>\n",
       "      <td>Correlation: It measures the strength or degre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Explain the Gradient Descent algorithm with ...</td>\n",
       "      <td>Gradient descent is a first-order optimization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What are dimensionality reduction and its bene...</td>\n",
       "      <td>Dimensionality reduction refers to the process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>NaN</td>\n",
       "      <td>How can you select k for k-means?</td>\n",
       "      <td>We use the elbow method to select k for k-mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What are the feature vectors?</td>\n",
       "      <td>A feature vector is an n-dimensional vector of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What is root cause analysis?</td>\n",
       "      <td>Root cause analysis was initially developed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>NaN</td>\n",
       "      <td>What is collaborative filtering?</td>\n",
       "      <td>Most recommender systems use this filtering pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>963 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                          Questions  \\\n",
       "0           0.0               What is Linear Regression Algorithm?   \n",
       "1           1.0    How do you interpret a linear regression model?   \n",
       "2           2.0    What are the basic assumptions of the Linear...   \n",
       "3           3.0    Explain the difference between Correlation a...   \n",
       "4           4.0    Explain the Gradient Descent algorithm with ...   \n",
       "..          ...                                                ...   \n",
       "958         NaN  What are dimensionality reduction and its bene...   \n",
       "959         NaN                  How can you select k for k-means?   \n",
       "960         NaN                      What are the feature vectors?   \n",
       "961         NaN                       What is root cause analysis?   \n",
       "962         NaN                   What is collaborative filtering?   \n",
       "\n",
       "                                                Answer  \n",
       "0    In simple terms: It is a method of finding the...  \n",
       "1    As we know that the linear regression model is...  \n",
       "2    The basic assumptions of the Linear regression...  \n",
       "3    Correlation: It measures the strength or degre...  \n",
       "4    Gradient descent is a first-order optimization...  \n",
       "..                                                 ...  \n",
       "958  Dimensionality reduction refers to the process...  \n",
       "959  We use the elbow method to select k for k-mean...  \n",
       "960  A feature vector is an n-dimensional vector of...  \n",
       "961  Root cause analysis was initially developed to...  \n",
       "962  Most recommender systems use this filtering pr...  \n",
       "\n",
       "[963 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = pd.concat([df, new_df], ignore_index=True)\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the basic assumptions of the Linear...</td>\n",
       "      <td>The basic assumptions of the Linear regression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the difference between Correlation a...</td>\n",
       "      <td>Correlation: It measures the strength or degre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the Gradient Descent algorithm with ...</td>\n",
       "      <td>Gradient descent is a first-order optimization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>What are dimensionality reduction and its bene...</td>\n",
       "      <td>Dimensionality reduction refers to the process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>How can you select k for k-means?</td>\n",
       "      <td>We use the elbow method to select k for k-mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>What are the feature vectors?</td>\n",
       "      <td>A feature vector is an n-dimensional vector of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>What is root cause analysis?</td>\n",
       "      <td>Root cause analysis was initially developed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>What is collaborative filtering?</td>\n",
       "      <td>Most recommender systems use this filtering pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>963 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Questions  \\\n",
       "0                 What is Linear Regression Algorithm?   \n",
       "1      How do you interpret a linear regression model?   \n",
       "2      What are the basic assumptions of the Linear...   \n",
       "3      Explain the difference between Correlation a...   \n",
       "4      Explain the Gradient Descent algorithm with ...   \n",
       "..                                                 ...   \n",
       "958  What are dimensionality reduction and its bene...   \n",
       "959                  How can you select k for k-means?   \n",
       "960                      What are the feature vectors?   \n",
       "961                       What is root cause analysis?   \n",
       "962                   What is collaborative filtering?   \n",
       "\n",
       "                                                Answer  \n",
       "0    In simple terms: It is a method of finding the...  \n",
       "1    As we know that the linear regression model is...  \n",
       "2    The basic assumptions of the Linear regression...  \n",
       "3    Correlation: It measures the strength or degre...  \n",
       "4    Gradient descent is a first-order optimization...  \n",
       "..                                                 ...  \n",
       "958  Dimensionality reduction refers to the process...  \n",
       "959  We use the elbow method to select k for k-mean...  \n",
       "960  A feature vector is an n-dimensional vector of...  \n",
       "961  Root cause analysis was initially developed to...  \n",
       "962  Most recommender systems use this filtering pr...  \n",
       "\n",
       "[963 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Questions    0\n",
       "Answer       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv(\"final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Linear Regression Algorithm?</td>\n",
       "      <td>In simple terms: It is a method of finding the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do you interpret a linear regression model?</td>\n",
       "      <td>As we know that the linear regression model is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the basic assumptions of the Linear...</td>\n",
       "      <td>The basic assumptions of the Linear regression...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explain the difference between Correlation a...</td>\n",
       "      <td>Correlation: It measures the strength or degre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain the Gradient Descent algorithm with ...</td>\n",
       "      <td>Gradient descent is a first-order optimization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>What are dimensionality reduction and its bene...</td>\n",
       "      <td>Dimensionality reduction refers to the process...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>How can you select k for k-means?</td>\n",
       "      <td>We use the elbow method to select k for k-mean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>What are the feature vectors?</td>\n",
       "      <td>A feature vector is an n-dimensional vector of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>What is root cause analysis?</td>\n",
       "      <td>Root cause analysis was initially developed to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>What is collaborative filtering?</td>\n",
       "      <td>Most recommender systems use this filtering pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>711 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Questions  \\\n",
       "0                 What is Linear Regression Algorithm?   \n",
       "1      How do you interpret a linear regression model?   \n",
       "2      What are the basic assumptions of the Linear...   \n",
       "3      Explain the difference between Correlation a...   \n",
       "4      Explain the Gradient Descent algorithm with ...   \n",
       "..                                                 ...   \n",
       "706  What are dimensionality reduction and its bene...   \n",
       "707                  How can you select k for k-means?   \n",
       "708                      What are the feature vectors?   \n",
       "709                       What is root cause analysis?   \n",
       "710                   What is collaborative filtering?   \n",
       "\n",
       "                                                Answer  \n",
       "0    In simple terms: It is a method of finding the...  \n",
       "1    As we know that the linear regression model is...  \n",
       "2    The basic assumptions of the Linear regression...  \n",
       "3    Correlation: It measures the strength or degre...  \n",
       "4    Gradient descent is a first-order optimization...  \n",
       "..                                                 ...  \n",
       "706  Dimensionality reduction refers to the process...  \n",
       "707  We use the elbow method to select k for k-mean...  \n",
       "708  A feature vector is an n-dimensional vector of...  \n",
       "709  Root cause analysis was initially developed to...  \n",
       "710  Most recommender systems use this filtering pr...  \n",
       "\n",
       "[711 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
